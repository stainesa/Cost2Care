---
title: "rstanarm Logistic"
author: "Anthony Staines"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: true
    fig_caption: true
    number_sections: true
    latex_engine: xelatex
  word_document: default
  html_document: default
editor_options:
  chunk_output_type: console
bibliography: references.bib
csl: vancouver.csl
---

```{r setup, include=FALSE}

#rm(list = ls())

library(rstanarm)

library(tidyverse)
library(bayesplot)
library(tidybayes)
library(lubridate)
library(tibble)
library(tidymodels)
library(readxl)

library(lme4)

library(knitr)
library(kableExtra)
library(summarytools)

library(patchwork)

library(ranger)
library(mlr3verse)
library(mlr3viz)

library(data.table)
library(future)

library(sjPlot)
library(sjtable2df)

library(broom)
library(broom.mixed)

library(stargazer)

library(gt)
library(gtsummary)

tidymodels_prefer(quiet = TRUE)
st_options(ctable.round.digits = 2)

#How many CPU's?
N = parallel::detectCores()
  options(Ncpus = N - 1)
  options(mc.cores = N - 1)
  setDTthreads(threads = N - 1,
               restore_after_fork = TRUE,
               throttle = 1024)

options(dplyr.summarise.inform = FALSE, # shut up summarise
        ranger.num.threads = N - 1) # Prepare for rf models

knitr::opts_chunk$set(echo = FALSE,
                      cache = TRUE, cache.lazy = FALSE, 
                      warning = NA, message = NA, fig.pos = 'H',
                      cache.extra = knitr::rand_seed)
```

# Logistic models for adverse outcomes in HIPE

The idea here is to fit a completely different model - a logistic model fitted using rstanarm (or STAN, if I feel confident enough), using the variables selected by the random forest classifiers in MLR3, and see where we get to.

# data

```{r Load existing data}
HIPE <- readRDS('data/HIPE.Rds')
CCH <- readRDS('data/CCH.Rds')
glimpse(CCH)

# STAN does not like character variables, so we turn them into factors (No =  0, and  Yes = 1, more or less.

CCH <- CCH |>
  mutate(across(where(is.character), ~as_factor(.)))  |>
  rowid_to_column(var = 'rowid')
glimpse(CCH)

HIPE <- HIPE |>
  mutate(across(where(is.character), ~as_factor(.)))
glimpse(HIPE)

table(CCH$PN) # 79 positive out of 1,000

TruePlot <- ggplot(CCH |> arrange(PN),
       aes(y = PN, x = rowid, colour = PN)) +
  geom_jitter() +
  guides(colour = 'none') +
  labs(title = "Observed data from chart review",
       x = '',
       y = 'Pneumonia')
TruePlot
```

Load the tasks to get at the chosen variables

```{r Load tasks}
tsk_PN   <- readRDS('data/tsk.PN')
tsk_PNk  <- readRDS('data/tsk.PNk')
tsk_UTI  <- readRDS('data/tsk.UTI')
tsk_UTIk <- readRDS('data/tsk.UTIk')
tsk_PU   <- readRDS('data/tsk.PU')
tsk_PUk  <- readRDS('data/tsk.PUk')
tsk_DL   <- readRDS('data/tsk.DL')
tsk_DLk  <- readRDS('data/tsk.DLk')
tsk_F2M  <- readRDS('data/tsk.F2M')
tsk_F2Mk <- readRDS('data/tsk.F2Mk')

tsk_F2Mk$feature_names
CCH |> select(tsk_F2Mk$feature_names, Weights) |> names() # this is how we subset
```

Split CCH into training and test sets

```{r Train test split model}
#Split CCH into training and test sets
#
set.seed(123)
splits      <- initial_split(CCH, strata = PN)

CCH_train <- training(splits)
CCH_test  <- testing(splits)

t.train <- table(CCH_train$PN)
t.train
  pTrainYes <-  t.train[[1]]/(t.train[[1]] + t.train[[2]])
  pTrainNo <-  t.train[[2]]/(t.train[[1]] + t.train[[2]])
pTrainNo
pTrainYes

t.test <- table(CCH_test$PN)
t.test
  pTestYes <-  t.test[[1]]/(t.test[[1]] + t.test[[2]])
  pTestNo <-  t.test[[2]]/(t.test[[1]] + t.test[[2]])
pTestNo
pTestYes

```

## Variable selection

These come from the ranger fits, and variables are listed in order of variable permutation importance.


```{r Rstanarm model}
  
  t_prior <- student_t(df = 7, location = 0, scale = 2.5)

fit.PNk <- stan_glm(PN ~ .,
                 data = CCH |> select(c(tsk_PNk$feature_names, Weight)),
                 family = binomial(link = "logit"),
                 weights = Weight,
                 prior = t_prior, prior_intercept = t_prior,
                 cores = 4, seed = 12345)
fit.PNk
launch_shinystan(fit.PNk, ppd = TRUE) # Allows me to check fit etc.
```


```{r Review training predictions}

get_variables(fit.PN)

# Get a sample of posterior draws from a model as a tibble
fit.PN.td <- fit.PN |>
  tidy_draws() |>
  mutate(Intercept = `(Intercept)`) |> # Rename Intercept from Stan's default
  select(.chain:.draw,Intercept,HadxCount:energy__) # Back to original order


fit.PN.td |>
  head(10)

Observations = nrow(CCH_TRAIN)

Confusions <- CCH_TRAIN |> 
  add_predicted_draws(fit.PN) |>
  ungroup() |>
  select(.draw, PN, .prediction) |>
  group_by(PN, .prediction, .draw) |>
  summarise( N = n()) |>
  arrange(.draw, PN, .prediction) |>
  select(.draw, PN, .prediction, N) |>
  mutate(P = N/Observations) |>
  mutate(Type = case_when(
    PN == 'No'  & .prediction == 0 ~ 'TN',
    PN == 'No'  & .prediction == 1 ~ 'FP',
    PN == 'Yes' & .prediction == 0 ~ 'FN',
    PN == 'Yes' & .prediction == 1 ~ 'TP',  )) |>
  mutate(Prediction = case_when(
    .prediction == 0 ~ 'N',
    .prediction == 1 ~ 'P',  )) |>
  ungroup()

  ggplot(Confusions, aes(x = Type, y = P)) + geom_boxplot()
  ggplot(Confusions, aes(x = Type, y = N)) + geom_boxplot()

# 4,000 confusion matrices!

  Confusions |> filter(Type == 'TN') |> summary()
  Confusions |> filter(Type == 'FN') |> summary()
  Confusions |> filter(Type == 'FP') |> summary()
  Confusions |> filter(Type == 'TP') |> summary()

  Confusions |>
    group_by(Prediction, .draw) |>
    summarise(P = sum(P)) |>
    arrange(.draw, Prediction) |>
    ggplot(aes(y = Prediction,
               x = P,
               colour = Prediction, fill = Prediction)) +
  geom_violin() +
    geom_vline(xintercept = c(pTrainNo, pTrainYes), colour = c('blue', 'blue'), linewidth = 1.2) +
  guides(colour = 'none') +
  labs(title = "Predictions for training data from chart review",
       subtitle = "Training data probability shown as vertical lines",
       x = 'Estimated probability',
       y = 'Pneumonia') +
    theme_minimal() +
    scale_fill_viridis_d(option = 'G') +
    scale_colour_viridis_d(option = 'G')

```

The predictions on the test data are pretty good, but lower than on the training data.


```{r Test data}

Confusions <- CCH_test |> 
  add_predicted_draws(object = fit.PN) |>
  ungroup() |>
  select(.draw, PN, .prediction) |>
  group_by(PN, .prediction, .draw) |>
  summarise( N = n()) |>
  arrange(.draw, PN, .prediction) |>
  select(.draw, PN, .prediction, N) |>
  mutate(P = N/322) |>
  mutate(Type = case_when(
    PN == 'No'  & .prediction == 0 ~ 'TN',
    PN == 'No'  & .prediction == 1 ~ 'FP',
    PN == 'Yes' & .prediction == 0 ~ 'FN',
    PN == 'Yes' & .prediction == 1 ~ 'TP',  )) |>
  mutate(Prediction = case_when(
    .prediction == 0 ~ 'No',
    .prediction == 1 ~ 'Yes',  )) |>
  ungroup() |>
  select(-c(PN, .prediction))

Confusions |> group_by(.draw) |> summarise(N = n()) |> filter((N !=4)) # 1277

  ggplot(Confusions, aes(x = Type, y = P)) + geom_boxplot()
  ggplot(Confusions, aes(x = Type, y = N)) + geom_boxplot()

# 4,000 confusion matrices on test data!

  Confusions |> filter(Type == 'TN') |> summary(P)
  Confusions |> filter(Type == 'FN') |> summary(P)
  Confusions |> filter(Type == 'FP') |> summary(P)
  Confusions |> filter(Type == 'TP') |> summary(P)

Confusions |>
    group_by(Prediction, .draw) |>
    summarise(P = sum(P)) |>
    arrange(.draw, Prediction) |>
    ggplot(aes(y = Prediction,
               x = P,
               colour = Prediction, fill = Prediction)) +
  geom_violin() +
    geom_vline(xintercept = c(pTestNo, pTestYes), colour = c('blue', 'blue'), linewidth = 1.2) +
  guides(colour = 'none') +
  labs(title = "Predictions for test data from chart review",
       subtitle = "Test data probability shown as vertical lines",
       x = 'Estimated probability',
       y = 'Pneumonia') +
    theme_minimal() +
    scale_fill_viridis_d(option = 'G') +
    scale_colour_viridis_d(option = 'G')

```

```{r Collate predictions}
CCH_linpred <- CCH_test |> 
  add_linpred_draws(object = fit.PN) |>
  ungroup() |> 
  select(rowid, .draw, PN, .linpred) |>
  arrange(rowid, .draw)

CCH_linpred |> group_by(.draw) |> summarise(N = n()) |> summary()

CCH_t.linpred <- CCH_test |> 
  add_linpred_draws(object = fit.PN, transform = TRUE, value = 't.linpred') |>
  ungroup() |> 
  select(rowid, .draw, PN, t.linpred) |>
  arrange(rowid, .draw)

CCH_t.linpred |> group_by(.draw) |> summarise(N = n()) |> summary()

CCH_epred <- CCH_test |> 
  add_epred_draws(object = fit.PN) |>
  ungroup() |>
  select(rowid, .draw, PN, .epred) |>
  arrange(rowid, .draw)

CCH_epred |> group_by(.draw) |> summarise(N = n()) |> summary()

CCH_pred <- CCH_test |> 
  add_predicted_draws(object = fit.PN) |>
  ungroup() |>
  select(rowid, .draw, PN, .prediction) |>
  arrange(rowid, .draw)

CCH_pred |> group_by(.draw) |> summarise(N = n()) |> summary()

Predictions <- CCH_pred |>
  left_join(CCH_epred,by = join_by(rowid, .draw, PN)) |>
  left_join(CCH_linpred,by = join_by(rowid, .draw, PN)) |>
  left_join(CCH_t.linpred,by = join_by(rowid, .draw, PN)) 

ggplot(Predictions |> filter(.draw <= 1000),
       aes(x= .epred, y = .prediction, colour = PN)) +
  geom_jitter()
summarytools::ctable(Predictions$PN, Predictions$.prediction, prop = 't')
#ggplot(Predictions, aes(x = .epred, y = t.linpred)) + geom_point() # All identical
```


```{r}
pp_check(fit.PN)
pp_check(fit.PN, plotfun = "boxplot", nreps = 10, notch = FALSE)
pp_check(fit.PN, plotfun = "hist", nreps = 10)

rstan::stan_plot(fit.PN, show_density=TRUE)
rstan::stan_plot(fit.PN, pars = get_variables(fit.PN)[2:28], show_density=TRUE)

plot(loo(fit.PN))

performance::model_performance(fit.PN)
```

# What matters?

This is not a classification problem. We are trying to estimate the prevalence, that is the probability that a random person discharged will have developed hospital acquired pneumonia. In terms of our models, this is just the transformed linear predictor, or the .epred.


```{r}
EPred.PN.h <- posterior_epred(fit.PN,
                              newdata = HIPE) # Probabilities
# 4000 rows of one column per record

EPred.PN.ht <- as_tibble(t(EPred.PN.h))

# 131478 rows of 4,000 draws in columns

Median <- EPred.PN.h |>  summarise(across(everything(), median))
Epred.PN <- Median |> pivot_longer(cols = everything(), names_to = 'Row', values_to = 'Median')

summary(Epred.PN)

ggplot(Epred.PN, aes(x = round(100*Median,0))) + geom_density()

```
 
