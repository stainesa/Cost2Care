---
title: "Bayes predictions"
author: "Anthony Staines"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: true
    fig_caption: true
    number_sections: true
    latex_engine: xelatex
  word_document: default
  html_document: default
editor_options:
  chunk_output_type: console
bibliography: references.bib
csl: vancouver.csl
---

```{r Clean environment, include = FALSE}
  rm(list = ls())
```

```{r setup, include=FALSE}
library(rstanarm)
library(tidybayes)
library(posterior)

library(tidyverse)
library(bayesplot)
library(bayestestR)

library(tidybayes)
library(lubridate)
library(tibble)
library(tidymodels)
library(readxl)

library(lme4)

library(knitr)
library(kableExtra)
library(summarytools)

library(patchwork)
library(dotwhisker)
library(ggdist)

library(ranger)
library(mlr3verse)
library(mlr3viz)

library(data.table)
library(future)

library(sjPlot)
library(sjtable2df)

library(broom)
library(broom.mixed)

library(matrixStats)

tidymodels_prefer(quiet = TRUE)
st_options(ctable.round.digits = 2)

#How many CPU's?
N = parallel::detectCores()
  options(Ncpus = N - 1)
  options(mc.cores = N - 1)
  setDTthreads(threads = N - 1,
               restore_after_fork = TRUE,
               throttle = 1024)

options(dplyr.summarise.inform = FALSE, # shut up summarise
        ranger.num.threads = N - 1) # Prepare for rf models

knitr::opts_chunk$set(
	echo = FALSE,
	fig.pos = "H",
	message = FALSE,
	warning = FALSE,
	cache = TRUE,
	cache.extra = knitr::rand_seed,
	cache.lazy = FALSE
)
rm(N)
```

# Logistic models for adverse outcomes in HIPE

The idea here is to fit a completely different model - a logistic model fitted using rstanarm (or STAN, if I feel confident enough), using the variables selected by the random forest classifiers in MLR3, and see where we get to.

# Data

```{r Load existing data}
HIPE <- readRDS('data/HIPE.Rds')
CCH <- readRDS('data/CCH.Rds')
glimpse(CCH)

# STAN does not like character variables, so we turn them into factors (No =  0, and  Yes = 1, more or less.

CCH <- CCH |>
  mutate(across(where(is.character), ~as_factor(.)))  |>
  rowid_to_column(var = 'rowid')
glimpse(CCH)

HIPE <- HIPE |>
  mutate(across(where(is.character), ~as_factor(.))) |>
  rowid_to_column('rowid') # Add a rowid variable

glimpse(HIPE)

table(CCH$PN) # 79 positive out of 1,000
```

```{r Plots of actual event occurrence}
CCH |>
  select(rowid,PN:F2M) |>
  pivot_longer(cols = c(PN:F2M),
               names_to = 'Outcome',
               values_to = 'Result') |>
  ggplot(
    aes(y = Result, x = rowid, colour = Outcome)) +
  geom_jitter() +
  guides(colour = 'none') +
  labs(title = "Observed data from chart review",
       x = 'Observation number',
       y = 'Result') +
  theme_minimal() +
  facet_wrap(~ Outcome)

```

## Load tasks
We're fitting a series of models where we've already done the variable selection with a bunch of random forest models.
Here we load the tasks to get at the chosen variables, and pull the variables out. Neither the weighting variable, Weights, nor the adverse outcome (PN, UTI, ...) is a feature in the tasks.

```{r Load tasks}
tsk_PN   <- readRDS('data/tsk.PN')
tsk_PNk  <- readRDS('data/tsk.PNk')
tsk_UTI  <- readRDS('data/tsk.UTI')
tsk_UTIk <- readRDS('data/tsk.UTIk')
tsk_PU   <- readRDS('data/tsk.PU')
tsk_PUk  <- readRDS('data/tsk.PUk')
tsk_DL   <- readRDS('data/tsk.DL')
tsk_DLk  <- readRDS('data/tsk.DLk')
tsk_F2M  <- readRDS('data/tsk.F2M')
tsk_F2Mk <- readRDS('data/tsk.F2Mk')
```

### What do the tasks contain?

We check the weighting of the tasks, to ensure that the row numbers align, and show how to subset data for one task

```{r Review selected tasks and weights}
# What information have we got on each task?
# 
#tsk_F2Mk$feature_names # List of names of variables
weights <- tsk_F2M$weights # Dataframe of row.id and weight
#tsk_F2M$target_names # One character variable

#Check alignment of weights
CCH |>
  select(Weights, rowid) |>
  full_join(weights, by = join_by(rowid == row_id)) |>
  mutate(NotEqual = ifelse(Weights == weight,0,1)) |>
  filter(NotEqual == 1) # 0 entries
# alignment is correct, weights are identical.

CCH |>
  select(c('rowid', 'Weights', tsk_PNk$target_names, tsk_PNk$feature_names)) |>
  names() # this is how we subset for each task

rm(weights)
```

### What's where?

This checks that all the features used are in the dataframe CCH, and that there are no extra features we didn't know about.

```{r What is where?}
LIST = names(CCH)
NAMES <- as_tibble(LIST) |>
  rename(NAME = value) 

PN <- as_tibble(tsk_PN$feature_names) |>
      rename(PN = value)
PNk <- as_tibble(tsk_PNk$feature_names) |>
      rename(PNk = value)
UTI <- as_tibble(tsk_UTI$feature_names) |>
      rename(UTI = value)
UTIk <- as_tibble(tsk_UTIk$feature_names) |>
      rename(UTIk = value)
PU <- as_tibble(tsk_PU$feature_names) |>
      rename(PU = value)
PUk <- as_tibble(tsk_PUk$feature_names) |>
      rename(PUk = value)
DL <- as_tibble(tsk_DL$feature_names) |>
      rename(DL = value)
DLk <- as_tibble(tsk_DLk$feature_names) |>
      rename(DLk = value)
F2M <- as_tibble(tsk_F2M$feature_names) |>
      rename(F2M = value)
F2Mk <- as_tibble(tsk_F2Mk$feature_names) |>
      rename(F2Mk = value)


NAMES <- NAMES |>
  full_join(PN,
    by = join_by(NAME == PN),
    suffix = c('','.PN'),
    keep = TRUE) |>
  full_join(PNk,
    by = join_by(NAME == PNk),
    suffix = c('','.PNk'),
    keep = TRUE) |>
  full_join(UTI,
    by = join_by(NAME == UTI),
    suffix = c('','.UTI'),
    keep = TRUE) |>
  full_join(UTIk,
    by = join_by(NAME == UTIk),
    suffix = c('','.UTIk'),
    keep = TRUE) |>
  full_join(PU,
    by = join_by(NAME == PU),
    suffix = c('','.PU'),
    keep = TRUE) |>
  full_join(PUk,
    by = join_by(NAME == PUk),
    suffix = c('','.PUk'),
    keep = TRUE) |>
  full_join(DL,
    by = join_by(NAME == DL),
    suffix = c('','.DL'),
    keep = TRUE) |>
  full_join(DLk,
    by = join_by(NAME == DLk),
    suffix = c('','.DLk'),
    keep = TRUE) |>
  full_join(F2M,
    by = join_by(NAME == F2M),
    suffix = c('','.F2M'),
    keep = TRUE) |>
  full_join(F2Mk,
    by = join_by(NAME == F2Mk),
    suffix = c('','.F2Mk'),
    keep = TRUE)

# View(NAMES) - for interactive use only

rm(LIST)
rm(PN,PNk, UTI, UTIk, PU, PUk, DL, DLk, F2M, F2Mk)
rm(NAMES)

```

## Check variability

Some of the variables are almost all 0 or 1. It can happen that in the training, or test set, these are all 1 or zero, which generates an error message, and prevents fitting. These variable are not contributory, and need to be omitted.

```{r check variability}
CCH |>
  mutate(across(everything(), ~ length(unique(.)))) |>
  distinct() |>
  pivot_longer(cols = everything(), names_to = 'Name', values_to = 'Count') |>
filter(Count == 1)

CCH |>
  select(Anaesthesia:Therapeutic.Interventions) |>
 mutate(across(everything(), ~sum(.))) |>
  distinct() |>
  glimpse()

# Two variables 
#  CCH$Client.Support.Interventions
#  CCH$Procedures.On.Ear.And.Mastoid.Process
# have no occurrences in the Chart Review data set
# Two more
#  CCH$Procedures.On.Endocrine.System and
#  CCH$Radiation.Oncology.Procedures
# have only one, and hence cannot be considered further
#  CCH$Dental.Services has only 3, which is really
# too small to be any use, so we lose it too.
```

## Training and test datasets

Split CCH into training and test sets per task.

```{r Train test split model}
#Split CCH into training and test sets
#
set.seed(4763765)

# Ancillary function to make the necessary pieces
make_training <- function(df, TASK) {
  name <- TASK$target_names
  NAME <- {{name}}
  NAMEs <- ensym(NAME)
  
  splits <- initial_split(df,
                prop = 700/1000,
                strata = name)
  
  train <- training(splits)
  test  <- testing(splits)
  
    counts <- train |>
      select(!!NAMEs) |> group_by(!!NAMEs) |>
      summarise(N = n()) |>
      mutate(Type = 'Train') |>
      bind_rows(test |>
        select(!!NAMEs) |> group_by(!!NAMEs) |>
        summarise(N = n()) |>
        mutate(Type = 'Test')) 

    totals <- counts |>
      group_by(Type) |>
      summarise(Sum = sum(N))

    probs <- counts |>
      full_join(totals, by = join_by(Type)) |>
      mutate(Prob = N/Sum)
    
    tsk_name = deparse(substitute(TASK))
    names_for_list <- c(paste(tsk_name, 'splits', sep = '_'),
                        paste(tsk_name, 'train', sep = '_'),
                        paste(tsk_name, 'test', sep = '_'),
                        paste(tsk_name, 'probs', sep = '_'))

    LIST = list(splits, train, test, probs)
    names(LIST) <- names_for_list

  return(LIST)
}

# Named list with 4 elements
# XX_splits Split from which test and train are derived
# XX_train Training data set
# XX_test  Test data set
# XX_probs Counts, totals, and probabilities by test and train

list_PN  <- make_training(CCH, tsk_PN)
list_PNk <- make_training(CCH, tsk_PNk)

list_UTI  <- make_training(CCH, tsk_UTI)
list_UTIk <- make_training(CCH, tsk_UTIk)

list_PU  <- make_training(CCH, tsk_PU)
list_PUk <- make_training(CCH, tsk_PUk)

list_DL  <- make_training(CCH, tsk_DL)
list_DLk <- make_training(CCH, tsk_DLk)

list_F2M  <- make_training(CCH, tsk_F2M)
list_F2Mk <- make_training(CCH, tsk_F2Mk)

```

## Variable selection

We create separate test and training datasets for each of the models. The training datasets are around 700 cases from the chart review, and the test datasets around 300. The variables included come from the ranger fits for each task.

```{r Variable selection for the two sets of models}
#list_XX[[2]] is the training data as a tibble.

df.train.PN <- list_PN[[2]] |>
  select(c('rowid', 'Weights', tsk_PN$target_names, tsk_PN$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Dental.Services,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.train.PNk <- list_PNk[[2]] |>
  select(c('rowid', 'Weights', tsk_PNk$target_names, tsk_PNk$feature_names))

df.train.UTI <- list_UTI[[2]] |>
  select(c('rowid', 'Weights', tsk_UTI$target_names, tsk_UTI$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Dental.Services,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.train.UTIk <- list_UTIk[[2]] |>
  select(c('rowid', 'Weights', tsk_UTIk$target_names, tsk_UTIk$feature_names))

df.train.PU <- list_PU[[2]] |>
  select(c('rowid', 'Weights', tsk_PU$target_names, tsk_PU$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Dental.Services,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.train.PUk <- list_PUk[[2]] |>
  select(c('rowid', 'Weights', tsk_PUk$target_names, tsk_PUk$feature_names)) |>
  select(-c(Dental.Services))

df.train.DL <- list_DL[[2]] |>
  select(c('rowid', 'Weights', tsk_DL$target_names, tsk_DL$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Dental.Services,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.train.DLk <- list_DLk[[2]] |>
  select(c('rowid', 'Weights', tsk_DLk$target_names, tsk_DLk$feature_names))

df.train.F2M <- list_F2M[[2]] |>
  select(c('rowid', 'Weights', tsk_F2M$target_names, tsk_F2M$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Dental.Services,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.train.F2Mk <- list_F2Mk[[2]] |>
  select(c('rowid', 'Weights', tsk_F2Mk$target_names, tsk_F2Mk$feature_names))
#
###################################################
#list_XX[[3]] is the test data as a tibble.

df.test.PN <- list_PN[[3]] |>
  select(c('rowid', 'Weights', tsk_PN$target_names, tsk_PN$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Dental.Services,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.test.PNk <- list_PNk[[3]] |>
  select(c('rowid', 'Weights', tsk_PNk$target_names, tsk_PNk$feature_names))

df.test.UTI <- list_UTI[[3]] |>
  select(c('rowid', 'Weights', tsk_UTI$target_names, tsk_UTI$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Dental.Services,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.test.UTIk <- list_UTIk[[3]] |>
  select(c('rowid', 'Weights', tsk_UTIk$target_names, tsk_UTIk$feature_names))

df.test.PU <- list_PU[[3]] |>
  select(c('rowid', 'Weights', tsk_PU$target_names, tsk_PU$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Dental.Services,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.test.PUk <- list_PUk[[3]] |>
  select(c('rowid', 'Weights', tsk_PUk$target_names, tsk_PUk$feature_names)) |>
  select(-c(Dental.Services))

df.test.DL <- list_DL[[3]] |>
  select(c('rowid', 'Weights', tsk_DL$target_names, tsk_DL$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Dental.Services,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.test.DLk <- list_DLk[[3]] |>
  select(c('rowid', 'Weights', tsk_DLk$target_names, tsk_DLk$feature_names))

df.test.F2M <- list_F2M[[3]] |>
  select(c('rowid', 'Weights', tsk_F2M$target_names, tsk_F2M$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Dental.Services,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.test.F2Mk <- list_F2Mk[[3]] |>
  select(c('rowid', 'Weights', tsk_F2Mk$target_names, tsk_F2Mk$feature_names))

```


```{r Make dataframes for HIPE applied to the 10 models}
## Outcomes 
HIPE.PN <- HIPE |>
  select(c('rowid', 'Weights', tsk_PN$feature_names))
HIPE.PNk <- HIPE |>
  select(c('rowid', 'Weights', tsk_PNk$feature_names))

HIPE.UTI <- HIPE |>
  select(c('rowid', 'Weights', tsk_UTI$feature_names))
HIPE.UTIk <- HIPE |>
  select(c('rowid', 'Weights', tsk_UTIk$feature_names))

HIPE.PU <- HIPE |>
  select(c('rowid', 'Weights', tsk_PU$feature_names))
HIPE.PUk <- HIPE |>
  select(c('rowid', 'Weights', tsk_PUk$feature_names))

HIPE.DL <- HIPE |>
  select(c('rowid', 'Weights', tsk_DL$feature_names))
HIPE.DLk <- HIPE |>
  select(c('rowid', 'Weights', tsk_DLk$feature_names))

HIPE.F2M <- HIPE |>
  select(c('rowid', 'Weights', tsk_F2M$feature_names))
HIPE.F2Mk <- HIPE |>
  select(c('rowid', 'Weights', tsk_F2Mk$feature_names))


saveRDS(HIPE.PN, file = "data/HIPE.PN")
saveRDS(HIPE.PNk, file = "data/HIPE.PNk")

saveRDS(HIPE.UTI, file = "data/HIPE.UTI")
saveRDS(HIPE.UTIk, file = "data/HIPE.UTIk")

saveRDS(HIPE.PU, file = "data/HIPE.PU")
saveRDS(HIPE.PUk, file = "data/HIPE.PUk")

saveRDS(HIPE.DL, file = "data/HIPE.DL")
saveRDS(HIPE.DLk, file = "data/HIPE.DLk")

saveRDS(HIPE.F2M, file = "data/HIPE.F2M")
saveRDS(HIPE.F2Mk, file = "data/HIPE.F2Mk")
```

```{r tidy up unwanted data}
rm( list = ls()[str_detect(ls(),'HIPE.')])
rm( list = ls()[str_detect(ls(),'tsk_')])
rm( list = ls()[str_detect(ls(),'list_')])

gc(reset = TRUE, full = TRUE)

```

# Bayesian fit

We use a common mildly informative prior.

```{r t_prior}
  t_prior <- student_t(df = 7, location = 0, scale = 2.5)
```

We fit the models (this takes some little time, as there are 10)

```{r Rstanarm model fits}
# ancillary functions
# 
# Create the formula needed for stan_glm
# 
make_formula <- function(df.TSK) {
  names <- names(df.TSK)
  N = length(df.TSK)
  Formula = paste( names[3], ' ~ ',
                   paste(c(names[4:N]),
                         collapse = ' + ')) # This is the formula needed for the fit
  return(Formula)
}
#
# Fit and save the desired model
#
fit.task <- function(df.train){
  model_name <- deparse(substitute(df.train))
  model_name <- str_split_i(model_name,
                            pattern = '\\.', # Cut on .
                            i = -1) # Last element

  fit <- stan_glm(make_formula(df.train),
                 data = df.train,
                 family = binomial(link = "logit"),
                 weights = Weights,
                 prior = t_prior, prior_intercept = t_prior,
                 cores = 6, seed = 12345,
                 chains = 6,
                 iter = 6000, warmup = 2000,
                 refresh = 0 ) # Quietly!

  saveRDS(fit, file = paste0('data/fit.', model_name))

return(fit)
}
fit.PN <- fit.task(df.train.PN)
fit.PNk <- fit.task(df.train.PNk)

fit.UTI <- fit.task(df.train.UTI)
fit.UTIk <- fit.task(df.train.UTIk)

fit.PU <- fit.task(df.train.PU)
fit.PUk <- fit.task(df.train.PUk)

fit.DL <- fit.task(df.train.DL)
fit.DLk <- fit.task(df.train.DLk)

fit.F2M <- fit.task(df.train.F2M)
fit.F2Mk <- fit.task(df.train.F2Mk)

```

# Model tests

First we run a series of pretty routine diagnostics for each of the 10 models

```{r diagnostic measures for each model}
kable(bayestestR::diagnostic_posterior(fit.PN), caption = 'Diagnostics for PN model')

kable(bayestestR::diagnostic_posterior(fit.PNk), caption = 'Diagnostics for PNk model')

kable(bayestestR::diagnostic_posterior(fit.UTI), caption = 'Diagnostics for UTI model')

kable(bayestestR::diagnostic_posterior(fit.UTIk), caption = 'Diagnostics for UTIk model')

kable(bayestestR::diagnostic_posterior(fit.PU), caption = 'Diagnostics for PU model')

kable(bayestestR::diagnostic_posterior(fit.PUk), caption = 'Diagnostics for PUk model')

kable(bayestestR::diagnostic_posterior(fit.DL), caption = 'Diagnostics for DL model')

kable(bayestestR::diagnostic_posterior(fit.DLk), caption = 'Diagnostics for DLk model')

kable(bayestestR::diagnostic_posterior(fit.F2M), caption = 'Diagnostics for F2M model')

kable(bayestestR::diagnostic_posterior(fit.F2Mk), caption = 'Diagnostics for F2Mk model')

kable(bayestestR::diagnostic_posterior(fit.PN), caption = 'Diagnostics for PN model')

kable(bayestestR::diagnostic_posterior(fit.PNk), caption = 'Diagnostics for PNk model')


```

Then we prepare a set of 10 dot and whisker plots to visually display the point estimates and the uncertainty (HDI) for each parameter in each model.

```{r Dot and Whisker plots of parameter estimates from each model}
make_dwPlot_fit <- function(fit) {
  model_name <- deparse(substitute(fit))
  model_name <-
    str_split_i(model_name,
              pattern = '\\.', # Cut on .
              i = -1) # Last element

pe <- point_estimate(fit, centrality = 'mean')
hdi <- ci(fit, method = 'hdi') |>
    full_join(pe,
              by = join_by(Parameter, Effects, Component)) |>
     rename(estimate = Mean) |>
     rename(term = Parameter) |>
     rename(conf.low = CI_low) |>
     rename(conf.high = CI_high)

Graph <- dwplot(hdi,  vline = geom_vline(
        xintercept = 0,
        colour = "red",
        linetype = 2), # plot line at zero _behind_ coefs
    whisker_args = list(size = 1, colour = 'green')) +
  theme_minimal() +
  labs( title = paste0('Parameter estimates for ', model_name),
        y = 'Item', x = 'Effect size') +
  guides(colour = 'none')

Graph

ggsave(filename =
      paste0('image/Dot_plot_fit_', model_name, '.pdf'),
       height = 10, width = 15, dpi = 1200)

return(Graph)
}

dw_PN   <- make_dwPlot_fit(fit.PN)
dw_PNk  <- make_dwPlot_fit(fit.PNk)

dw_UTI  <- make_dwPlot_fit(fit.UTI)
dw_UTIk <- make_dwPlot_fit(fit.UTIk)

dw_PU   <- make_dwPlot_fit(fit.PU)
dw_PUk  <- make_dwPlot_fit(fit.PUk)

dw_DL   <- make_dwPlot_fit(fit.DL)
dw_DLk  <- make_dwPlot_fit(fit.DLk)

dw_F2M  <- make_dwPlot_fit(fit.F2M)
dw_F2mk <- make_dwPlot_fit(fit.F2Mk)

rm( list = ls()[str_detect(ls(),'dw_.')])
```

We do live interactive testing of model fit, which proves satisfactory, suggesting good mixing of all chains, no issues with divergence, and no other striking anomalies.

```{r Rstanarm model checks in shinystan, eval = FALSE}
conflicted::conflicts_prefer(shiny::observe)

launch_shinystan(fit.PN, ppd = TRUE) # Allows me to check fit etc.
launch_shinystan(fit.PNk, ppd = TRUE) # Allows me to check fit etc.

launch_shinystan(fit.UTI, ppd = TRUE) # Allows me to check fit etc.
launch_shinystan(fit.UTIk, ppd = TRUE) # Allows me to check fit etc.

launch_shinystan(fit.PU, ppd = TRUE) # Allows me to check fit etc.
launch_shinystan(fit.PUk, ppd = TRUE) # Allows me to check fit etc.

launch_shinystan(fit.DL, ppd = TRUE) # Allows me to check fit etc.
launch_shinystan(fit.DLk, ppd = TRUE) # Allows me to check fit etc.

launch_shinystan(fit.F2M, ppd = TRUE) # Allows me to check fit etc.
launch_shinystan(fit.F2Mk, ppd = TRUE) # Allows me to check fit etc.

```

# Predictions

We use the epred predictions, which are the linear predictions, transformed by the invlogit function, and so directly represent probabilities.

## Training data predictions

```{r Collect predictions - training data}

TRAIN_Predicted <- bind_rows(

  epred_draws(fit.PN, df.train.PN,
          ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'PN'),
  epred_draws(fit.PNk, df.train.PNk,
          ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'PNk'),
  
  epred_draws(fit.UTI, df.train.UTI,
          ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'UTI'),
  epred_draws(fit.UTIk, df.train.UTIk,
          ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'UTIk'),
  
    epred_draws(fit.PU, df.train.PU,
          ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'PU'),
  epred_draws(fit.PUk, df.train.PUk,
          ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'PUk'),
  
    epred_draws(fit.DL, df.train.DL,
          ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'DL'),
  epred_draws(fit.DLk, df.train.DLk,
          ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'DLk'),
  
    epred_draws(fit.F2M, df.train.F2M,
          ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'F2M'),
  epred_draws(fit.F2Mk, df.train.F2Mk,
        ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'F2Mk'),
  
  )
nrow(TRAIN_Predicted)/nrow(df.train.PNk)

rm( list = ls()[str_detect(ls(),'df.train.')])
```

## Test data predictions

```{r Collect predictions - test data}

TEST_Predicted <- bind_rows(

  epred_draws(fit.PN, df.test.PN,
        ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'PN'),
  epred_draws(fit.PNk, df.test.PNk,
        ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'PNk'),
  
  epred_draws(fit.UTI, df.test.UTI,
        ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'UTI'),
  epred_draws(fit.UTIk, df.test.UTIk,
        ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'UTIk'),
  
    epred_draws(fit.PU, df.test.PU,
          ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'PU'),
  epred_draws(fit.PUk, df.test.PUk,
        ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'PUk'),
  
    epred_draws(fit.DL, df.test.DL,
          ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'DL'),
  epred_draws(fit.DLk, df.test.DLk,
        ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'DLk'),
  
    epred_draws(fit.F2M, df.test.F2M,
          ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'F2M'),
  epred_draws(fit.F2Mk, df.test.F2Mk,
        ndraws = 2000, value = '.prediction') |>
    ungroup() |>
    select(1:3, .row, .draw, .prediction) |>
      rename(Result = 3) |>
    mutate(Outcome = 'F2Mk'),
  
  )
nrow(TEST_Predicted)/nrow(df.test.PNk)

rm( list = ls()[str_detect(ls(),'df.test.')])
```

## Interpret predictions

We have 2000 estimates for each of 10 models of the predicted value of the probability of the outcome (PN, UTI, ... F2M), derived from the models we have fitted, both for the training datasets, and, more relevantly, for the test datasets. This ought to be sufficient for posterior checking.

For the Chart review data (training and test datasets) we also have the ground truth, in a variable 'Observed'. Clearly we won't have this for the HIPE data.

We want three things (for each model)

* Summary tables of the predicted and observed data

```{r Summary tables  of observed, and of predictions test and train}
#########################################
# summary of chart review
Summary.CHART <- CCH |>
  select(PN:F2M) |>
  mutate(across(everything(),
                ~ ifelse( . == 'Yes', 1, 0))) |>
  pivot_longer(cols = everything(),
               names_to = 'Outcome',
               values_to = 'Observed') |>
  group_by(Outcome) |>
  summarise(across(Observed, list(mean = mean, median = median, sd = stats::sd))) # 5 values

kable(Summary.CHART, caption = 'Summary of observed outcomes for  chart review')

#########################################
# Add (identical) rows for the restricted models
SCR <- Summary.CHART |>
  mutate(Outcome = str_replace(Outcome, '$', 'k')) # Add 'k' on to outcome names to indicate restricted models

Summary.CHART <- Summary.CHART |>
  bind_rows(SCR) |>
  arrange(Outcome) # 10 Values
rm(SCR)

#########################################
# Summary of training predictions
Summary.TRAIN <- TRAIN_Predicted |>
  select(Outcome, .prediction) |>
  group_by(Outcome) |>
  summarise(across(.prediction, list(mean = mean, median = median, sd = stats::sd))) # 10 values

kable(Summary.TRAIN, caption = 'Summary of modelled predictions from TRAINING data for chart review HIPE')

#########################################
# Summary of test predictions
Summary.TEST <- TEST_Predicted |>
  select(Outcome, .prediction) |>
  group_by(Outcome) |>
  summarise(across(.prediction, list(mean = mean, median = median, sd = stats::sd))) # 10 values

kable(Summary.TEST, caption = 'Summary of modelled predictions from TEST data for chart review HIPE')

```

This tells us a number of things. The events under consideration are relatively uncommon, with prevalences mostly under 10$% in the chart review. The predictions on the training data are pretty good, which is to be expected.

The predictions on the test data are also pretty good, which suggests that the modelling has worked reasonably well, and that it is possible to estimate the prevalence of these hospital acquired complications from features derived directly from routine data.

### TRAINING and TEST data plots

* A plot of the distribution of the predictions, compared with the observed data

```{r Plot of predictions - training, fig.cap= 'Distribution of predicted individual risk estimates for adverse outcomes in the TRAINING data. The mean predicted risk for each outcome is a vertical line, the same colour as that outcome. The observed risk is shown as a green vertical line.', fig.dpi=1200, fig.height=7, fig.width=7}
G_TRAIN_Predicted = ggplot(TRAIN_Predicted,
       aes(y = Result, x = .prediction, colour = Outcome)) +
  geom_violin() +
  geom_vline(data = Summary.TRAIN,
             mapping = aes(xintercept = .prediction_mean, colour = Outcome)) +
  geom_vline(data = Summary.CHART,
             aes(xintercept = Observed_mean), colour = 'green') +
  theme_minimal() +
  guides(colour = 'none') +
  labs(title = 'Predictions on TRAINING data',
       x = 'Prediction') +
    coord_cartesian(xlim = c(0,0.3)) +
  facet_wrap( ~Outcome ) 
G_TRAIN_Predicted

ggsave('image/G_TRAIN_Predicted.pdf', G_TRAIN_Predicted, units = 'cm', height = 20, width = 20)

rm(G_TRAIN_Predicted)
```

```{r Plot of predictions - TEST, fig.cap= 'Distribution of predicted individual risk estimates for adverse outcomes in the TEST data. The mean predicted risk for each outcome is a vertical line, the same colour as that outcome. The observed risk is shown as a green vertical line.', fig.dpi=1200, fig.height=7, fig.width=7}
G_TEST_Predicted = ggplot(TEST_Predicted,
       aes(y = Result, x = .prediction, colour = Outcome)) +
  geom_violin() +
  geom_vline(data = Summary.TEST,
             mapping = aes(xintercept = .prediction_mean, colour = Outcome)) +
  geom_vline(data = Summary.CHART,
             aes(xintercept = Observed_mean), colour = 'green') +
  theme_minimal() +
  guides(colour = 'none') + 
  labs(title = 'Predictions on TEST data',
       x = 'Prediction') +
    coord_cartesian(xlim = c(0,0.3)) +
  facet_wrap( ~Outcome )
G_TEST_Predicted

ggsave('image/G_TEST_Predicted.pdf', G_TEST_Predicted, units = 'cm', height = 20, width = 20)

rm(G_TEST_Predicted)
```


```{r Clean up by running gc again}

rm(TEST_Predicted, TRAIN_Predicted)
rm( list = ls()[str_detect(ls(),'fit.')])

gc(reset = TRUE, full = TRUE)
```

### HIPE predictions

We draw 2,000 predictions of epred (which is the probability of having an adverse event) for each of the 10 models.

```{r Prepare HIPE predictions}
#
probs <- c(0.10, 0.5, 0.90)
#
# Ancillary function to make and save the predictions
#
make_Predictions <- function (name, probs = c(0.10, 0.5, 0.90)) {
  fit = paste0('fit.',name)
  HIPE = paste0('HIPE.',name)

fit   <- readRDS(paste0('data/fit.',name))
HIPE  <- readRDS(paste0('data/HIPE.',name))

EP <- posterior_epred(object = fit,
                  draws = 2000,
                newdata = HIPE)

  EP.MAD <- colMads(EP)
    names(EP.MAD) <- NULL
  EP.Quantiles <- colQuantiles(EP,
                      probs = probs)

HIPE <- HIPE |>
  bind_cols(EP.Quantiles) |>
  bind_cols(MAD = EP.MAD) |>
  rename(Q10 = '10%') |>
  rename(Q50 = '50%') |>
  rename(Q90 = '90%') |>
  mutate(Outcome = name) |>
  bind_cols(as.data.frame(t(EP)))

saveRDS(HIPE, paste0('data/Predictions_',name,'.Rds'))

return(str(HIPE))
}

str_PN <- make_Predictions('PN')
str_PNk <- make_Predictions('PNk')

str_UTI <- make_Predictions('UTI')
str_UTIk <- make_Predictions('UTIk')

str_PU <- make_Predictions('PU')
str_PUk <- make_Predictions('PUk')

str_DL <- make_Predictions('DL')
str_DLk <- make_Predictions('DLk')

str_F2M <- make_Predictions('F2M')
str_F2Mk <- make_Predictions('F2Mk')

rm(probs)
rm( list = ls()[str_detect(ls(),'str_')])
rm( list = ls()[str_detect(ls(),'fit.')])
```

```{r Merge HIPE predictions, eval = FALSE}
gc(reset = TRUE, full = TRUE)

if(file_test('-f', 'data/HIPE.Predicted.Rds')) {

  cat('HIPE.Predicted exists, and is ',
      format(file.size('data/HIPE.Predicted.Rds'), scientific = TRUE),
      ' in size.')
    HIPE.Predicted <- readRDS('data/HIPE.Predicted.Rds')
  
  } else {

  HIPE.Predicted <-
  readRDS(file = 'data/Predictions_PN.Rds') |>
  select(c(rowid,Q10:Outcome)) |>
  bind_rows(readRDS(file = 'data/Predictions_PNk.Rds') |>
  select(c(rowid,Q10:Outcome))) |>
  bind_rows(readRDS(file = 'data/Predictions_UTI.Rds') |>
  select(c(rowid,Q10:Outcome))) |>
  bind_rows(readRDS(file = 'data/Predictions_UTIk.Rds') |>
  select(c(rowid,Q10:Outcome))) |>
  bind_rows(readRDS(file = 'data/Predictions_PU.Rds') |>
  select(c(rowid,Q10:Outcome))) |>
  bind_rows(readRDS(file = 'data/Predictions_PUk.Rds') |>
  select(c(rowid,Q10:Outcome))) |>
  bind_rows(readRDS(file = 'data/Predictions_DL.Rds') |>
  select(c(rowid,Q10:Outcome))) |>
  bind_rows(readRDS(file = 'data/Predictions_DLk.Rds') |>
  select(c(rowid,Q10:Outcome))) |>
  bind_rows(readRDS(file = 'data/Predictions_F2M.Rds') |>
  select(c(rowid,Q10:Outcome))) |>
  bind_rows(readRDS(file = 'data/Predictions_F2Mk.Rds') |>
  select(c(rowid,Q10:Outcome)))

    saveRDS(HIPE.Predicted, file = 'data/HIPE.Predicted.Rds')
    rm(HIPE.Predicted)
  }

```

```{r Summarise HIPE predictions overall}
#########################################
# Summary of HIPE predictions
#
HIPE.Predicted |>
    select(Outcome, Q50) |>
        group_by(Outcome) |>
        summarise(across(Q50,
        list(mean = mean, median = median,
             sd = stats::sd)))
kable(Summary.HIPE, caption = 'Summary of modelled predictions for full HIPE')

```

```{r Plot of all hospitals}

Graph <- ggplot(HIPE.Predicted,
       aes(x = Q50, fill = Outcome, colour = Outcome )) +
  stat_halfeye(colour = 'darkblue',
               normalize = 'panels') +
  geom_vline(data = Summary.HIPE,
             aes(xintercept = Q50_mean),
                 colour = 'red') +
  geom_vline(data = Summary.CHART,
             aes(xintercept = Observed_mean),
             colour = 'green') +
  theme_minimal() +
  guides(colour = 'none') + 
  guides(fill = 'none') + 
  labs(x = 'Predicted probability', y = 'Density') +
    coord_cartesian(xlim = c(0,0.3)) +
    facet_wrap(~ Outcome)
Graph
ggsave(filename = 'HIPE_density_predictions.pdf', Graph, dpi = 1200, height= 7, width = 10)

rm( list = ls()[str_detect(ls(),'pg_')])
rm(Graph)
```

```{r, eval = FALSE} 
summarytools::ctable(Predictions$PN, Predictions$.prediction, prop = 't')
ggplot(Predictions, aes(x = .epred, y = t.linpred)) + geom_point() # All identical
```

```{r HIPE breakdown}
#########################################
# HIPE predictions data
#
HIPE.Codes <- HIPE |>
    select(rowid, ModelF, HospCode)

make_summary_row <- function(name, CODES = HIPE.Codes) {
    filename = paste0('data/Predictions_', name, '.Rds')
    ROWs <-  readRDS(filename) |>
        full_join(CODES, by = join_by(rowid)) |>
        select(ModelF, Outcome, Q50) |>
        group_by(ModelF, Outcome) |>
        summarise(across(Q50,
        list(mean = mean, median = median,
             sd = stats::sd)))
return(ROWs)
}

HIPE_PN <- make_summary_row('PN')
HIPE_PNk <- make_summary_row('PNk')
    
HIPE_UTI <- make_summary_row('UTI')
HIPE_UTIk <- make_summary_row('UTIk')
    
HIPE_PU <- make_summary_row('PU')
HIPE_PUk <- make_summary_row('PUk')
    
HIPE_DL <- make_summary_row('DL')
HIPE_DLk <- make_summary_row('DLk')
    
HIPE_F2M <- make_summary_row('F2M')
HIPE_F2Mk <- make_summary_row('F2Mk')
    

#########################################
# Summary of test predictions
Summary.Model.HIPE <- bind_rows(
    HIPE_PN, HIPE_PNk,
    HIPE_UTI, HIPE_UTIk,
    HIPE_PU, HIPE_PUk,
    HIPE_DL, HIPE_DLk,
    HIPE_F2M, HIPE_F2Mk)
    
kable(Summary.Model.HIPE, caption = 'Summary of modelled predictions for full HIPE')

rm( list = ls()[str_detect(ls(),'HIPE_')])
rm(HIPE.Codes)
```

We see a striking difference between the Model 3 and the Model 4 hospitals, with higher predicted adverse event rates in the Model 4 hospitals. Given the casemix they see, this is not surprising. The chart review was done in one Model 4 hospital, and accordingly has higher observed adverse event rates than the overall predicted rates.


```{r}

HIPE.Codes <- HIPE |>
    select(rowid, ModelF, HospCode) 
HIPE.Predicted <- HIPE.Predicted |>
        full_join(HIPE.Codes, by = join_by(rowid))

ggplot(HIPE.Predicted,
       aes(x = Q50, fill = Outcome, colour = Outcome)) +
    stat_halfeye(colour = 'darkblue',
               normalize = 'panels') +
#  geom_density(data = HIPE.Predicted |> filter(ModelF == 'Model 3'), linewidth = 2) +
  geom_vline(data = Summary.Model.HIPE |>
               filter(ModelF == 'Model 3'),
             mapping = aes(xintercept = Q50_mean,
                           colour = Outcome)) +
  geom_vline(data = Summary.CHART,
             aes(xintercept = Observed_mean), colour = 'green') +
  theme_minimal() +
  guides(colour = 'none') + 
  guides(fill = 'none') + 
  labs(title = 'Predictions on HIPE data - Model 3 hospitals only',
       x = 'Prediction', y = 'Density') +
    coord_cartesian(xlim = c(0,0.3)) +
  facet_wrap( ~Outcome )

ggplot(HIPE.Predicted,
       aes(x = Q50, fill = Outcome, colour = Outcome)) +
    stat_halfeye(colour = 'darkblue',
               normalize = 'panels') +
#  geom_density(data = HIPE.Predicted |> filter(ModelF == 'Model 4'), linewidth = 2) +
  geom_vline(data = Summary.Model.HIPE |>
               filter(ModelF == 'Model 4'),
             mapping = aes(xintercept = Q50_mean,
                           colour = Outcome)) +
  geom_vline(data = Summary.CHART,
             aes(xintercept = Observed_mean), colour = 'green') +
  theme_minimal() +
  guides(colour = 'none') + 
  guides(fill = 'none') + 
  labs(title = 'Predictions on HIPE data - Model 4 hospitals only',
       x = 'Prediction', y = 'Density') +
    coord_cartesian(xlim = c(0,0.3)) +
  facet_wrap( ~Outcome )

```

Each graph compares the results from the chart review with the predicted results from the model for Model 3 and Model 4 hospitals, respectively. The predictions for Model 4 hospitals, except those for UTI, are closer to the Chart review ground truth than those for Model 3 centres. As the hospital used in the Chart review study was a Model 4 hospital, this is not surprising. Given the observed area under the ROC curve, just over 0.6, for the UTI model, our view is that it is not reliable.


```{r, eval = FALSE}
pp_check(fit.PNk)
pp_check(fit.PNk, plotfun = "boxplot", nreps = 10, notch = FALSE)
pp_check(fit.PNk, plotfun = "hist", nreps = 10)

rstan::stan_plot(fit.PNk, show_density=TRUE)
rstan::stan_plot(fit.PNk, pars = get_variables(fit.PNk)[2:28], show_density=TRUE)

plot(loo(fit.PNk))

performance::model_performance(fit.PNk)
```

# What matters?

This is not a classification problem. We are trying to estimate the prevalence, that is the probability that a random person discharged will have developed hospital acquired pneumonia. In terms of our models, this is just the transformed linear predictor, or the .epred.

```{r, eval = FALSE}
EPred.PNk.h <- posterior_epred(fit.PNk,
                              newdata = HIPE) # Probabilities
# 4000 rows of one column per record

EPred.PNk.ht <- as_tibble(t(EPred.PNk.h))

# 131478 rows of 4,000 draws in columns

Median <- EPred.PNk.ht |>  summarise(across(everything(), median))
Epred.PNk <- Median |> pivot_longer(cols = everything(), names_to = 'Row', values_to = 'Median')

summary(Epred.PN)

ggplot(Epred.PNk, aes(x = round(100*Median,2))) + geom_density()

```
 
