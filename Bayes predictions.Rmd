---
title: "Bayes predictions"
author: "Anthony Staines"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: true
    fig_caption: true
    number_sections: true
    latex_engine: xelatex
  word_document: default
  html_document: default
editor_options:
  chunk_output_type: console
bibliography: references.bib
csl: vancouver.csl
---

```{r setup, include=FALSE}

rm(list = ls())

library(rstanarm)

library(tidyverse)
library(bayesplot)
library(tidybayes)
library(lubridate)
library(tibble)
library(tidymodels)
library(readxl)

library(lme4)

library(knitr)
library(kableExtra)
library(summarytools)

library(patchwork)

library(ranger)
library(mlr3verse)
library(mlr3viz)

library(data.table)
library(future)

library(sjPlot)
library(sjtable2df)

library(broom)
library(broom.mixed)

library(stargazer)

library(gt)
library(gtsummary)

tidymodels_prefer(quiet = TRUE)
st_options(ctable.round.digits = 2)

#How many CPU's?
N = parallel::detectCores()
  options(Ncpus = N - 1)
  options(mc.cores = N - 1)
  setDTthreads(threads = N - 1,
               restore_after_fork = TRUE,
               throttle = 1024)

options(dplyr.summarise.inform = FALSE, # shut up summarise
        ranger.num.threads = N - 1) # Prepare for rf models

knitr::opts_chunk$set(echo = FALSE,
                      cache = TRUE, cache.lazy = FALSE, 
                      warning = NA, message = NA, fig.pos = 'H',
                      cache.extra = knitr::rand_seed)
```

# Logistic models for adverse outcomes in HIPE

The idea here is to fit a completely different model - a logistic model fitted using rstanarm (or STAN, if I feel confident enough), using the variables selected by the random forest classifiers in MLR3, and see where we get to.

# data

```{r Load existing data}
HIPE <- readRDS('data/HIPE.Rds')
CCH <- readRDS('data/CCH.Rds')
glimpse(CCH)

# STAN does not like character variables, so we turn them into factors (No =  0, and  Yes = 1, more or less.

CCH <- CCH |>
  mutate(across(where(is.character), ~as_factor(.)))  |>
  rowid_to_column(var = 'rowid')
glimpse(CCH)

HIPE <- HIPE |>
  mutate(across(where(is.character), ~as_factor(.)))
glimpse(HIPE)

table(CCH$PN) # 79 positive out of 1,000

TruePlot <- ggplot(CCH |> arrange(PN),
       aes(y = PN, x = rowid, colour = PN)) +
  geom_jitter() +
  guides(colour = 'none') +
  labs(title = "Observed data from chart review",
       x = '',
       y = 'Pneumonia')
TruePlot
```

Split CCH into training and test sets

```{r Train test split model}
#Split CCH into training and test sets
#
set.seed(18237497)
splits      <- initial_split(CCH, strata = PN)

CCH_train <- training(splits)
CCH_test  <- testing(splits)

t.train <- table(CCH_train$PN)
t.train
  pTrainYes <-  t.train[[1]]/(t.train[[1]] + t.train[[2]])
  pTrainNo <-  t.train[[2]]/(t.train[[1]] + t.train[[2]])
pTrainNo
pTrainYes

t.test <- table(CCH_test$PN)
t.test
  pTestYes <-  t.test[[1]]/(t.test[[1]] + t.test[[2]])
  pTestNo <-  t.test[[2]]/(t.test[[1]] + t.test[[2]])
pTestNo
pTestYes

```

## Load tasks
We're fitting a series of models where we've already done the variable selection with a bunch of random forest models.
Here we load the tasks to get at the chosen variables, and pull the variables out. Neither the weighting variable, Weights, nor the adverse outcome (PN, UTI, ...) is a feature in the tasks.

```{r Load tasks}
tsk_PN   <- readRDS('data/tsk.PN')
tsk_PNk  <- readRDS('data/tsk.PNk')
tsk_UTI  <- readRDS('data/tsk.UTI')
tsk_UTIk <- readRDS('data/tsk.UTIk')
tsk_PU   <- readRDS('data/tsk.PU')
tsk_PUk  <- readRDS('data/tsk.PUk')
tsk_DL   <- readRDS('data/tsk.DL')
tsk_DLk  <- readRDS('data/tsk.DLk')
tsk_F2M  <- readRDS('data/tsk.F2M')
tsk_F2Mk <- readRDS('data/tsk.F2Mk')
```

### What do the tasks contain?

```{r process tasks}
# What information have we got on each task?
# 
tsk_F2Mk$feature_names # List of names of variables
weights <- tsk_F2M$weights # Dataframe of row.id and weight
tsk_F2M$target_names # One character variable

#Check alignment of weights
CCH |>
  select(Weights, rowid) |>
  full_join(weights, by = join_by(rowid == row_id)) |>
  mutate(NotEqual = ifelse(Weights == weight,0,1)) |>
  filter(NotEqual == 1) # 0 entries
# alignment is correct, weights are identical.


CCH |>
  select(c('rowid', 'Weights', tsk_PNk$target_names, tsk_PNk$feature_names)) |>
  names() # this is how we subset for each task

```

### What's where?

This checks that all the features used are in the datframe CCH, and that there are no extra features we didn't know about.

```{r}
LIST = names(CCH)
NAMES <- as_tibble(LIST) |>
  rename(NAME = value) 

PN <- as_tibble(tsk_PN$feature_names) |>
      rename(PN = value)
PNk <- as_tibble(tsk_PNk$feature_names) |>
      rename(PNk = value)
UTI <- as_tibble(tsk_UTI$feature_names) |>
      rename(UTI = value)
UTIk <- as_tibble(tsk_UTIk$feature_names) |>
      rename(UTIk = value)
PU <- as_tibble(tsk_PU$feature_names) |>
      rename(PU = value)
PUk <- as_tibble(tsk_PUk$feature_names) |>
      rename(PUk = value)
DL <- as_tibble(tsk_DL$feature_names) |>
      rename(DL = value)
DLk <- as_tibble(tsk_DLk$feature_names) |>
      rename(DLk = value)
F2M <- as_tibble(tsk_F2M$feature_names) |>
      rename(F2M = value)
F2Mk <- as_tibble(tsk_F2Mk$feature_names) |>
      rename(F2Mk = value)


NAMES <- NAMES |>
  full_join(PN,
    by = join_by(NAME == PN),
    suffix = c('','.PN'),
    keep = TRUE) |>
  full_join(PNk,
    by = join_by(NAME == PNk),
    suffix = c('','.PNk'),
    keep = TRUE) |>
  full_join(UTI,
    by = join_by(NAME == UTI),
    suffix = c('','.UTI'),
    keep = TRUE) |>
  full_join(UTIk,
    by = join_by(NAME == UTIk),
    suffix = c('','.UTIk'),
    keep = TRUE) |>
  full_join(PU,
    by = join_by(NAME == PU),
    suffix = c('','.PU'),
    keep = TRUE) |>
  full_join(PUk,
    by = join_by(NAME == PUk),
    suffix = c('','.PUk'),
    keep = TRUE) |>
  full_join(DL,
    by = join_by(NAME == DL),
    suffix = c('','.DL'),
    keep = TRUE) |>
  full_join(DLk,
    by = join_by(NAME == DLk),
    suffix = c('','.DLk'),
    keep = TRUE) |>
  full_join(F2M,
    by = join_by(NAME == F2M),
    suffix = c('','.F2M'),
    keep = TRUE) |>
  full_join(F2Mk,
    by = join_by(NAME == F2Mk),
    suffix = c('','.F2Mk'),
    keep = TRUE)


```

## Check variability

Some of the variables are mostly 0 or mostly 1. It can happen that in the training, or test set, these are all 1 or zero. These need to be omitted.

```{r check variability}
CCH_variability = 

CCH_train |>
  mutate(across(everything(), ~ length(unique(.)))) |>
  distinct() |>
  pivot_longer(cols = everything(), names_to = 'Name', values_to = 'Count') |>
filter(Count == 1)

CCH_test |>
  mutate(across(everything(), ~ length(unique(.)))) |>
  distinct() |>
  pivot_longer(cols = everything(), names_to = 'Name', values_to = 'Count') |>
filter(Count == 1)

# Two variables 
#  CCH$Client.Support.Interventions
#  CCH$Procedures.On.Ear.And.Mastoid.Process
# have no occurrences in the Chart Review data set
# Two more
#  CCH$Procedures.On.Endocrine.System and
#  CCH$Radiation.Oncology.Procedures
# have only one, and hence cannot be considered!
```

### Variable selection

These come from the ranger fits.

```{r Variable selection}
df.PN <- CCH |>
  select(c('rowid', 'Weights', tsk_PN$target_names, tsk_PN$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.PNk <- CCH |>
  select(c('rowid', 'Weights', tsk_PNk$target_names, tsk_PNk$feature_names))

df.UTI <- CCH |>
  select(c('rowid', 'Weights', tsk_UTI$target_names, tsk_UTI$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.UTIk <- CCH |>
  select(c('rowid', 'Weights', tsk_UTIk$target_names, tsk_UTIk$feature_names))

df.PU <- CCH |>
  select(c('rowid', 'Weights', tsk_PU$target_names, tsk_PU$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.PUk <- CCH |>
  select(c('rowid', 'Weights', tsk_PUk$target_names, tsk_PUk$feature_names))

df.DL <- CCH |>
  select(c('rowid', 'Weights', tsk_DL$target_names, tsk_DL$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.DLk <- CCH |>
  select(c('rowid', 'Weights', tsk_DLk$target_names, tsk_DLk$feature_names))

df.F2M <- CCH |>
  select(c('rowid', 'Weights', tsk_F2M$target_names, tsk_F2M$feature_names)) |>
  select(-c(Client.Support.Interventions,
            Procedures.On.Ear.And.Mastoid.Process,
            Procedures.On.Endocrine.System,
            Radiation.Oncology.Procedures))

df.F2Mk <- CCH |>
  select(c('rowid', 'Weights', tsk_F2Mk$target_names, tsk_F2Mk$feature_names))

```

# Bayesian fit

We use a common mildly informative prior.

```{r t_prior}
  t_prior <- student_t(df = 7, location = 0, scale = 2.5)
```

We fit the models (this takes some little time, as there are 10)

```{r Rstanarm model fits}
# ancillary function
make_formula <- function(df.TSK) {
  names <- names(df.TSK)
  N = length(df.TSK)
  Formula = paste( names[3], ' ~ ',
                   paste(c(names[4:N]),
                         collapse = ' + '))
  return(Formula)
}

fit.PN <- stan_glm(make_formula(df.PN),
                 data = df.PN,
                 family = binomial(link = "logit"),
                 weights = Weights,
                 prior = t_prior, prior_intercept = t_prior,
                 cores = 4, seed = 12345)
fit.PNk <- stan_glm(make_formula(df.PNk),
                 data = df.PNk,
                 family = binomial(link = "logit"),
                 weights = CCH$Weights,
                 prior = t_prior, prior_intercept = t_prior,
                 cores = 4, seed = 12345)

fit.UTI <- stan_glm(make_formula(df.UTI),
                 data = df.UTI,
                 family = binomial(link = "logit"),
                 weights = Weights,
                 prior = t_prior, prior_intercept = t_prior,
                 cores = 4, seed = 12345)
fit.UTIk <- stan_glm(make_formula(df.UTIk),
                 data = df.UTIk,
                 family = binomial(link = "logit"),
                 weights = CCH$Weights,
                 prior = t_prior, prior_intercept = t_prior,
                 cores = 4, seed = 12345)

fit.PU <- stan_glm(make_formula(df.PU),
                 data = df.PU,
                 family = binomial(link = "logit"),
                 weights = Weights,
                 prior = t_prior, prior_intercept = t_prior,
                 cores = 4, seed = 12345)
fit.PUk <- stan_glm(make_formula(df.PUk),
                 data = df.PUk,
                 family = binomial(link = "logit"),
                 weights = CCH$Weights,
                 prior = t_prior, prior_intercept = t_prior,
                 cores = 4, seed = 12345)

fit.DL <- stan_glm(make_formula(df.DL),
                 data = df.DL,
                 family = binomial(link = "logit"),
                 weights = Weights,
                 prior = t_prior, prior_intercept = t_prior,
                 cores = 4, seed = 12345)
fit.DLk <- stan_glm(make_formula(df.DLk),
                 data = df.DLk,
                 family = binomial(link = "logit"),
                 weights = CCH$Weights,
                 prior = t_prior, prior_intercept = t_prior,
                 cores = 4, seed = 12345)

fit.F2M <- stan_glm(make_formula(df.F2M),
                 data = df.F2M,
                 family = binomial(link = "logit"),
                 weights = Weights,
                 prior = t_prior, prior_intercept = t_prior,
                 cores = 4, seed = 12345)
fit.F2Mk <- stan_glm(make_formula(df.F2Mk),
                 data = df.F2Mk,
                 family = binomial(link = "logit"),
                 weights = CCH$Weights,
                 prior = t_prior, prior_intercept = t_prior,
                 cores = 4, seed = 12345)

```

We do live interactive testing of model fit

```{r Rstanarm model checks in shinystan}
conflicted::conflicts_prefer(shiny::observe)

launch_shinystan(fit.PN, ppd = TRUE) # Allows me to check fit etc.
launch_shinystan(fit.PNk, ppd = TRUE) # Allows me to check fit etc.

launch_shinystan(fit.UTI, ppd = TRUE) # Allows me to check fit etc.
launch_shinystan(fit.UTIk, ppd = TRUE) # Allows me to check fit etc.

launch_shinystan(fit.PU, ppd = TRUE) # Allows me to check fit etc.
launch_shinystan(fit.PUk, ppd = TRUE) # Allows me to check fit etc.

launch_shinystan(fit.DL, ppd = TRUE) # Allows me to check fit etc.
launch_shinystan(fit.DLk, ppd = TRUE) # Allows me to check fit etc.

launch_shinystan(fit.F2M, ppd = TRUE) # Allows me to check fit etc.
launch_shinystan(fit.F2Mk, ppd = TRUE) # Allows me to check fit etc.

```


```{r Collect predictions}
TEST <- epred_draws(fit.DL, CCH_train, value = '.prediction') |>
  ungroup() |>
  select(rowid, DL, .row, .draw, .prediction)
```

```{r Confusion matrices for test and trained}

Observations_trained = nrow(CCH_train)
Observations_test = nrow(CCH_test)

Confusions_train <- CCH_train |> 
  add_predicted_draws(fit.PNk) |>
  ungroup() |>
  select(.draw, PN, .prediction) |>
  group_by(PN, .prediction, .draw) |>
  summarise( N = n()) |>
  arrange(.draw, PN, .prediction) |>
  select(.draw, PN, .prediction, N) |>
  mutate(P = N/Observations_trained) |>
  mutate(Type = case_when(
    PN == 'No'  & .prediction == 0 ~ 'TN',
    PN == 'No'  & .prediction == 1 ~ 'FP',
    PN == 'Yes' & .prediction == 0 ~ 'FN',
    PN == 'Yes' & .prediction == 1 ~ 'TP',  )) |>
  mutate(Prediction = case_when(
    .prediction == 0 ~ 'N',
    .prediction == 1 ~ 'P',  )) |>
  ungroup()

ggplot(Confusions_train,
       aes(x = Type, y = P)) +
  geom_boxplot()

ggplot(Confusions,
       aes(x = Type, y = N)) +
  geom_boxplot()

# 4,000 confusion matrices!

  Confusions_train |> filter(Type == 'TN') |> summary()
  Confusions_train |> filter(Type == 'FN') |> summary()
  Confusions_train |> filter(Type == 'FP') |> summary()
  Confusions_train |> filter(Type == 'TP') |> summary()

  Confusions_train |>
    group_by(Prediction, .draw) |>
    summarise(P = sum(P)) |>
    arrange(.draw, Prediction) |>
    ggplot(aes(y = Prediction,
               x = P,
               colour = Prediction, fill = Prediction)) +
  geom_violin() +
    geom_vline(xintercept = c(pTrainNo, pTrainYes),
               colour = c('blue', 'blue'),
               linewidth = 1.2) +
  guides(colour = 'none') +
  labs(title = "Predictions for *training data* from chart review",
       subtitle = "Observed training data probability shown as vertical lines",
       x = 'Estimated probability',
       y = 'Pneumonia') +
    theme_minimal() +
    scale_fill_viridis_d(option = 'G') +
    scale_colour_viridis_d(option = 'G')

```

The predictions on the test data are pretty good, but lower than on the training data.


```{r Test data}

Observations_test = nrow(CCH_test)

Confusions <- CCH_test |> 
  add_predicted_draws(object = fit.PNk) |>
  ungroup() |>
  select(.draw, PN, .prediction) |>
  group_by(PN, .prediction, .draw) |>
  summarise( N = n()) |>
  arrange(.draw, PN, .prediction) |>
  select(.draw, PN, .prediction, N) |>
  mutate(P = N/Observations_test) |>
  mutate(Type = case_when(
    PN == 'No'  & .prediction == 0 ~ 'TN',
    PN == 'No'  & .prediction == 1 ~ 'FP',
    PN == 'Yes' & .prediction == 0 ~ 'FN',
    PN == 'Yes' & .prediction == 1 ~ 'TP',  )) |>
  mutate(Prediction = case_when(
    .prediction == 0 ~ 'No',
    .prediction == 1 ~ 'Yes',  )) |>
  ungroup() |>
  select(-c(PN, .prediction))

Confusions |> group_by(.draw) |> summarise(N = n()) |> filter((N !=4)) # 1277

  ggplot(Confusions, aes(x = Type, y = P)) + geom_boxplot()
  ggplot(Confusions, aes(x = Type, y = N)) + geom_boxplot()

# 4,000 confusion matrices on test data!

  Confusions |> filter(Type == 'TN') |> summary(P)
  Confusions |> filter(Type == 'FN') |> summary(P)
  Confusions |> filter(Type == 'FP') |> summary(P)
  Confusions |> filter(Type == 'TP') |> summary(P)

Confusions |>
    group_by(Prediction, .draw) |>
    summarise(P = sum(P)) |>
    arrange(.draw, Prediction) |>
    ggplot(aes(y = Prediction,
               x = P,
               colour = Prediction, fill = Prediction)) +
  geom_violin() +
    geom_vline(xintercept = c(pTestNo, pTestYes), colour = c('blue', 'blue'), linewidth = 1.2) +
  guides(colour = 'none') +
  labs(title = "Predictions for test data from chart review",
       subtitle = "Test data probability shown as vertical lines",
       x = 'Estimated probability',
       y = 'Pneumonia') +
    theme_minimal() +
    scale_fill_viridis_d(option = 'G') +
    scale_colour_viridis_d(option = 'G')

```

```{r Collate predictions}
CCH_linpred <- CCH_test |> 
  add_linpred_draws(object = fit.PNk) |>
  ungroup() |> 
  select(rowid, .draw, PN, .linpred) |>
  arrange(rowid, .draw)

CCH_linpred |> group_by(.draw) |> summarise(N = n()) |> summary()

CCH_t.linpred <- CCH_test |> 
  add_linpred_draws(object = fit.PNk, transform = TRUE, value = 't.linpred') |>
  ungroup() |> 
  select(rowid, .draw, PN, t.linpred) |>
  arrange(rowid, .draw)

CCH_t.linpred |> group_by(.draw) |> summarise(N = n()) |> summary()

CCH_epred <- CCH_test |> 
  add_epred_draws(object = fit.PNk) |>
  ungroup() |>
  select(rowid, .draw, PN, .epred) |>
  arrange(rowid, .draw)

CCH_epred |> group_by(.draw) |> summarise(N = n()) |> summary()

CCH_pred <- CCH_test |> 
  add_predicted_draws(object = fit.PNk) |>
  ungroup() |>
  select(rowid, .draw, PN, .prediction) |>
  arrange(rowid, .draw)

CCH_pred |> group_by(.draw) |> summarise(N = n()) |> summary()

Predictions <- CCH_pred |>
  left_join(CCH_epred,by = join_by(rowid, .draw, PN)) |>
  left_join(CCH_linpred,by = join_by(rowid, .draw, PN)) |>
  left_join(CCH_t.linpred,by = join_by(rowid, .draw, PN)) 

ggplot(Predictions |> filter(.draw <= 1000),
       aes(x= .epred, y = .prediction, colour = PN)) +
  geom_jitter()

summarytools::ctable(Predictions$PN, Predictions$.prediction, prop = 't')
ggplot(Predictions, aes(x = .epred, y = t.linpred)) + geom_point() # All identical
```


```{r}
pp_check(fit.PNk)
pp_check(fit.PNk, plotfun = "boxplot", nreps = 10, notch = FALSE)
pp_check(fit.PNk, plotfun = "hist", nreps = 10)

rstan::stan_plot(fit.PNk, show_density=TRUE)
rstan::stan_plot(fit.PNk, pars = get_variables(fit.PNk)[2:28], show_density=TRUE)

plot(loo(fit.PNk))

performance::model_performance(fit.PNk)
```

# What matters?

This is not a classification problem. We are trying to estimate the prevalence, that is the probability that a random person discharged will have developed hospital acquired pneumonia. In terms of our models, this is just the transformed linear predictor, or the .epred.


```{r}
EPred.PNk.h <- posterior_epred(fit.PNk,
                              newdata = HIPE) # Probabilities
# 4000 rows of one column per record

EPred.PNk.ht <- as_tibble(t(EPred.PNk.h))

# 131478 rows of 4,000 draws in columns

Median <- EPred.PNk.ht |>  summarise(across(everything(), median))
Epred.PNk <- Median |> pivot_longer(cols = everything(), names_to = 'Row', values_to = 'Median')

summary(Epred.PN)

ggplot(Epred.PNk, aes(x = round(100*Median,2))) + geom_density()

```
 
