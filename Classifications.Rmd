---
title: "Classifications"
author: "Anthony Staines"
date: "`r Sys.Date()`"
output:
  html_document: default
  word_document: default
  pdf_document: 
    toc: true
    fig_caption: true
    number_sections: true
    latex_engine: xelatex
editor_options:
  chunk_output_type: console
bibliography: references.bib
csl: vancouver.csl
---

# Predictions

Prepares predictions for selected outcome variables for the study site file, using the study (chart review) outcomes, and the HIPE predictors.

```{r setup, include=FALSE}
rm(list=ls())

library(tidyverse)
library(lubridate)
library(tibble)
library(tidymodels)
library(readxl)

library(lme4)

library(knitr)
library(kableExtra)
library(summarytools)


library(patchwork)

library(ranger)
library(mlr3verse)
library(mlr3viz)

library(data.table)
library(future)

library(sjPlot)
library(sjtable2df)

library(broom)
library(broom.mixed)

library(stargazer)

library(gt)
library(gtsummary)


tidymodels_prefer(quiet = TRUE)

knitr::opts_chunk$set(echo = FALSE, cache = TRUE, warning = NA, message = NA, fig.pos = 'H',
      cache.extra = knitr::rand_seed)

st_options(ctable.round.digits = 2)

#How many CPU's?
N = parallel:::detectCores()

  options(Ncpus = N - 1)
  options(mc.cores = N - 1)
  setDTthreads(threads = N - 1,
               restore_after_fork = TRUE,
               throttle = 1024)

options(dplyr.summarise.inform = FALSE, # shut up summarise
        ranger.num.threads = N - 1) # Prepare for rf models

lgr::get_logger("mlr3")$set_threshold("error")
lgr::get_logger("bbotk")$set_threshold("error")

set.seed(979)
rm(N)
```

# Load the merged data file
This is the chart review file merged with the HIPE file provided by HPO.

```{r Load data file}

#####################################################
# Load the merged HIPE/Study data file
# 
Cost2Care.HIPE <- readRDS('data/Cost2Care.HIPE.Rds')
NAMES <- read_excel(
      'data/Clean Cost2Care Merged Chart Review and HIPE Data_WORKING.xlsx',
      sheet = 'NAMES')

#####################################################
# Load the total HIPE data file
# 
HIPE <- readRDS('data/HIPE.Rds') %>%
  mutate(Weight = NA)
# These are the sampling weights for the training data (IPW's) and are not relevant for the HIPE data, but need to be there to match the training data for predictions.
```

# Intent

The aim here is to some quick and simple predictions, and summaries

These models can be sense checked by their ability to predict the length of stay, the destination on discharge, and specifically if this was different from the source of admission, and death in hospital.

The most obvious limitation is that the chart review was done in one hospital, a model 4 centre.

A good deal of thought needs to be given to deciding what outcomes to look at - basically the outcome, or the outcome definitely associated with health care.

# Outcomes

We look at the set of outcomes suggested.

```{r Summary of Outcomes}

Outcomes <- Cost2Care.HIPE |>
  select(PN, UTI, PU, DL, F2M, rawlos, LosC, Source, Died, Destination)

skimr::skim(Outcomes)

Proportions <- Outcomes |>
  select(PN:F2M, Died) |>
  mutate(across(PN:Died,
                ~ fct_count(as_factor(.),
                            prop=TRUE)$p[[2]])) |> # Proportion of Yes for each adverse Outcome (the smaller proportion)
  unique()

Proportions

Proportions |>
  mutate(across(everything(), ~(round(. * 100,1)))) # %

```

```{r Outcome_recode, eval = FALSE}
#Keep this cos it's clever
#Outcome_recode <- function(DF, COND) {

  ASSOC  <- paste0({{COND}},'_Associated')
  ABBREV <- paste0({{COND}},'_A')

  COND <- ensym(COND)
  ASSOC <- ensym(ASSOC)
  ABBREV <- ensym(ABBREV)

  DF <- DF %>%
  mutate(!!COND := ifelse(is.na(!!COND), 'No', !!COND )) %>% # Missing -> No
  mutate(!!ASSOC := ifelse(is.na(!!ASSOC),'No', !!ASSOC)) %>%
  mutate(!!ASSOC := ifelse(!!ASSOC == 'No' &
                            !!COND == 'No',
                        'Not Applicable',
                        !!ASSOC)) %>%
  mutate(!!ASSOC := case_match(!!ASSOC,
                            'Not Applicable' ~ 'No Dx',
                            'No' ~ 'Unlikely',
                            'LIKELY associated' ~ 'Likely',
                            'DEFINITELY associated.' ~ 'Definite')) %>%
  mutate(!!ABBREV := case_match(!!ASSOC,
                            'No Dx' ~ 'No',
                            'Unlikely' ~ 'No',
                            'Likely' ~ 'Yes',
                            'Definite' ~ 'Yes'
                            ))
return(DF)
}

# Outcomes <- Outcomes %>%
#  Outcome_recode('PN') %>%
#  Outcome_recode('UTI') %>%
#  Outcome_recode('PU') %>%
#  Outcome_recode('DL')

#CtCH <- CtCH %>%
#  Outcome_recode('PN') %>%
#  Outcome_recode('UTI') %>%
#  Outcome_recode('PU') %>%
#  Outcome_recode('DL')
 
```

# Quick and dirty predictions

We prepare some rapid predictions, using the RF models on these data for the first set of outcomes - those reported whether or not associated with health care

## Prepare subset of data

First, guided by earlier experiments, we prepare a subset of key data items to work with.

```{r DATA}
#################################################
# Trim CtCH, a lot
CCH <- Cost2Care.HIPE %>%
  select(PN, UTI, PU, DL, F2M, # Outcomes from review
         Source, Died, Destination, rawlos, LosC, # Outcomes from HIPE
         ModelF, HospCode, MedSurg, AgeC, SeasonOfAdmission,  # Variables derived from HIPE
         ProcCount, DxCount, HadxCount, ScoreEl, ScoreCh, # Variables derived from HIPE
         Complexity:Therapeutic.Interventions, 
         elem, fullelig, mdc, # HIPE variables
         Weight # Sampling weights from chart review
         )
HIPE.Trimmed <- HIPE %>%
    select(Source, Died, Destination, rawlos, LosC, # Outcomes from HIPE
         ModelF, HospCode, MedSurg, AgeC, SeasonOfAdmission,  # Variables derived from HIPE
         ProcCount, DxCount, HadxCount, ScoreEl, ScoreCh, # Variables derived from HIPE
         Complexity:Therapeutic.Interventions, 
         elem, fullelig, mdc # HIPE variables
         )
```

## Define the classification tasks

```{r TASKS}
#################################################
# Define a classification task
#
# PN
tsk_PN <- as_task_classif(CCH %>%
  select(-c(UTI, PU, DL, F2M, rawlos)), # Outcomes from review,
                         target = "PN",
                         positive = "Yes",
                         weights="Weights",
                         id = 'PN')
  split_PN = mlr3::partition(tsk_PN)

#################################################
# UTI
tsk_UTI <- as_task_classif(CCH %>%
  select(-c(PN, PU, DL, F2M, rawlos)), # Outcomes from review,
                         target = "UTI",
                         positive = "Yes",
                         weights="Weights",
                         id = 'UTI')
  split_UTI = mlr3::partition(tsk_UTI)

#################################################
# PU
tsk_PU <- as_task_classif(CCH %>%
  select(-c(PN, UTI, DL, F2M, rawlos)), # Outcomes from review,
                         target = "PU",
                         positive = "Yes",
                         weights="Weights",
                         id = 'PU')
  split_PU = mlr3::partition(tsk_PU)

#################################################
# DL
tsk_DL <- as_task_classif(CCH %>%
  select(-c(PN, UTI, PU, F2M, rawlos)), # Outcomes from review,
                         target = "DL",
                         positive = "Yes",
                         weights="Weights",
                         id = 'DL')
  split_DL = mlr3::partition(tsk_DL)

#################################################
# F2M
tsk_F2M <- as_task_classif(CCH %>%
  select(-c(PN, UTI, PU, DL, rawlos)), # Outcomes from review,
                         target = "F2M",
                         positive = "Yes",
                         weights="Weights",
                         id = 'F2M')
  split_F2M = mlr3::partition(tsk_F2M)
```  

## Define learners

```{r LEARNERS}
lrn_featureless = lrn('classif.featureless',
                      predict_type = 'prob')

lrn_C = lrn('classif.ranger',
            predict_type = "prob",
            num.trees = 2000,
            importance = "permutation"
            )

lrn_T = lrn('classif.ranger',
            predict_type = "prob",
            num.trees = 2000,
            mtry = to_tune(1,5),
            importance = "permutation"
            )

```

## Assess importance

For each outcome, we prepare a restricted prediction set.

```{r IMPORTANCE}

#Ancillary function to store importance scores in a tibble
to_tibble <- function(flt_scores) {
  DF <- as.data.frame(flt_scores)
  DF$names <- row.names(DF)
  DF <- as_tibble(DF)
  return(DF)
}

flt_Imp = flt("importance", learner = lrn_C)

###################################################
#PN
  flt_Imp$calculate(tsk_PN)
    PN_Imp <- to_tibble(flt_Imp$scores)
    PN <- autoplot(flt_Imp, title="Prediction for pneumonia") +
      labs(title="Prediction for Pneumonia")
    PN
    summary(1000*flt_Imp$scores)
    keep = names(which(flt_Imp$scores > 0.0001))

tsk_PNk <- tsk_PN$clone(deep = TRUE)
tsk_PNk <- tsk_PNk$select(keep)  
  flt_Imp$calculate(tsk_PNk)
    PNk_Imp <- to_tibble(flt_Imp$scores)
    summary(1000*flt_Imp$scores)
    PNk <- autoplot(flt_Imp, title="Prediction for pneumoniak") +
      labs(title="Prediction for Pneumoniak")
    PNk

###################################################
#UTI
  flt_Imp$calculate(tsk_UTI)
    UTI_Imp <- to_tibble(flt_Imp$scores)
    UTI <- autoplot(flt_Imp, title="Prediction for UTI") +
      labs(title="Prediction for UTI")
    UTI
    1000*flt_Imp$scores
    summary(1000*flt_Imp$scores)
    keep = names(which(flt_Imp$scores > 0.0001))

tsk_UTIk <- tsk_UTI$clone(deep = TRUE)
tsk_UTIk <- tsk_UTIk$select(keep)  
  flt_Imp$calculate(tsk_UTIk)
    UTIk_Imp <- to_tibble(flt_Imp$scores)
    UTIk <- autoplot(flt_Imp, title="Prediction for UTIk") +
      labs(title="Prediction for UTIk")
    UTIk

###################################################
#PU
  flt_Imp$calculate(tsk_PU)
    PU_Imp <- to_tibble(flt_Imp$scores)
    PU <- autoplot(flt_Imp, title="Prediction for Pressure Ulcer") +
      labs(title="Prediction for Pressure Ulcer")
    PU
    1000*flt_Imp$scores
    summary(1000*flt_Imp$scores)
    keep = names(which(flt_Imp$scores > 0.0001))

tsk_PUk <- tsk_PU$clone(deep = TRUE)
tsk_PUk <- tsk_PUk$select(keep)  
  flt_Imp$calculate(tsk_PUk)
    summary(1000*flt_Imp$scores)
    PUk_Imp <- to_tibble(flt_Imp$scores)
    PUk <- autoplot(flt_Imp, title="Prediction for Pressure Ulcerk") +
      labs(title="Prediction for Pressure Ulcerk")
    PUk

###################################################
#DL
  flt_Imp$calculate(tsk_DL)
    DL_Imp <- to_tibble(flt_Imp$scores)
    DL <- autoplot(flt_Imp, title="Prediction for Delirium") +
      labs(title="Prediction for Delirium")
    DL
    1000*flt_Imp$scores
    summary(1000*flt_Imp$scores)
    keep = names(which(flt_Imp$scores > 0.0001))

tsk_DLk <- tsk_DL$clone(deep = TRUE)
tsk_DLk <- tsk_DLk$select(keep)  
  flt_Imp$calculate(tsk_DLk)
    DLk_Imp <- to_tibble(flt_Imp$scores)
    DLk <- autoplot(flt_Imp, title="Prediction for Deliriumk") +
      labs(title="Prediction for Deliriumk")
    DLk

###################################################
#F2M
  flt_Imp$calculate(tsk_F2M)
    F2M_Imp <- to_tibble(flt_Imp$scores)
    F2M <- autoplot(flt_Imp, title="Prediction for F2M") +
      labs(title="Prediction for F2M")
    F2M
    summary(1000*flt_Imp$scores)
    keep = names(which(flt_Imp$scores > 0.0001))

tsk_F2Mk <- tsk_F2M$clone(deep = FALSE)
tsk_F2Mk <- tsk_F2Mk$select(keep)  
  flt_Imp$calculate(tsk_F2Mk)
      summary(1000*flt_Imp$scores)
  F2Mk_Imp <- to_tibble(flt_Imp$scores)
    F2Mk <- autoplot(flt_Imp, title="Prediction for F2Mk") +
      labs(title="Prediction for F2Mk")
    F2Mk

PN  + PNk
UTI + UTIk
PU  + PUk
DL  + DLk
F2M + F2Mk

PN_Imp <- PN_Imp |> mutate(Source = 'PN_IMP')
UTI_Imp <- UTI_Imp |> mutate(Source = 'UTI_IMP')
PU_Imp <- PU_Imp |> mutate(Source = 'PU_IMP')
DL_Imp <- DL_Imp |> mutate(Source = 'DL_IMP')
F2M_Imp <- F2M_Imp |> mutate(Source = 'F2M_IMP')

PNk_Imp <- PNk_Imp |> mutate(Source = 'PNk_IMP')
UTIk_Imp <- UTIk_Imp |> mutate(Source = 'UTIk_IMP')
PUk_Imp <- PUk_Imp |> mutate(Source = 'PUk_IMP')
DLk_Imp <- DLk_Imp |> mutate(Source = 'DLk_IMP')
F2Mk_Imp <- F2Mk_Imp |> mutate(Source = 'F2Mk_IMP')

Imp <- bind_rows(PN_Imp, PNk_Imp, UTI_Imp, UTIk_Imp,
                 PU_Imp, PUk_Imp, DL_Imp, DLk_Imp,
                 F2M_Imp, F2Mk_Imp) %>%
  select(Source, names, flt_scores ) %>%
  rename(Variable = names) %>%
  rename(Importance = flt_scores) %>%
  mutate(Full  =ifelse(str_detect(Source, 'k_'), 'Reduced' , 'Full'))

Sums <- Imp %>%
  group_by(Variable) %>%
  summarise(Mean_Imp = mean(Importance), Count = n())

Imp <- Imp %>%
  left_join(Sums, by = join_by(Variable)) %>%
  arrange(Mean_Imp) %>%
  mutate(Variable = fct_inorder(as_factor(Variable)))

ggplot(Imp, aes(x=Importance, y = Variable,
                colour=Source, shape = Full,group = Source)) +
  geom_point(size = 2) + geom_line() +
  facet_wrap(~Full) +
  theme_minimal()

  rm(PN_Imp, PNk_Imp, UTI_Imp, UTIk_Imp, PU_Imp, PUk_Imp,
     DL_Imp, DLk_Imp, F2M_Imp, F2Mk_Imp)
```

Comparing the full set of variables considered, to those kept on the basis of their importance, there is little difference between the model measures.

## Resampling

We have seen the effect of a single split of the data, separating training and test data completely. Performance is much better on the trained data than on the test data. Resampling is another approach to the same issue. This involves creating multiple training and test sets, and repeating the analysis for each, then aggregating the results of these.

```{r RESAMPLING}
cv10 = rsmp("cv", folds = 10) # 10 folds
```

## Measures

```{r MEASURES}
measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc'))
```

## Classify

```{r FIT and EVALUATE}

########################################################################
# PN
lrn_featureless$train(tsk_PN, split_PN$train)
  prediction_PN.f = lrn_featureless$predict(tsk_PN, row_ids = split_PN$test)
  prediction_PN.f$score(measures)
  prediction_PN.f$confusion
autoplot(prediction_PN.f, type = 'roc')

lrn_C$train(tsk_PN, row_ids = split_PN$train)
  prediction_PN.rng = lrn_C$predict(tsk_PN, split_PN$test)
  prediction_PN.rng$score(measures)
  prediction_PN.rng$confusion
autoplot(prediction_PN.rng, type = 'roc')

lrn_C$train(tsk_PNk, row_ids = split_PN$train)
  prediction_PNk.rng = lrn_C$predict(tsk_PNk, split_PN$test)
  prediction_PNk.rng$score(measures)
  prediction_PNk.rng$confusion
autoplot(prediction_PNk.rng, type = 'roc')

########################################################################
# UTI
lrn_C$train(tsk_UTI, row_ids = split_UTI$train)
  prediction_UTI.rng = lrn_C$predict(tsk_UTI, split_UTI$test)
  prediction_UTI.rng$score(measures)
  prediction_UTI.rng$confusion
autoplot(prediction_UTI.rng, type = 'roc')

lrn_C$train(tsk_UTIk, row_ids = split_UTI$train)
  prediction_UTIk.rng = lrn_C$predict(tsk_UTIk, split_UTI$test)
  prediction_UTIk.rng$score(measures)
  prediction_UTIk.rng$confusion
autoplot(prediction_UTIk.rng, type = 'roc')

########################################################################
# PU
lrn_C$train(tsk_PU, row_ids = split_PU$train)
  prediction_PU.rng = lrn_C$predict(tsk_PU, split_PU$test)
  prediction_PU.rng$score(measures)
  prediction_PU.rng$confusion
autoplot(prediction_PU.rng, type = 'roc')

lrn_C$train(tsk_PUk, row_ids = split_PU$train)
  prediction_PUk.rng = lrn_C$predict(tsk_PUk, split_PU$test)
  prediction_PUk.rng$score(measures)
  prediction_PUk.rng$confusion
autoplot(prediction_PUk.rng, type = 'roc')

########################################################################
# DL
lrn_C$train(tsk_DL, row_ids = split_DL$train)
  prediction_DL.rng = lrn_C$predict(tsk_DL, split_DL$test)
  prediction_DL.rng$score(measures)
  prediction_DL.rng$confusion
autoplot(prediction_DL.rng, type = 'roc')

lrn_C$train(tsk_DLk, row_ids = split_DL$train)
  prediction_DLk.rng = lrn_C$predict(tsk_DLk, split_DL$test)
  prediction_DLk.rng$score(measures)
  prediction_DLk.rng$confusion
autoplot(prediction_DLk.rng, type = 'roc')

########################################################################
# F2M
lrn_C$train(tsk_F2M, row_ids = split_F2M$train)
  prediction_F2M.rng = lrn_C$predict(tsk_F2M, split_F2M$test)
  prediction_F2M.rng$score(measures)
  prediction_F2M.rng$confusion
autoplot(prediction_F2M.rng, type = 'roc')

lrn_C$train(tsk_F2Mk, row_ids = split_F2M$train)
  prediction_F2Mk.rng = lrn_C$predict(tsk_F2Mk, split_F2M$test)
  prediction_F2Mk.rng$score(measures)
  prediction_F2Mk.rng$confusion
autoplot(prediction_F2Mk.rng, type = 'roc',title='Failure to maintain - restricted predictor set') + labs(title='Failure to maintain - restricted predictor set')


```

## Optimise mtry for ranger models

```{r Ancillary function for graphs}

GRAPH_TUNING <- function(df, TITLE = 'TITLE'){
    # Maximise
    AUC <- ggplot(data = Tuner_df,
          aes(x = mtry,
              y = classif.auc)) +
        geom_smooth(colour = 'red')
    #Minimise
    BBRIER <- ggplot(data = Tuner_df,
                 aes(x = mtry,
                     y = classif.bbrier)) +
        geom_smooth(colour = 'green')
    #Maximise
    ACC <- ggplot(data = Tuner_df,
              aes(x = mtry,
                  y = classif.acc)) +
        geom_smooth(colour = 'red')
    #Minimise
    LOGLOSS <- ggplot(data = Tuner_df,
                  aes(x = mtry,
                      y = classif.logloss)) +
        geom_smooth(colour = 'green')
    IMAGE <- AUC + BBRIER + ACC + LOGLOSS +
        plot_annotation(title = TITLE) +
        plot_layout(axis_titles = 'collect')
return(IMAGE)
}

```
### PN

```{r OPTIMISE PN}

instance_T = ti(
    task = tsk_PN,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

PN_image <- GRAPH_TUNING(Tuner_df, "PN")
PN_image
ggsave("image/PN_image.png",PN_image)

```

### PNk

```{r OPTIMISE PNk}

instance_T = ti(
    task = tsk_PNk,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

PNk_image <- GRAPH_TUNING(Tuner_df, "PNk")
PNk_image
ggsave("image/PNk_image.png",PNk_image)

```


### UTI

```{r OPTIMISE UTI}

instance_T = ti(
    task = tsk_UTI,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))



UTI_image <- GRAPH_TUNING(Tuner_df, "UTI")
UTI_image
ggsave("image/UTI_image.png",UTI_image)

```

### UTIk

```{r OPTIMISE UTIk}

instance_T = ti(
    task = tsk_UTIk,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

UTIk_image <- GRAPH_TUNING(Tuner_df, "UTIk")
UTIk_image
ggsave("image/UTIk_image.png",UTIk_image)

```

### PU

```{r OPTIMISE PU}

instance_T = ti(
    task = tsk_PU,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

PU_image <- GRAPH_TUNING(Tuner_df, "PU")
PU_image
ggsave("image/PU_image.png",PU_image)

```

### PUk

```{r OPTIMISE PUk}

instance_T = ti(
    task = tsk_PUk,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

PUk_image <- GRAPH_TUNING(Tuner_df, "PUk")
PUk_image
ggsave("image/PUk_image.png",PUk_image)

```

### DL

```{r OPTIMISE DL}

instance_T = ti(
    task = tsk_DL,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

DL_image <- GRAPH_TUNING(Tuner_df, "DL")
DL_image
ggsave("image/DL_image.png",DL_image)

```

### DLk

```{r OPTIMISE DLk}

instance_T = ti(
    task = tsk_DLk,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

DLk_image <- GRAPH_TUNING(Tuner_df, "DLk")
DLk_image
ggsave("image/DLk_image.png",DLk_image)

```

### F2M

```{r OPTIMISE F2M}

instance_T = ti(
    task = tsk_F2M,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

F2M_image <- GRAPH_TUNING(Tuner_df, "F2M")
F2M_image
ggsave("image/F2M_image.png",F2M_image)

```

### F2Mk

```{r OPTIMISE F2Mk}

instance_T = ti(
    task = tsk_F2Mk,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

F2Mk_image <- GRAPH_TUNING(Tuner_df, "F2Mk")
F2Mk_image
ggsave("image/F2Mk_image.png",F2Mk_image)

```

### Merge graphs

```{r Merge optimisation graphs}
PN_image / PNk_image
UTI_image / UTIk_image
PU_image / PUk_image
DL_image / DLk_image
F2M_image / F2Mk_image


```

In pretty much every example, for the full data, an mtry of between 1 and 3 is optimal. Note that while the scale of some of the graphs suggest very little effect of mtry, in that some of the parameters change little for any value of mtry studied. For the restricted data, an mtry of 3 seems best, but for every condition, the restricted data set performs better.

## Evaluation



# Fit optimised models

We now have five models to fit and predict from, the five restricted models, with an mtry of 3.

## Define learners

```{r LEARNERS for opimised models}
lrn_featureless = lrn('classif.featureless',
                      predict_type = 'prob')

lrn_T = lrn('classif.ranger',
            predict_type = "prob",
            num.trees = 2000, # Determined in earlier experiments
            mtry = 3, # Assessed above
            importance = "permutation" # Most sensible choice
            )
```

## Run Models

### Resampling

```{r Run all 5 models}
rr_PNk.f    = resample(tsk_PNk, lrn_featureless, cv10, store_models = TRUE)
rr_PNk.rng  = resample(tsk_PNk, lrn_T, cv10, store_models = TRUE)
rr_UTIk.f   = resample(tsk_UTIk, lrn_featureless, cv10, store_models = TRUE)
rr_UTIk.rng = resample(tsk_UTIk, lrn_T, cv10, store_models = TRUE)
rr_PUk.f    = resample(tsk_PUk, lrn_featureless, cv10, store_models = TRUE)
rr_PUk.rng  = resample(tsk_PUk, lrn_T, cv10, store_models = TRUE)
rr_DLk.f    = resample(tsk_DLk, lrn_featureless, cv10, store_models = TRUE)
rr_DLk.rng  = resample(tsk_DLk, lrn_T, cv10, store_models = TRUE)
rr_F2Mk.f   = resample(tsk_F2Mk, lrn_featureless, cv10, store_models = TRUE)
rr_F2Mk.rng = resample(tsk_F2Mk, lrn_T, cv10, store_models = TRUE)

```

## ROC curves

```{r ROC curves}
roc_PNk <- autoplot(rr_PNk.rng, type = 'roc')
roc_UTIk <- autoplot(rr_UTIk.rng, type = 'roc')
roc_PUk <- autoplot(rr_PUk.rng, type = 'roc')
roc_DLk <- autoplot(rr_DLk.rng, type = 'roc')
roc_F2Mk <- autoplot(rr_F2Mk.rng, type = 'roc')

design = 
'12
345'

roc_PNk + roc_UTIk + plot_spacer()+ roc_PUk + roc_DLk + roc_F2Mk +
  plot_layout(ncol = 3, axes = 'collect', axis_titles = 'collect') +
  plot_annotation(title = 'ROC curves for optimised models',
                  tag_levels = list(c('PNk','UTIk','PUk','DLk','F2Mk')))


```

## Predictions on test data and confusion matrices

The requirement is to predict the total number of adverse events as well as possible. Direct inspection of the ROC curves suggests that around 0.25 to 0.35 is about right for most. UTI is probably the worst predicted outcome.

```{r}
pred_PNk <- rr_PNk.rng$prediction()
pred_UTIk <- rr_UTIk.rng$prediction()
pred_PUk <- rr_PUk.rng$prediction()
pred_DLk <- rr_DLk.rng$prediction()
pred_F2Mk <- rr_F2Mk.rng$prediction()

pred_PNk$set_threshold(0.2)
autoplot(pred_PNk) + labs(title = 'PNk Threshold 0.3')

pred_UTIk$set_threshold(0.2)
autoplot(pred_UTIk) + labs(title = 'UTIk Threshold 0.2')

pred_PUk$set_threshold(0.23)
autoplot(pred_PUk) + labs(title = 'PUk Threshold 0.23')

pred_DLk$set_threshold(0.275)
autoplot(pred_DLk) + labs(title = 'DLk Threshold 0.275')

pred_F2Mk$set_threshold(0.35)
autoplot(pred_F2Mk) + labs(title = 'F2Mk Threshold 0.35')


```

### Training

Thresholds are set to roughly equalise the true number of events in the training set, and the number of events predicted.

```{r}
task_PNk <- rr_PNk.rng$task
task_UTIk <- rr_UTIk.rng$task
task_PUk <- rr_PUk.rng$task
task_DLk <- rr_DLk.rng$task
task_F2Mk <- rr_F2Mk.rng$task

PNk_trained <- lrn_T$train(task_PNk)
  PNk_pred <- PNk_trained$predict(task_PNk)
  PNk_pred$set_threshold(0.3)
  autoplot(PNk_pred)

UTIk_trained <- lrn_T$train(task_UTIk)
  UTIk_pred <- UTIk_trained$predict(task_UTIk)
  UTIk_pred$set_threshold(0.3)
  autoplot(UTIk_pred)

PUk_trained <- lrn_T$train(task_PUk)
  PUk_pred <- PUk_trained$predict(task_PUk)
  PUk_pred$set_threshold(0.3)
  autoplot(PUk_pred)

DLk_trained <- lrn_T$train(task_DLk)
  DLk_pred <- DLk_trained$predict(task_DLk)
  DLk_pred$set_threshold(0.3)
  autoplot(DLk_pred)

F2Mk_trained <- lrn_T$train(task_F2Mk)
  F2Mk_pred <- F2Mk_trained$predict(task_F2Mk)
  F2Mk_pred$set_threshold(0.39)
  autoplot(F2Mk_pred)

```

# Prediction on HIPE data

```{r PREDICT HIPE}

lrn_T$predict_newdata(HIPE, task = task_PNk)

pred_H.PNk = PNk_trained$predict_newdata(HIPE)
PNk <-  fct_count(as_factor(pred_H.PNk$response),
            prop = TRUE)
PNk

pred_H.UTIk = UTIk_trained$predict_newdata(HIPE)
UTIk <- fct_count(as_factor(pred_H.UTIk$response),
                 prop = TRUE)
UTIk

pred_H.PUk = PUk_trained$predict_newdata(HIPE)
PUk <-  fct_count(as_factor(pred_H.PUk$response),
            prop = TRUE)
PUk

pred_H.DLk = DLk_trained$predict_newdata(HIPE)
DLk <-  fct_count(as_factor(pred_H.DLk$response),
            prop = TRUE)
DLk

pred_H.F2Mk = F2Mk_trained$predict_newdata(HIPE)
F2Mk <-  fct_count(as_factor(pred_H.F2Mk$response),
            prop = TRUE)
F2Mk
```


```{r}
predict(F2Mk_trained, HIPE, predict_type = '<Prediction>')
```
