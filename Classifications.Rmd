---
title: "Classifications"
author: "Anthony Staines"
date: "`r Sys.Date()`"
output:
  html_document: default
  word_document: default
  pdf_document: 
    toc: true
    fig_caption: true
    number_sections: true
    latex_engine: xelatex
editor_options:
  chunk_output_type: console
bibliography: references.bib
csl: vancouver.csl
---

# Predictions

Prepares predictions for selected outcome variables for the study site file, using the study (chart review) outcomes, and the HIPE predictors.

```{r setup, include=FALSE}
rm(list=ls())

library(tidyverse)
library(lubridate)
library(tibble)
library(tidymodels)
library(readxl)

library(lme4)

library(knitr)
library(kableExtra)
library(summarytools)


library(patchwork)

library(ranger)
library(mlr3verse)
library(mlr3viz)

library(data.table)
library(future)

library(sjPlot)
library(sjtable2df)

library(broom)
library(broom.mixed)

library(stargazer)

library(gt)
library(gtsummary)


tidymodels_prefer(quiet = TRUE)

knitr::opts_chunk$set(echo = FALSE, cache = TRUE, warning = NA, message = NA, fig.pos = 'H',
      cache.extra = knitr::rand_seed)

st_options(ctable.round.digits = 2)

#How many CPU's?
N = parallel:::detectCores()

  options(Ncpus = N - 1)
  options(mc.cores = N - 1)
  setDTthreads(threads = N - 1,
               restore_after_fork = TRUE,
               throttle = 1024)

options(dplyr.summarise.inform = FALSE, # shut up summarise
        ranger.num.threads = N) # Prepare for rf models
lgr::get_logger("mlr3")$set_threshold("error")
lgr::get_logger("bbotk")$set_threshold("error")


set.seed(979)
rm(N)
```

# Load the merged data file
This is the chart review file merged with the HIPE file provided by HPO.

```{r Load data file}

#####################################################
# Load the merged HIPE/Study data file
# 
Cost2Care.HIPE <- readRDS('data/Cost2Care.HIPE.Rds')
NAMES <- read_excel(
      'data/Clean Cost2Care Merged Chart Review and HIPE Data_WORKING.xlsx',
      sheet = 'NAMES')

#####################################################
# Load the total HIPE data file
# 
HIPE <- readRDS('data/HIPE.Rds')

```

## Make analysis data file

We give it a shorter typable name 

```{r simplify data}
 
CtCH <- Cost2Care.HIPE
   
HIPE %>% filter(LosC <= 5) %>% summarise( N = n())
ggplot(data = HIPE %>% filter(LosC <= 5),
       aes(x=rawlos)) + geom_boxplot()
```

# Intent

The aim here is to some quick and simple predictions, and summaries


These models can be sense checked by their ability to predict the length of stay, the destination on discharge, and specifically if this was different from the source of admission, and death in hospital.

The most obvious limitation is that the chart review was done in one hospital, a model 4 centre.

A good deal of thought needs to be given to deciding what outcomes to look at - basically the outcome, or the outcome definitely associated with health care.


# Outcomes

We look at the set of outcomes suggested.


```{r}

Outcomes <- CtCH |>
  select(PN, PN_Associated, UTI, UTI_Associated, PU, PU_Associated, DL, DL_Associated, rawlos, LosC, Source, Died, Destination)

skimr::skim(Outcomes)
```

Then we complete the outcomes by setting NA to No, and creating a definitely or likely health care associated variable for each of the four key conditions.

```{r Outcome_recode}
Outcome_recode <- function(DF, COND) {

  ASSOC  <- paste0({{COND}},'_Associated')
  ABBREV <- paste0({{COND}},'_A')

  COND <- ensym(COND)
  ASSOC <- ensym(ASSOC)
  ABBREV <- ensym(ABBREV)

  DF <- DF %>%
  mutate(!!COND := ifelse(is.na(!!COND), 'No', !!COND )) %>% # Missing -> No
  mutate(!!ASSOC := ifelse(is.na(!!ASSOC),'No', !!ASSOC)) %>%
  mutate(!!ASSOC := ifelse(!!ASSOC == 'No' &
                            !!COND == 'No',
                        'Not Applicable',
                        !!ASSOC)) %>%
  mutate(!!ASSOC := case_match(!!ASSOC,
                            'Not Applicable' ~ 'No Dx',
                            'No' ~ 'Unlikely',
                            'LIKELY associated' ~ 'Likely',
                            'DEFINITELY associated.' ~ 'Definite')) %>%
  mutate(!!ABBREV := case_match(!!ASSOC,
                            'No Dx' ~ 'No',
                            'Unlikely' ~ 'No',
                            'Likely' ~ 'Yes',
                            'Definite' ~ 'Yes'
                            ))
return(DF)
}


 Outcomes <- Outcomes %>%
  Outcome_recode('PN') %>%
  Outcome_recode('UTI') %>%
  Outcome_recode('PU') %>%
  Outcome_recode('DL')

CtCH <- CtCH %>%
  Outcome_recode('PN') %>%
  Outcome_recode('UTI') %>%
  Outcome_recode('PU') %>%
  Outcome_recode('DL')
 
```

# Quick and dirty predictions

We prepare some rapid predictions, using the RF models on these data for the first set of outcomes - those reported whether or not associated with health care

## Prepare subset of data

first, guided by earlier experiments, we prepare a subset of key data items to work with.

```{r DATA}
#################################################
# Trim CtCH, a lot
CCH <- CtCH %>%
  select(PN, PN_A, UTI, UTI_A,
         PU, PU_A, DL, DL_A, # Outcomes _A - health care associated
         Source, Died, Destination, rawlos,  # Outcomes
         Training, # Derived random split variable
         ModelF, MedSurg, AgeC, SeasonOfAdmission,  # Variables derived from HIPE
         ProcCount, DxCount, HadxCount, ScoreEl, ScoreCh, # Variables derived from HIPE
         Complexity,
         elem, fullelig, drgv.8 # HIPE variables
         )
HIPE.Trimmed <- HIPE %>%
  select(Source, Died, Destination, rawlos,  # Outcomes
         Training, # Derived random split variable
         ModelF, MedSurg, AgeC, SeasonOfAdmission,  # Variables derived from HIPE
         ProcCount, DxCount, HadxCount, ScoreEl, ScoreCh, # Variables derived from HIPE
         Complexity,
         elem, fullelig, drgv.8 # HIPE variables
         )
```

## Define the classification task

```{r TASK}
#################################################
# Define a classification task
#
# PN
tsk_C <- as_task_classif(CtCH %>%
                         select( -c(Encrypted_mrn:Frailty_score),
                                -c(LOS:ID),
                                -c(PN_Associated, UTI_Associated,
                                   PU_Associated, DL_Associated),
                                -c(PN, PN_A, UTI, UTI_A, PU, PU_A, DL_A, LosC),
                                -matches('Date'),
                                -contains('desc'),
                                -c(StudyID, ICD_combined),
                                -c(age, drgv.8, ScoreCh),
                                -ends_with('Ch')),
                         target = "DL",
                         positive = "Yes",
                         id = 'PN')
  split_C = mlr3::partition(tsk_C)
```  

## Define learners

```{r LEARNERS}
lrn_featureless = lrn('classif.featureless',
                      predict_type = 'prob')

lrn_C = lrn('classif.ranger',
            predict_type = "prob",
            num.trees = 2000,
            importance = "permutation"
            )

glrn_C = GraphLearner$new(lrn_C  %>>% po("threshold"))

search_space = ps(
  threshold.thresholds = p_dbl(lower = 0, upper = 1)
)
```

## Assess importance

```{r IMPORTANCE}
flt_Imp = flt("importance", learner = lrn_C)
  flt_Imp$calculate(tsk_C)
autoplot(flt_Imp)
summary(1000*flt_Imp$scores)
keep = names(which(1000*flt_Imp$scores > 0.5))
tsk_C$select(keep)  
```

## Resampling

We have seen the effect of a single split of the data, separating training and test data completely. Performance is much better on the trained data than on the test data. Resampling is another approach to the same issue. This involves creating multiple training and test sets, and repeating the analysis for each, then aggregating the results of these.

```{r RESAMPLING}
cv10 = rsmp("cv", folds = 10) # 10 folds
```

## Measures

```{r MEASURES}
measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc'))
```

## Classify

```{r FIT}
lrn_featureless$train(tsk_C, split_C$train)
prediction_C.f = lrn_featureless$predict(tsk_C, row_ids = split_C$test)
prediction_C.f
prediction_C.f$score(measures)
prediction_C.f$confusion
autoplot(prediction_C.f, type = 'roc')

lrn_C$train(tsk_C, row_ids = split_C$train)
prediction_C.rng = lrn_C$predict(tsk_C, split_C$test)
prediction_C.rng
prediction_C.rng$score(measures)
prediction_C.rng$confusion
autoplot(prediction_C.rng, type = 'roc')

prediction_C.rng$set_threshold(0.3)
C.rng <- as_tibble((as.data.table(prediction_C.rng)))

rr_C.f = resample(tsk_C, lrn_featureless, cv10, store_models = TRUE)
rr_C.rng = resample(tsk_C, lrn_C, cv10, store_models = TRUE)

```

## Evaluation

```{EVALUATE at T = 0.5}

autoplot(rr_C.f, type = 'roc') + labs(title="C")
autoplot(rr_C.rng, type = 'roc') + labs(title="C")

autoplot(rr_C.f,   type = 'prc') + labs(title='C')
autoplot(rr_C.rng, type = 'prc') + labs(title='C')

prediction_C.f = lrn_featureless$predict(tsk_C, split_C$test)
prediction_C.f$score(measures)
prediction_C.f$confusion
autoplot(prediction_C.f, type = 'roc')
autoplot(rr_C.f, type = 'prc')

prediction_C.rng = lrn_C$predict(tsk_C, row_ids = split_C$test)
prediction_C.rng$score(measures)
prediction_C.rng$confusion
autoplot(prediction_C.rng, type = 'roc')
autoplot(prediction_C.rng, type = 'prc') + labs(title='C')

autoplot(rr_C.rng, type = 'prc') + labs(title='C')

prediction_C.rng$score(measures)
prediction_C.rng$confusion
autoplot(prediction_C.rng, type = 'roc')
```

## Optimise threshold for ranger model

```{r OPTIMISE, eval = FALSE}
instance_C = ti(
  task = tsk_C,
  learner = glrn_C,
  resampling = cv10,
  measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')),
  terminator = trm("evals", n_evals=500), # For tuner "random_search"
  search_space = search_space
  )
tuner = tnr("random_search")
  future::plan(multisession, workers = 10)
tuner$optimize(instance_C)

Tuner_df <- as_tibble(
  as.data.table(instance_C$archive))


AUC <- ggplot(data = Tuner_df,
       aes(x = threshold.thresholds,
           y = classif.auc)) +
  geom_smooth(colour = 'red')
BBRIER <- ggplot(data = Tuner_df,
       aes(x = threshold.thresholds,
           y = classif.bbrier)) +
  geom_smooth()
LOGLOSS <- ggplot(data = Tuner_df,
       aes(x = threshold.thresholds,
           y = classif.logloss)) +
  geom_smooth(colour = 'red')
ACC <- ggplot(data = Tuner_df,
       aes(x = threshold.thresholds,
           y = classif.acc)) +
  geom_smooth()
AUC + BBRIER + LOGLOSS + ACC
```

Best shot at threshold is about 0.7

# Fit at optimised threshold

```{r}
prediction_C.rng$set_threshold(0.3)
prediction_C.rng$score(measures)
prediction_C.rng$confusion
glimpse(as.data.table(prediction_C.rng))
```

# Prediction on HIPE data

```{r PREDICT HIPE}
H.Predicted = lrn_C$predict_newdata(HIPE, tsk_C)
H.Predicted$set_threshold(0.3)

glimpse(as.data.table(H.Predicted))
```


```{R Save predictions DL, eval=FALSE}
DL <- as_tibble( as.data.table( H.Predicted ) ) %>%
  mutate(Response = case_match(response,
                        'Yes' ~ 1,
                        'No'  ~ 0))
table(DL$Response, useNA = 'ifany')

saveRDS(DL, file = 'data/DL_predicted.Rds')
```

```{R Save predictions PN, eval=FALSE}
PN <- as_tibble( as.data.table( H.Predicted ) ) %>%
  mutate(Response = case_match(response,
                        'Yes' ~ 1,
                        'No'  ~ 0))
table(PN$Response, useNA = 'ifany')
saveRDS(PN, file = 'data/PN_predicted.Rds')

```
