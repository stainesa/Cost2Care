---
title: "Classifications"
author: "Anthony Staines"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: true
    fig_caption: true
    number_sections: true
    latex_engine: xelatex
  word_document: default
  html_document: default
editor_options:
  chunk_output_type: console
bibliography: references.bib
csl: vancouver.csl
---

```{r setup, include=FALSE}
rm(list = ls())

library(tidyverse)
library(lubridate)
library(tibble)
library(tidymodels)
library(readxl)

library(lme4)

library(knitr)
library(kableExtra)
library(summarytools)

library(patchwork)

library(ranger)
library(mlr3verse)
library(mlr3viz)

library(data.table)
library(future)

library(sjPlot)
library(sjtable2df)

library(broom)
library(broom.mixed)

library(stargazer)

library(gt)
library(gtsummary)

tidymodels_prefer(quiet = TRUE)

st_options(ctable.round.digits = 2)

#How many CPU's?
N = parallel:::detectCores()

  options(Ncpus = N - 1)
  options(mc.cores = N - 1)
  setDTthreads(threads = N - 1,
               restore_after_fork = TRUE,
               throttle = 1024)

options(dplyr.summarise.inform = FALSE, # shut up summarise
        ranger.num.threads = N - 1) # Prepare for rf models

lgr::get_logger("mlr3")$set_threshold("error")
lgr::get_logger("bbotk")$set_threshold("error")

set.seed(979)
rm(N)

knitr::opts_chunk$set(echo = FALSE,
                      cache = TRUE, cache.lazy = FALSE, 
                      warning = NA, message = NA, fig.pos = 'H',
                      cache.extra = knitr::rand_seed)
```

# Predictions

Prepares predictions for selected outcome variables for the study site file, using the study (chart review) outcomes, and the HIPE predictors.

# Load the merged data file
This is the chart review file merged with the HIPE file provided by HPO, and the full 2022 HIPE file for discharges in people aged over 65, who spent at least three days in hospital.

```{r Load data files}

#####################################################
# Load the merged HIPE/Study data file
# 
Cost2Care.HIPE <- readRDS('data/Cost2Care.HIPE.Rds')
NAMES <- read_excel(
      'data/Clean Cost2Care Merged Chart Review and HIPE Data_WORKING.xlsx',
      sheet = 'NAMES')

#####################################################
# Load the total HIPE data file
# 
HIPE <- readRDS('data/HIPE.Rds') %>%
  mutate(Weight = 1)
# These are the sampling weights for the training data (IPW's) and are not relevant for the HIPE data, but need to be there to match the training data for predictions.
```

# Intent 

The aim here is to some quick and simple predictions, and summaries

These models can be sense checked by their ability to predict the length of stay, the destination on discharge, and specifically if this was different from the source of admission, and death in hospital.

The most obvious limitation is that the chart review was done in one hospital, a model 4 centre.

A good deal of thought needs to be given to deciding what outcomes to look at - basically the outcome, or the outcome definitely associated with health care.

# Outcomes

We look at the set of outcomes suggested.

```{r Summary of Outcomes}

Outcomes <- Cost2Care.HIPE |>
  select(PN, UTI, PU, DL, F2M, rawlos, LosC, Source, Died, Destination)

skimr::skim(Outcomes)

## Chart review data

Proportions <- Outcomes |>
  select(PN:F2M, Died) |>
  mutate(across(PN:Died,
                ~ fct_count(as_factor(.),
                            prop=TRUE)$p[[2]])) |> # Proportion of Yes for each adverse Outcome (the smaller proportion)
  unique() 

Proportions |>
  kable(caption = 'Proportion of people from chart review  experiencing each adverse outcome')


Proportions |>
  mutate(across(everything(), ~(round(. * 100,1)))) |>
  kable(caption = 'Percentage of people from chart review  experiencing each adverse outcome')
```

```{r Outcome_recode, eval = FALSE}
#Keep this cos it's clever
Outcome_recode <- function(DF, COND) {

  ASSOC  <- paste0({{COND}},'_Associated')
  ABBREV <- paste0({{COND}},'_A')

  COND <- ensym(COND)
  ASSOC <- ensym(ASSOC)
  ABBREV <- ensym(ABBREV)

  DF <- DF %>%
  mutate(!!COND := ifelse(is.na(!!COND), 'No', !!COND )) %>% # Missing -> No
  mutate(!!ASSOC := ifelse(is.na(!!ASSOC),'No', !!ASSOC)) %>%
  mutate(!!ASSOC := ifelse(!!ASSOC == 'No' &
                            !!COND == 'No',
                        'Not Applicable',
                        !!ASSOC)) %>%
  mutate(!!ASSOC := case_match(!!ASSOC,
                            'Not Applicable' ~ 'No Dx',
                            'No' ~ 'Unlikely',
                            'LIKELY associated' ~ 'Likely',
                            'DEFINITELY associated.' ~ 'Definite')) %>%
  mutate(!!ABBREV := case_match(!!ASSOC,
                            'No Dx' ~ 'No',
                            'Unlikely' ~ 'No',
                            'Likely' ~ 'Yes',
                            'Definite' ~ 'Yes'
                            ))
return(DF)
}

# Outcomes <- Outcomes %>%
#  Outcome_recode('PN') %>%
#  Outcome_recode('UTI') %>%
#  Outcome_recode('PU') %>%
#  Outcome_recode('DL')

#CtCH <- CtCH %>%
#  Outcome_recode('PN') %>%
#  Outcome_recode('UTI') %>%
#  Outcome_recode('PU') %>%
#  Outcome_recode('DL')
 
```

# Quick and dirty predictions

We prepare some rapid predictions, using the RF models on these data for the first set of outcomes - those reported whether or not associated with health care

## Prepare subset of data

First, guided by earlier experiments, and the literature, we prepare a subset of key data items to work with.

```{r DATA}
#################################################
# Trim Cost2Care.HIPE, a lot
CCH <- Cost2Care.HIPE %>%
  select(PN, UTI, PU, DL, F2M, # Outcomes from review
         Source, Died, Destination, rawlos, LosC, # Outcomes from HIPE
         MedSurg, AgeC, SeasonOfAdmission,  # Variables derived from HIPE
         ProcCount, DxCount, HadxCount, ScoreEl, # HIPE variables derived by me
         Complexity:Therapeutic.Interventions, # HIPE variables derived by me
         uti:any.f2m.hadx, # Hipe variables derived by HPO
         elem, fullelig, mdc, # HIPE variables
         Weights # Sampling weights from chart review
         )

saveRDS(CCH,'data/CCH.Rds')
```

## Define the classification tasks

```{r TASKS}
#################################################
# Define a classification task
#
# PN and PNk
tsk_PN <- as_task_classif(CCH %>%
  select(-c(UTI, PU, DL, F2M, rawlos)), # Outcomes from review,
                         target = "PN",
                         positive = "Yes",
                         id = 'PN')
  tsk_PN$col_roles$stratum = "PN"
  tsk_PN$col_roles$weight = "Weights"
  tsk_PN$select(setdiff(tsk_PN$feature_names, "Weights"))

  split_PN = mlr3::partition(tsk_PN)
  
    kable(table(CCH$PN[split_PN$train]), caption='PN_train')
    kable(table(CCH$PN[split_PN$test]), caption='PN_test')
    
tsk_PNk <- as_task_classif(CCH %>%
  select(-c(UTI, PU, DL, F2M, rawlos)), # Outcomes from review,
                         target = "PN",
                         positive = "Yes",
                         id = 'PNk')
  tsk_PNk$col_roles$stratum = "PN"
  tsk_PNk$col_roles$weight = "Weights"
  tsk_PNk$select(setdiff(tsk_PNk$feature_names, "Weights"))

  split_PNk = mlr3::partition(tsk_PNk)
  
    kable(table(CCH$PN[split_PNk$train]), caption='PNk_train')
    kable(table(CCH$PN[split_PNk$test]), caption='PNk_test')
    
  
#################################################
# UTI
tsk_UTI <- as_task_classif(CCH %>%
  select(-c(PN, PU, DL, F2M, rawlos)), # Outcomes from review,
                         target = "UTI",
                         positive = "Yes",
                         id = 'UTI')
  tsk_UTI$col_roles$stratum = "UTI"
  tsk_UTI$col_roles$weight = "Weights"
  tsk_UTI$select(setdiff(tsk_UTI$feature_names, "Weights"))

  split_UTI = mlr3::partition(tsk_UTI)
  
    kable(table(CCH$UTI[split_UTI$train]), caption='UTI_train')
    kable(table(CCH$UTI[split_UTI$test]), caption='UTI_test')

tsk_UTIk <- as_task_classif(CCH %>%
  select(-c(PN, PU, DL, F2M, rawlos)), # Outcomes from review,
                         target = "UTI",
                         positive = "Yes",
                         id = 'UTIk')
  tsk_UTIk$col_roles$stratum = "UTI"
  tsk_UTIk$col_roles$weight = "Weights"
  tsk_UTIk$select(setdiff(tsk_UTIk$feature_names, "Weights"))
  
  split_UTIk = mlr3::partition(tsk_UTIk)
  
    kable(table(CCH$UTI[split_UTIk$train]), caption='UTI_train')
    kable(table(CCH$UTI[split_UTIk$test]), caption='UTIk_test')

#################################################
# PU
tsk_PU <- as_task_classif(CCH %>%
  select(-c(PN, UTI, DL, F2M, rawlos)), # Outcomes from review,
                         target = "PU",
                         positive = "Yes",
                         id = 'PU')
  tsk_PU$col_roles$stratum = "PU"
  tsk_PU$col_roles$weight = "Weights"
  tsk_PU$select(setdiff(tsk_PU$feature_names, "Weights"))

  split_PU = mlr3::partition(tsk_PU)
  
    kable(table(CCH$PU[split_PU$train]), caption='PU_train')
    kable(table(CCH$PU[split_PU$test]), caption='PU_test')

tsk_PUk <- as_task_classif(CCH %>%
  select(-c(PN, UTI, DL, F2M, rawlos)), # Outcomes from review,
                         target = "PU",
                         positive = "Yes",
                         weights="Weights",
                         id = 'PUk')
  tsk_PUk$col_roles$stratum = "PU"
  tsk_PUk$col_roles$weight = "Weights"
  tsk_PUk$select(setdiff(tsk_PUk$feature_names, "Weights"))
  
  split_PUk = mlr3::partition(tsk_PUk)
  
    kable(table(CCH$PU[split_PUk$train]), caption='PUk_train')
    kable(table(CCH$PU[split_PUk$test]), caption='PUk_test')

#################################################
# DL
tsk_DL <- as_task_classif(CCH %>%
  select(-c(PN, UTI, PU, F2M, rawlos)), # Outcomes from review,
                         target = "DL",
                         positive = "Yes",
                         id = 'DL')
  tsk_DL$col_roles$stratum = "DL"
  tsk_DL$col_roles$weight = "Weights"
  tsk_DL$select(setdiff(tsk_DL$feature_names, "Weights"))
  
  split_DL = mlr3::partition(tsk_DL)
  
    kable(table(CCH$DL[split_DL$train]), caption='DL_train')
    kable(table(CCH$DL[split_DL$test]), caption='DL_test')

tsk_DLk <- as_task_classif(CCH %>%
  select(-c(PN, UTI, PU, F2M, rawlos)), # Outcomes from review,
                         target = "DL",
                         positive = "Yes",
                         id = 'DLk')
  tsk_DLk$col_roles$stratum = "DL"
  tsk_DLk$col_roles$weight = "Weights"
  tsk_DLk$select(setdiff(tsk_DLk$feature_names, "Weights"))

  split_DLk = mlr3::partition(tsk_DLk)
  
    kable(table(CCH$DL[split_DLk$train]), caption='DLk_train')
    kable(table(CCH$DL[split_DLk$test]), caption='DLk_test')

#################################################
# F2M
tsk_F2M <- as_task_classif(CCH %>%
  select(-c(PN, UTI, PU, DL, rawlos)), # Outcomes from review,
                         target = "F2M",
                         positive = "Yes",
                         id = 'F2M')
  tsk_F2M$col_roles$stratum = "F2M"
  tsk_F2M$col_roles$weight = "Weights"
  tsk_F2M$select(setdiff(tsk_F2M$feature_names, "Weights"))

  split_F2M = mlr3::partition(tsk_F2M)
  
    kable(table(CCH$F2M[split_F2M$train]), caption='F2M_train')
    kable(table(CCH$F2M[split_F2M$test]), caption='F2M_test')

tsk_F2Mk <- as_task_classif(CCH %>%
  select(-c(PN, UTI, PU, DL, rawlos)), # Outcomes from review,
                         target = "F2M",
                         positive = "Yes",
                         id = 'F2Mk')
  tsk_F2Mk$col_roles$stratum = "F2M"
  tsk_F2Mk$col_roles$weight = "Weights"
  tsk_F2Mk$select(setdiff(tsk_F2Mk$feature_names, "Weights"))
  
  split_F2Mk = mlr3::partition(tsk_F2Mk)
  
    kable(table(CCH$F2M[split_F2Mk$train]), caption='F2Mk_train')
    kable(table(CCH$F2M[split_F2Mk$test]), caption='F2Mk_test')

```  

## Define learners

```{r LEARNERS}
lrn_featureless = lrn('classif.featureless',
                      predict_type = 'prob')

lrn_C = lrn('classif.ranger',
            predict_type = "prob",
            num.trees = 2000,
            importance = "permutation"
            )

lrn_T = lrn('classif.ranger',
            predict_type = "prob",
            num.trees = 2000,
            mtry = to_tune(3,19),
            importance = "permutation"
            )

```

## Assess importance

For each outcome, we prepare a restricted prediction set.

```{r IMPORTANCE}

#Ancillary function to store importance scores in a tibble
to_tibble <- function(flt_scores) {
  DF <- as.data.frame(flt_scores)
  DF$names <- row.names(DF)
  DF <- as_tibble(DF)
  return(DF)
}

flt_Imp = flt("importance", learner = lrn_C)

###################################################
#PN
  flt_Imp$calculate(tsk_PN)
    PN_Imp <- to_tibble(flt_Imp$scores)
    PN <- autoplot(flt_Imp, title="Prediction for pneumonia") +
      labs(title="Prediction for Pneumonia")
#    PN
#    summary(1000*flt_Imp$scores)
    keep.PN = names(which(flt_Imp$scores > 0.0001))

tsk_PNk <- tsk_PNk$select(keep.PN)  
  flt_Imp$calculate(tsk_PNk)
    PNk_Imp <- to_tibble(flt_Imp$scores)
#    summary(1000*flt_Imp$scores)
    PNk <- autoplot(flt_Imp, title="Prediction for pneumoniak") +
      labs(title="Prediction for Pneumoniak")
#    PNk

###################################################
#UTI
  flt_Imp$calculate(tsk_UTI)
    UTI_Imp <- to_tibble(flt_Imp$scores)
    UTI <- autoplot(flt_Imp, title="Prediction for UTI") +
      labs(title="Prediction for UTI")
#    UTI
#    1000*flt_Imp$scores
#    summary(1000*flt_Imp$scores)
    keep.UTI = names(which(flt_Imp$scores > 0.0001))

tsk_UTIk <- tsk_UTIk$select(keep.UTI)  
  flt_Imp$calculate(tsk_UTIk)
    UTIk_Imp <- to_tibble(flt_Imp$scores)
    UTIk <- autoplot(flt_Imp, title="Prediction for UTIk") +
      labs(title="Prediction for UTIk")
#    UTIk

###################################################
#PU
  flt_Imp$calculate(tsk_PU)
    PU_Imp <- to_tibble(flt_Imp$scores)
    PU <- autoplot(flt_Imp, title="Prediction for Pressure Ulcer") +
      labs(title="Prediction for Pressure Ulcer")
#    PU
#    1000*flt_Imp$scores
#    summary(1000*flt_Imp$scores)
    keep.PU = names(which(flt_Imp$scores > 0.0001))

tsk_PUk <- tsk_PUk$select(keep.PU)  
  flt_Imp$calculate(tsk_PUk)
#    summary(1000*flt_Imp$scores)
    PUk_Imp <- to_tibble(flt_Imp$scores)
    PUk <- autoplot(flt_Imp, title="Prediction for Pressure Ulcerk") +
      labs(title="Prediction for Pressure Ulcerk")
#    PUk

###################################################
#DL
  flt_Imp$calculate(tsk_DL)
    DL_Imp <- to_tibble(flt_Imp$scores)
    DL <- autoplot(flt_Imp, title="Prediction for Delirium") +
      labs(title="Prediction for Delirium")
#    DL
#    1000*flt_Imp$scores
#    summary(1000*flt_Imp$scores)
    keep.DL = names(which(flt_Imp$scores > 0.0001))

tsk_DLk <- tsk_DLk$select(keep.DL)
  flt_Imp$calculate(tsk_DLk)
    DLk_Imp <- to_tibble(flt_Imp$scores)
    DLk <- autoplot(flt_Imp, title="Prediction for Deliriumk") +
      labs(title="Prediction for Deliriumk")
#    DLk

###################################################
#F2M
  flt_Imp$calculate(tsk_F2M)
    F2M_Imp <- to_tibble(flt_Imp$scores)
    F2M <- autoplot(flt_Imp, title="Prediction for F2M") +
      labs(title="Prediction for F2M")
#    F2M
#    summary(1000*flt_Imp$scores)
    keep.F2M = names(which(flt_Imp$scores > 0.0001))

tsk_F2Mk <- tsk_F2Mk$select(keep.F2M)
  flt_Imp$calculate(tsk_F2Mk)
#      summary(1000*flt_Imp$scores)
  F2Mk_Imp <- to_tibble(flt_Imp$scores)
    F2Mk <- autoplot(flt_Imp, title="Prediction for F2Mk") +
      labs(title="Prediction for F2Mk")
#    F2Mk

############
# Save the tasks to disk
saveRDS(tsk_PN, 'data/tsk.PN')
saveRDS(tsk_PNk, 'data/tsk.PNk')
saveRDS(tsk_UTI, 'data/tsk.UTI')
saveRDS(tsk_UTIk, 'data/tsk.UTIk')
saveRDS(tsk_PU, 'data/tsk.PU')
saveRDS(tsk_PUk, 'data/tsk.PUk')
saveRDS(tsk_DL, 'data/tsk.DL')
saveRDS(tsk_DLk, 'data/tsk.DLk')
saveRDS(tsk_F2M, 'data/tsk.F2M')
saveRDS(tsk_F2Mk, 'data/tsk.F2Mk')

############
# Graphs
PN  + PNk
UTI + UTIk
PU  + PUk
DL  + DLk
F2M + F2Mk

###########
# Tibble
PN_Imp <- PN_Imp |> mutate(Source = 'PN_IMP')
UTI_Imp <- UTI_Imp |> mutate(Source = 'UTI_IMP')
PU_Imp <- PU_Imp |> mutate(Source = 'PU_IMP')
DL_Imp <- DL_Imp |> mutate(Source = 'DL_IMP')
F2M_Imp <- F2M_Imp |> mutate(Source = 'F2M_IMP')

PNk_Imp <- PNk_Imp |> mutate(Source = 'PNk_IMP')
UTIk_Imp <- UTIk_Imp |> mutate(Source = 'UTIk_IMP')
PUk_Imp <- PUk_Imp |> mutate(Source = 'PUk_IMP')
DLk_Imp <- DLk_Imp |> mutate(Source = 'DLk_IMP')
F2Mk_Imp <- F2Mk_Imp |> mutate(Source = 'F2Mk_IMP')

Imp <- bind_rows(PN_Imp, PNk_Imp, UTI_Imp, UTIk_Imp,
                 PU_Imp, PUk_Imp, DL_Imp, DLk_Imp,
                 F2M_Imp, F2Mk_Imp) %>%
  select(Source, names, flt_scores ) %>%
  rename(Variable = names) %>%
  rename(Importance = flt_scores) %>%
  mutate(Full  =ifelse(str_detect(Source, 'k_'), 'Reduced' , 'Full'))

Sums <- Imp %>%
  group_by(Variable) %>%
  summarise(Mean_Imp = mean(Importance), Count = n())

Imp <- Imp %>%
  left_join(Sums, by = join_by(Variable)) %>%
  arrange(Mean_Imp) %>%
  mutate(Variable = fct_inorder(as_factor(Variable)))

#######################
#Graph
Imp <- Imp |>
  mutate(Model = ifelse(str_detect(Source, 'k'), 'Restricted', 'Full')) |>
  mutate(Outcome = str_remove(Source, '_IMP$')) |>
  mutate(Outcome = str_remove(Outcome, 'k$'))
  
ggplot(Imp, aes(x=Importance, y = Variable,
                colour=Outcome, shape = Model, group = Outcome)) +
  geom_point(size = 2) +
  geom_line() +
  scale_shape(guide = 'none') +
  facet_wrap(~Model) +
  theme_minimal() +
  labs(title = 'Importance scores by variable and outcome',
       subtitle = 'Full and restricted models')

  rm(PN_Imp, PNk_Imp, UTI_Imp, UTIk_Imp, PU_Imp, PUk_Imp,
     DL_Imp, DLk_Imp, F2M_Imp, F2Mk_Imp)
```

Comparing the full set of variables considered, to those kept on the basis of their importance, there is little difference between the model measures.

## Resampling

We have seen the effect of a single split of the data, separating training and test data completely. Performance is much better on the trained data than on the test data. Resampling is another approach to the same issue. This involves creating multiple training and test sets, and repeating the analysis for each, then aggregating the results of these.

```{r RESAMPLING}
cv10 = rsmp("cv", folds = 10) # 10 folds
```

## Measures

```{r MEASURES}
measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc'))
```

## Classify

### FIT and EVALUATE

#### PN

```{r FIT and EVALUATE PN}
########################################################################
# PN
lrn_featureless$train(tsk_PN, split_PN$train)
  prediction_PN.f = lrn_featureless$predict(tsk_PN, row_ids = split_PN$test)
  prediction_PN.f$score(measures)
  prediction_PN.f$confusion
autoplot(prediction_PN.f, type = 'roc')

lrn_C$train(tsk_PN, row_ids = split_PN$train)
  prediction_PN.rng = lrn_C$predict(tsk_PN, split_PN$test)
  #prediction_PN.rng$score(measures)
  #prediction_PN.rng$confusion
autoplot(prediction_PN.rng) + autoplot(prediction_PN.rng, type = 'roc')
```

#### PNk

```{r FIT and EVALUATE PNk}
########################################################################
# PNk

lrn_C$train(tsk_PNk, row_ids = split_PNk$train)
  prediction_PNk.rng = lrn_C$predict(tsk_PNk, split_PNk$test)
  #prediction_PNk.rng$score(measures)
  #prediction_PNk.rng$confusion
autoplot(prediction_PN.rng) + autoplot(prediction_PNk.rng, type = 'roc')
```

#### UTI

```{r FIT and EVALUATE UTI}
########################################################################
# UTI
lrn_C$train(tsk_UTI, row_ids = split_UTI$train)
  prediction_UTI.rng = lrn_C$predict(tsk_UTI, split_UTI$test)
  #prediction_UTI.rng$score(measures)
  #prediction_UTI.rng$confusion
autoplot(prediction_UTI.rng) + autoplot(prediction_UTI.rng, type = 'roc')
```

#### UTIk

```{r FIT and EVALUATE UTIk}
########################################################################
# UTIk

lrn_C$train(tsk_UTIk, row_ids = split_UTIk$train)
  prediction_UTIk.rng = lrn_C$predict(tsk_UTIk, split_UTIk$test)
  #prediction_UTIk.rng$score(measures)
  #prediction_UTIk.rng$confusion
autoplot(prediction_UTIk.rng) + autoplot(prediction_UTIk.rng, type = 'roc')
```

#### PU

```{r FIT and EVALUATE PU}
########################################################################
# PU
lrn_C$train(tsk_PU, row_ids = split_PU$train)
  prediction_PU.rng = lrn_C$predict(tsk_PU, split_PU$test)
  #prediction_PU.rng$score(measures)
  #prediction_PU.rng$confusion
autoplot(prediction_PU.rng, type = 'roc')
```

#### PUk

```{r FIT and EVALUATE PUk}
########################################################################
# PUk

lrn_C$train(tsk_PUk, row_ids = split_PUk$train)
  prediction_PUk.rng = lrn_C$predict(tsk_PUk, split_PUk$test)
  #prediction_PUk.rng$score(measures)
  #prediction_PUk.rng$confusion
autoplot(prediction_PUk.rng, type = 'roc')
```

#### DL

```{r FIT and EVALUATE DL}
########################################################################
# DL
lrn_C$train(tsk_DL, row_ids = split_DL$train)
  prediction_DL.rng = lrn_C$predict(tsk_DL, split_DL$test)
  #prediction_DL.rng$score(measures)
  #prediction_DL.rng$confusion
autoplot(prediction_DL.rng, type = 'roc')
```

#### DLk

```{r FIT and EVALUATE DLk}
########################################################################
# DLk

lrn_C$train(tsk_DLk, row_ids = split_DLk$train)
  prediction_DLk.rng = lrn_C$predict(tsk_DLk, split_DLk$test)
  #prediction_DLk.rng$score(measures)
  #prediction_DLk.rng$confusion
autoplot(prediction_DLk.rng, type = 'roc')
```

#### F2M

```{r FIT and EVALUATE F2M}
########################################################################
# F2M
lrn_C$train(tsk_F2M, row_ids = split_F2M$train)
  prediction_F2M.rng = lrn_C$predict(tsk_F2M, split_F2M$test)
  #prediction_F2M.rng$score(measures)
  #prediction_F2M.rng$confusion
autoplot(prediction_F2M.rng, type = 'roc')
```

#### F2Mk

```{r FIT and EVALUATE F2Mk}
########################################################################
# F2Mk

lrn_C$train(tsk_F2Mk, row_ids = split_F2Mk$train)
  prediction_F2Mk.rng = lrn_C$predict(tsk_F2Mk, split_F2Mk$test)
  #prediction_F2Mk.rng$score(measures)
  #prediction_F2Mk.rng$confusion
autoplot(prediction_F2Mk.rng, type = 'roc',title='Failure to maintain - restricted predictor set') + labs(title='Failure to maintain - restricted predictor set')
```

```{r Measures for all models}
##############################################################
# Measures
# 
PN_measures <- prediction_PN.rng$score(measures)
PNk_measures <- prediction_PNk.rng$score(measures) # Better
UTI_measures <- prediction_UTI.rng$score(measures)
UTIk_measures <- prediction_UTIk.rng$score(measures) # Notably worse
PU_measures <- prediction_PU.rng$score(measures)
PUk_measures <- prediction_PUk.rng$score(measures) # Notably better
DL_measures <- prediction_DL.rng$score(measures) # Better
DLk_measures <- prediction_DLk.rng$score(measures) 
F2M_measures <- prediction_F2M.rng$score(measures) # Better
F2Mk_measures <- prediction_F2Mk.rng$score(measures)

## Turn into a tibble
model_list <- c('PN', 'PNk', 'UTI', 'UTIk', 'PU', 'PUk',
               'DL', 'DLk', 'F2M', 'F2Mk')

MEASURES <- as_tibble(bind_rows(PN_measures, PNk_measures,
                    UTI_measures, UTIk_measures,
                    PU_measures, PUk_measures,
                    DL_measures, DLk_measures,
                    F2M_measures, F2Mk_measures)) |>
  bind_cols(model_list) |>
  rename(Model = `...5`) |>
  select(Model,classif.auc:classif.acc) |>
  mutate(across(where(is.numeric), ~round(.,2)))
  

kable(MEASURES, caption = 'Measures across all 10 models')
```

## Optimise mtry for ranger models

```{r Ancillary function for graphs}

GRAPH_TUNING <- function(df, TITLE = 'TITLE'){
    # Maximise
    AUC <- ggplot(data = Tuner_df,
          aes(x = mtry,
              y = classif.auc)) +
        geom_smooth(colour = 'red')
    #Minimise
    BBRIER <- ggplot(data = Tuner_df,
                 aes(x = mtry,
                     y = classif.bbrier)) +
        geom_smooth(colour = 'green')
    #Maximise
    ACC <- ggplot(data = Tuner_df,
              aes(x = mtry,
                  y = classif.acc)) +
        geom_smooth(colour = 'red')
    #Minimise
    LOGLOSS <- ggplot(data = Tuner_df,
                  aes(x = mtry,
                      y = classif.logloss)) +
        geom_smooth(colour = 'green')
    IMAGE <- AUC + BBRIER + ACC + LOGLOSS +
        plot_annotation(title = TITLE) +
        plot_layout(axis_titles = 'collect')
return(IMAGE)
}

```

### PN

```{r OPTIMISE PN, eval = TRUE}

instance_T = ti(
    task = tsk_PN,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
#instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

PN_image <- GRAPH_TUNING(Tuner_df, "PN")
PN_image
ggsave("image/PN_image.png",PN_image)

```

### PNk

```{r OPTIMISE PNk, eval = TRUE}

instance_T = ti(
    task = tsk_PNk,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
#instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

PNk_image <- GRAPH_TUNING(Tuner_df, "PNk")
PNk_image
ggsave("image/PNk_image.png",PNk_image)

```

### UTI

```{r OPTIMISE UTI, eval = TRUE}

instance_T = ti(
    task = tsk_UTI,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
#instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))



UTI_image <- GRAPH_TUNING(Tuner_df, "UTI")
UTI_image
ggsave("image/UTI_image.png",UTI_image)

```

### UTIk

```{r OPTIMISE UTIk, eval = TRUE}

instance_T = ti(
    task = tsk_UTIk,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
#instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

UTIk_image <- GRAPH_TUNING(Tuner_df, "UTIk")
UTIk_image
ggsave("image/UTIk_image.png",UTIk_image)

```

### PU

```{r OPTIMISE PU, eval = TRUE}

instance_T = ti(
    task = tsk_PU,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
#instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

PU_image <- GRAPH_TUNING(Tuner_df, "PU")
PU_image
ggsave("image/PU_image.png",PU_image)

```

### PUk

```{r OPTIMISE PUk, eval = TRUE}

instance_T = ti(
    task = tsk_PUk,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
#instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

PUk_image <- GRAPH_TUNING(Tuner_df, "PUk")
PUk_image
ggsave("image/PUk_image.png",PUk_image)

```

### DL

```{r OPTIMISE DL, eval = TRUE}

instance_T = ti(
    task = tsk_DL,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
#instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

DL_image <- GRAPH_TUNING(Tuner_df, "DL")
DL_image
ggsave("image/DL_image.png",DL_image)

```

### DLk

```{r OPTIMISE DLk, eval = TRUE}

instance_T = ti(
    task = tsk_DLk,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
#instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

DLk_image <- GRAPH_TUNING(Tuner_df, "DLk")
DLk_image
ggsave("image/DLk_image.png",DLk_image)

```

### F2M

```{r OPTIMISE F2M, eval = TRUE}

instance_T = ti(
    task = tsk_F2M,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
#instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

F2M_image <- GRAPH_TUNING(Tuner_df, "F2M")
F2M_image
ggsave("image/F2M_image.png",F2M_image)

```

### F2Mk

```{r OPTIMISE F2Mk, eval = TRUE}

instance_T = ti(
    task = tsk_F2Mk,
    learner = lrn_T,
    resampling = rsmp("cv", folds = 10),
    measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc')), #measures,
    terminator = trm("evals", n_evals=200) # For tuner "random_search"
)
#instance_T

tuner = tnr("random_search")
tuner
future::plan(multisession, workers = 10)
tuner$optimize(instance_T)

Tuner_df <- as_tibble(
    as.data.table(instance_T$archive))

F2Mk_image <- GRAPH_TUNING(Tuner_df, "F2Mk")
F2Mk_image
ggsave("image/F2Mk_image.png",F2Mk_image)

```

### Merge graphs

```{r Merge optimisation graphs, eval = TRUE}
PN_image / PNk_image
UTI_image / UTIk_image
PU_image / PUk_image
DL_image / DLk_image
F2M_image / F2Mk_image
```

```{r Set mtry for each model}
#####################
# Set mtry
# 

mtry_PN   =  7
mtry_PNk  =  4
mtry_UTI  =  4
mtry_UTIk =  3
mtry_PU   = 10
mtry_PUk  =  5
mtry_DL   =  4
mtry_DLk  =  3
mtry_F2M  =  8
mtry_F2Mk =  6
```

Looking at these we come up with the following suggestions for mtry

* PN   7
* PNk  4
* UTI  4
* UTIk 3
* PU  10
* PUk  5
* DL   4
* DLk  3
* F2M  8
* F2Mk 6

Note that the scale of some of the graphs suggest very little effect of mtry, in that some of the parameters change little for any value of mtry studied.

# Fit optimised models

We now have five models to fit and predict from, the five restricted models, and the five full models each with a unique optimised mtry.

## Define learners

```{r LEARNERS for optimised models}
lrn_featureless = lrn('classif.featureless',
                      predict_type = 'prob')

lrn_T = lrn('classif.ranger',
            predict_type = "prob",
            num.trees = 2000, # Determined in earlier experiments
            mtry = 3, # Assessed above
            importance = "permutation" # Most sensible choice
            )
#lrn_T

setMtry <- function(lrn, mtry = 10) {
  lrn$param_set$values =
    mlr3misc::insert_named(lrn$param_set$values,
                           list(mtry=mtry))
return(lrn)
}

lrn_T <- setMtry(lrn_T, 15)
#lrn_T
```

# Run Models

## Resampling

```{r Run all 5 full models for the two learners}
rr_PN.f    = resample(tsk_PN, lrn_featureless, cv10, store_models = TRUE, store_backends =TRUE)

    lrn_T <- setMtry(lrn_T, mtry_PN)
rr_PN.rng  = resample(tsk_PN, lrn_T, cv10, store_models = TRUE, store_backends =TRUE)

rr_UTI.f   = resample(tsk_UTI, lrn_featureless, cv10, store_models = TRUE, store_backends =TRUE)

        lrn_T <- setMtry(lrn_T, mtry_UTI)
rr_UTI.rng = resample(tsk_UTI, lrn_T, cv10, store_models = TRUE, store_backends =TRUE)


rr_PU.f    = resample(tsk_PU, lrn_featureless, cv10, store_models = TRUE, store_backends =TRUE)

    lrn_T <- setMtry(lrn_T, mtry_PU)
rr_PU.rng  = resample(tsk_PU, lrn_T, cv10, store_models = TRUE, store_backends =TRUE)


rr_DL.f    = resample(tsk_DLk, lrn_featureless, cv10, store_models = TRUE, store_backends =TRUE)

    lrn_T <- setMtry(lrn_T, mtry_DL)
rr_DL.rng  = resample(tsk_DL, lrn_T, cv10, store_models = TRUE, store_backends =TRUE)


rr_F2M.f   = resample(tsk_F2M, lrn_featureless, cv10, store_models = TRUE, store_backends =TRUE)

    lrn_T <- setMtry(lrn_T, mtry_F2M)
rr_F2M.rng = resample(tsk_F2M, lrn_T, cv10, store_models = TRUE, store_backends =TRUE)

#lrn_T
```

```{r Run all 5 keep models for the two learners}
rr_PNk.f    = resample(tsk_PNk, lrn_featureless, cv10, store_models = TRUE, store_backends =TRUE)

    lrn_T <- setMtry(lrn_T, mtry_PNk)
rr_PNk.rng  = resample(tsk_PNk, lrn_T, cv10, store_models = TRUE, store_backends =TRUE)


rr_UTIk.f   = resample(tsk_UTIk, lrn_featureless, cv10, store_models = TRUE, store_backends =TRUE)

    lrn_T <- setMtry(lrn_T, mtry_UTIk)
rr_UTIk.rng = resample(tsk_UTIk, lrn_T, cv10, store_models = TRUE, store_backends =TRUE)


rr_PUk.f    = resample(tsk_PUk, lrn_featureless, cv10, store_models = TRUE, store_backends =TRUE)

    lrn_T <- setMtry(lrn_T, mtry_PUk)
rr_PUk.rng  = resample(tsk_PUk, lrn_T, cv10, store_models = TRUE, store_backends =TRUE)


rr_DLk.f    = resample(tsk_DLk, lrn_featureless, cv10, store_models = TRUE, store_backends =TRUE)

    lrn_T <- setMtry(lrn_T, mtry_DLk)
rr_DLk.rng  = resample(tsk_DLk, lrn_T, cv10, store_models = TRUE, store_backends =TRUE)


rr_F2Mk.f   = resample(tsk_F2Mk, lrn_featureless, cv10, store_models = TRUE, store_backends =TRUE)

    lrn_T <- setMtry(lrn_T, mtry_F2Mk)
rr_F2Mk.rng = resample(tsk_F2Mk, lrn_T, cv10, store_models = TRUE, store_backends =TRUE)

#lrn_T
```

### ROC curves

### For full models

```{r ROC curves}
roc_PN <- autoplot(rr_PN.rng, type = 'roc')
roc_UTI <- autoplot(rr_UTI.rng, type = 'roc')
roc_PU <- autoplot(rr_PU.rng, type = 'roc')
roc_DL <- autoplot(rr_DL.rng, type = 'roc')
roc_F2M <- autoplot(rr_F2M.rng, type = 'roc')

design = 
'12
345'

roc_PN + roc_UTI + plot_spacer()+ roc_PU + roc_DL + roc_F2M +
  plot_layout(ncol = 3, axes = 'collect', axis_titles = 'collect') +
  plot_annotation(title = 'ROC curves for full optimised models',
                  tag_levels = list(c('PN','UTI','PU','DL','F2M')))

```

### For limited models

```{r ROCk curves}
roc_PNk <- autoplot(rr_PNk.rng, type = 'roc')
roc_UTIk <- autoplot(rr_UTIk.rng, type = 'roc')
roc_PUk <- autoplot(rr_PUk.rng, type = 'roc')
roc_DLk <- autoplot(rr_DLk.rng, type = 'roc')
roc_F2Mk <- autoplot(rr_F2Mk.rng, type = 'roc')

design = 
'12
345'

roc_PNk + roc_UTIk + plot_spacer()+ roc_PUk + roc_DLk + roc_F2Mk +
  plot_layout(ncol = 3, axes = 'collect', axis_titles = 'collect') +
  plot_annotation(title = 'ROC curves for keep optimised models',
                  tag_levels = list(c('PNk','UTIk','PUk','DLk','F2Mk')))


```

### By outcome, comparing full and restricted models

```{r ROC curves per outcome}

roc_PN + roc_PNk  +
  plot_layout(ncol = 2, axes = 'collect', axis_titles = 'collect') +
  plot_annotation(title = 'ROC curves for PN outcome',
                  tag_levels = list(c('PN','PNk')))

roc_UTI + roc_UTIk  +
  plot_layout(ncol = 2, axes = 'collect', axis_titles = 'collect') +
  plot_annotation(title = 'ROC curves for UTI outcome',
                  tag_levels = list(c('UTI','UTIk')))


roc_PU + roc_PUk  +
  plot_layout(ncol = 2, axes = 'collect', axis_titles = 'collect') +
  plot_annotation(title = 'ROC curves for PU outcome',
                  tag_levels = list(c('PU','PUk')))


roc_DL + roc_DLk  +
  plot_layout(ncol = 2, axes = 'collect', axis_titles = 'collect') +
  plot_annotation(title = 'ROC curves for DL outcome',
                  tag_levels = list(c('DL','DLk')))


roc_F2M + roc_F2Mk  +
  plot_layout(ncol = 2, axes = 'collect', axis_titles = 'collect') +
  plot_annotation(title = 'ROC curves for F2M outcome',
                  tag_levels = list(c('F2M','F2Mk')))


```

## Model performance measures from resampling

```{r Resampling measures}
## PN
rs_PN <- rr_PN.rng$aggregate(measures)
rs_PNk <- rr_PNk.rng$aggregate(measures)

##UTI
rs_UTI <- rr_UTI.rng$aggregate(measures)
rs_UTIk <- rr_UTIk.rng$aggregate(measures)

##PU
rs_PU <- rr_PU.rng$aggregate(measures)
rs_PUk <- rr_PUk.rng$aggregate(measures)

## DL
rs_DL <- rr_DL.rng$aggregate(measures)
rs_DLk <- rr_DLk.rng$aggregate(measures)

##F2M
rs_F2M <- rr_F2M.rng$aggregate(measures)
rs_F2Mk <- rr_F2Mk.rng$aggregate(measures)

## Turn into a tibble
model_list <- c('PN', 'PNk', 'UTI', 'UTIk', 'PU', 'PUk',
               'DL', 'DLk', 'F2M', 'F2Mk')

rs_MEASURES <- as_tibble(bind_rows(rs_PN, rs_PNk,
                    rs_UTI, rs_UTIk,
                    rs_PU, rs_PUk,
                    rs_DL, rs_DLk,
                    rs_F2M, rs_F2Mk)) |>
  bind_cols(model_list) |>
  rename(Model = `...5`) |>
  select(Model,classif.auc:classif.acc) |>
  mutate(across(where(is.numeric), ~round(.,2)))
  

kable(rs_MEASURES, caption = 'Resampling measures across all 10 models')
```

## Resampling predictions on test data and confusion matrices

The requirement is to predict the total number of adverse events as well as possible. Direct inspection of the ROC curves suggests that around 0.25 to 0.35 is about right for most. UTI is probably the worst predicted outcome. We use a more formal procedure for the adjusted thresholds, setting it where the difference between sensitivity and specificity are as small as possible.

```{r ancillary function for threshold}
SELECT_THRESHOLD <- function(df) {
  df <- df %>%
  rowid_to_column(var = 'Threshold') %>%
  mutate(Threshold = Threshold/n()) %>%
  mutate(Specificity =  1 - x) %>%
  mutate(Sensitivity = y) %>%
  mutate(Diff = Sensitivity - Specificity) %>%
  slice(which.min(abs(Diff))) %>%
  select(Threshold)
    
return(df)
}
```

```{r Resampling predictions}

pred_PN <- rr_PN.rng$prediction()
pred_UTI <- rr_UTI.rng$prediction()
pred_PU <- rr_PU.rng$prediction()
pred_DL <- rr_DL.rng$prediction()
pred_F2M <- rr_F2M.rng$prediction()

pred_PNk <- rr_PNk.rng$prediction()
pred_UTIk <- rr_UTIk.rng$prediction()
pred_PUk <- rr_PUk.rng$prediction()
pred_DLk <- rr_DLk.rng$prediction()
pred_F2Mk <- rr_F2Mk.rng$prediction()
```

###PN

```{r Resampling prediction PN}
## PN
Threshold <- SELECT_THRESHOLD(roc_PN$data)
#  Threshold
  pred_PN$set_threshold(Threshold$Threshold)

autoplot(pred_PN) + labs(title = paste0('PN Threshold ', round(Threshold$Threshold,3)))
```

## PNk

```{r Resampling prediction PNk}
## PNk
Threshold <- SELECT_THRESHOLD(roc_PNk$data)
#  Threshold
  pred_PNk$set_threshold(Threshold$Threshold)

autoplot(pred_PNk) + labs(title = paste0('PNk Threshold ', round(Threshold$Threshold,3)))

```

## UTI

```{r Resampling prediction UTI}
## UTI
Threshold <- SELECT_THRESHOLD(roc_UTI$data)
#  Threshold
  pred_UTI$set_threshold(Threshold$Threshold)

autoplot(pred_UTI) + labs(title = paste0('UTI Threshold ', round(Threshold$Threshold,3)))

```

## UTIk

```{r Resampling prediction UTIk}
## UTIk
Threshold <- SELECT_THRESHOLD(roc_UTIk$data)
#  Threshold
  pred_UTIk$set_threshold(Threshold$Threshold)

autoplot(pred_UTIk) + labs(title = paste0('UTIk Threshold ', round(Threshold$Threshold,3)))

```

## PU

```{r Resampling prediction PU}
## PU
Threshold <- SELECT_THRESHOLD(roc_PU$data)
#  Threshold
  pred_PU$set_threshold(Threshold$Threshold)

autoplot(pred_PU) + labs(title = paste0('PU Threshold ', round(Threshold$Threshold,3)))

```

## PUk

```{r Resampling prediction PUk}
## PUk
Threshold <- SELECT_THRESHOLD(roc_PUk$data)
#  Threshold
  pred_PUk$set_threshold(Threshold$Threshold)

autoplot(pred_PUk) + labs(title = paste0('PUk Threshold ', round(Threshold$Threshold,3)))

```

## DL

```{r Resampling prediction DL}
## DL
Threshold <- SELECT_THRESHOLD(roc_DL$data)
#  Threshold
  pred_DL$set_threshold(Threshold$Threshold)

autoplot(pred_DL) + labs(title = paste0('DL Threshold ', round(Threshold$Threshold,3)))

```

## DLk

```{r Resampling prediction DLk}
## DLk
Threshold <- SELECT_THRESHOLD(roc_DLk$data)
#  Threshold
  pred_DLk$set_threshold(Threshold$Threshold)

autoplot(pred_DLk) + labs(title = paste0('DLk Threshold ', round(Threshold$Threshold,3)))

```

## F2M

```{r Resampling prediction F2M}
## F2M
Threshold <- SELECT_THRESHOLD(roc_F2M$data)
#  Threshold
  pred_F2M$set_threshold(Threshold$Threshold)

autoplot(pred_F2M) + labs(title = paste0('F2M Threshold ', round(Threshold$Threshold,3)))

```

## F2Mk

```{r Resampling prediction F2Mk}
## F2Mk
Threshold <- SELECT_THRESHOLD(roc_F2Mk$data)
#  Threshold
  pred_F2Mk$set_threshold(Threshold$Threshold)

autoplot(pred_F2Mk) + labs(title = paste0('F2Mk Threshold ', round(Threshold$Threshold,3)))

```

# Training and predicting

Thresholds are set to roughly equalise the true number of events in the training set, and the number of events predicted. The threshold is set where the difference between sensitivity and specificity is as small as possible.

Predictions are done using the test set, not the training set.

## PN

```{r PN_train_predict}
      lrn_T <- setMtry(lrn_T, mtry_PN)
PN_trained <- lrn_T$train(tsk_PN, row_ids = split_PN$train)
  PN_pred <- PN_trained$predict(tsk_PN, row_ids = split_PN$test)

ROC <- autoplot(PN_pred, type='roc')  
  ROC
  Threshold <- SELECT_THRESHOLD(ROC$data)
#  Threshold
  PN_pred$set_threshold(Threshold$Threshold)

  threshold_PN = Threshold$Threshold
  
PN_pred.H <- PN_trained$predict_newdata(HIPE)
```

## PNk

```{r PNK_train_predict}
      lrn_T <- setMtry(lrn_T, mtry_PNk)
PNk_trained <- lrn_T$train(tsk_PNk, row_ids = split_PNk$train)
  PNk_pred <- PNk_trained$predict(tsk_PNk, row_ids = split_PNk$test)
ROC <- autoplot(PNk_pred, type='roc')  
  ROC
  Threshold <- SELECT_THRESHOLD(ROC$data)
#  Threshold
  PNk_pred$set_threshold(Threshold$Threshold)

  threshold_PNk = Threshold$Threshold
  
PNk_pred.H <- PNk_trained$predict_newdata(HIPE)
```

## UTI

```{r UTI_train_predict}
      lrn_T <- setMtry(lrn_T, mtry_UTI)
UTI_trained <- lrn_T$train(tsk_UTI, row_ids = split_UTI$train)
  UTI_pred <- UTI_trained$predict(tsk_UTI, row_ids = split_UTI$test)
ROC <- autoplot(UTI_pred, type='roc')  
  ROC
  Threshold <- SELECT_THRESHOLD(ROC$data)
#  Threshold
  UTI_pred$set_threshold(Threshold$Threshold)

threshold_UTI = Threshold$Threshold

UTI_pred.H <- UTI_trained$predict_newdata(HIPE)
```

## UTIk

```{r UTIk_train_predict}
      lrn_T <- setMtry(lrn_T, mtry_UTIk)
UTIk_trained <- lrn_T$train(tsk_UTIk, row_ids = split_UTIk$train)
  UTIk_pred <- UTIk_trained$predict(tsk_UTIk, row_ids = split_UTIk$test)
ROC <- autoplot(UTIk_pred, type='roc')  
  ROC
  Threshold <- SELECT_THRESHOLD(ROC$data)
#  Threshold
  UTIk_pred$set_threshold(Threshold$Threshold)

threshold_UTIk = Threshold$Threshold

UTIk_pred.H <- UTIk_trained$predict_newdata(HIPE)
```

## PU

```{r PU_train_predict}
      lrn_T <- setMtry(lrn_T, mtry_PU)
PU_trained <- lrn_T$train(tsk_PU, row_ids = split_PU$train)
  PU_pred <- PU_trained$predict(tsk_PU, row_ids = split_PU$test)
ROC <- autoplot(PU_pred, type='roc')  
  ROC
  Threshold <- SELECT_THRESHOLD(ROC$data)
#  Threshold
  PU_pred$set_threshold(Threshold$Threshold)

threshold_PU = Threshold$Threshold

PU_pred.H <- PU_trained$predict_newdata(HIPE)
```

## PUk

```{r PUk_train_predict}
      lrn_T <- setMtry(lrn_T, mtry_PUk)
PUk_trained <- lrn_T$train(tsk_PUk, row_ids = split_PUk$train)
  PUk_pred <- PUk_trained$predict(tsk_PUk, row_ids = split_PUk$test)
ROC <- autoplot(PUk_pred, type='roc')  
  ROC
  Threshold <- SELECT_THRESHOLD(ROC$data)
#  Threshold
  PUk_pred$set_threshold(Threshold$Threshold)

threshold_PUk = Threshold$Threshold

PUk_pred.H <- PUk_trained$predict_newdata(HIPE)
```

## DL

```{r DL_train_predict}
      lrn_T <- setMtry(lrn_T, mtry_DL)
DL_trained <- lrn_T$train(tsk_DL, row_ids = split_DL$train)
  DL_pred <- DL_trained$predict(tsk_DL, row_ids = split_DL$test)
ROC <- autoplot(DL_pred, type='roc')  
  ROC
  Threshold <- SELECT_THRESHOLD(ROC$data)
#  Threshold
  DL_pred$set_threshold(Threshold$Threshold)

threshold_DL = Threshold$Threshold

DL_pred.H <- DL_trained$predict_newdata(HIPE)
```

## DLk

```{r DLk_train_predict}
      lrn_T <- setMtry(lrn_T, mtry_DLk)
DLk_trained <- lrn_T$train(tsk_DLk, row_ids = split_DLk$train)
  DLk_pred <- DLk_trained$predict(tsk_DLk, row_ids = split_DLk$test)
ROC <- autoplot(DLk_pred, type='roc')  
  ROC
  Threshold <- SELECT_THRESHOLD(ROC$data)
#  Threshold
  DLk_pred$set_threshold(Threshold$Threshold)

threshold_DLk = Threshold$Threshold

DLk_pred.H <- DLk_trained$predict_newdata(HIPE)
```

## F2M

```{r F2M_train_predict}
      lrn_T <- setMtry(lrn_T, mtry_F2M)
F2M_trained <- lrn_T$train(tsk_F2M, row_ids = split_F2M$train)
  F2M_pred <- F2M_trained$predict(tsk_F2M, row_ids = split_F2M$test)
ROC <- autoplot(F2M_pred, type='roc')  
  ROC
  Threshold <- SELECT_THRESHOLD(ROC$data)
#  Threshold
  F2M_pred$set_threshold(Threshold$Threshold)

threshold_F2M = Threshold$Threshold

F2M_pred.H <- F2M_trained$predict_newdata(HIPE)
```

## F2Mk

```{r F2Mk_train_predict}
      lrn_T <- setMtry(lrn_T, mtry_F2Mk)
F2Mk_trained <- lrn_T$train(tsk_F2Mk, row_ids = split_F2Mk$train)
  F2Mk_pred <- F2Mk_trained$predict(tsk_F2Mk, row_ids = split_F2Mk$test)
ROC <- autoplot(F2Mk_pred, type='roc')  
  ROC
  Threshold <- SELECT_THRESHOLD(ROC$data)
#  Threshold
  F2Mk_pred$set_threshold(Threshold$Threshold)

threshold_F2Mk = Threshold$Threshold

F2Mk_pred.H <- F2Mk_trained$predict_newdata(HIPE)
```

# Measures

## Training and prediction meaures (grouped)

```{r Training and prediction measures}
## PN
opt_PN <- PN_pred$score(measures)
opt_PNk <- PNk_pred$score(measures)

##UTI
opt_UTI <- UTI_pred$score(measures)
opt_UTIk <- UTIk_pred$score(measures)

##PU
opt_PU <- PU_pred$score(measures)
opt_PUk <- PUk_pred$score(measures)

## DL
opt_DL <- DL_pred$score(measures)
opt_DLk <- DLk_pred$score(measures)

##F2M
opt_F2M <- F2M_pred$score(measures)
opt_F2Mk <- F2Mk_pred$score(measures)

## Turn into a tibble
model_list <- c('PN', 'PNk', 'UTI', 'UTIk', 'PU', 'PUk',
               'DL', 'DLk', 'F2M', 'F2Mk')

opt_MEASURES <- as_tibble(bind_rows(opt_PN, opt_PNk,
                    opt_UTI, opt_UTIk,
                    opt_PU, opt_PUk,
                    opt_DL, opt_DLk,
                    opt_F2M, opt_F2Mk)) |>
  bind_cols(model_list) |>
  rename(Model = `...5`) |>
  select(Model,classif.auc:classif.acc) |>
  mutate(across(where(is.numeric), ~round(.,2)))
  

kable(opt_MEASURES, caption = 'Measures across all 10 optimised models')

```

## Importance (again)

```{r Final importance}
#flt_Imp = flt("importance", learner = lrn_T)
#flt_Imp = flt("importance", learner = lrn_C)

#Ancillary function to store importance scores in a tibble
to_tibble <- function(flt_scores) {
  DF <- as.data.frame(flt_scores)
  DF$names <- row.names(DF)
  DF <- as_tibble(DF)
  return(DF)
}

###################################################
#PN
  lrn_T <- setMtry(lrn_T, mtry_PN)
flt_Imp$calculate(tsk_PN)
    PN_Imp <- to_tibble(flt_Imp$scores)
    PN <- autoplot(flt_Imp, title="Prediction for pneumonia") +
      labs(title="Prediction for Pneumonia")
#    PN
#    summary(1000*flt_Imp$scores)
    keep.PN = names(which(flt_Imp$scores > 0.0001))

  lrn_T <- setMtry(lrn_T, mtry_PNk)
  flt_Imp$calculate(tsk_PNk)
    PNk_Imp <- to_tibble(flt_Imp$scores)
#    summary(1000*flt_Imp$scores)
    PNk <- autoplot(flt_Imp, title="Prediction for pneumoniak") +
      labs(title="Prediction for Pneumoniak")
#    PNk

###################################################
#UTI
  lrn_T <- setMtry(lrn_T, mtry_UTI)
flt_Imp$calculate(tsk_UTI)
    UTI_Imp <- to_tibble(flt_Imp$scores)
    UTI <- autoplot(flt_Imp, title="Prediction for UTI") +
      labs(title="Prediction for UTI")
#    UTI
#    1000*flt_Imp$scores
#    summary(1000*flt_Imp$scores)
    keep.UTI = names(which(flt_Imp$scores > 0.0001))

    lrn_T <- setMtry(lrn_T, mtry_UTI)
  lrn_T <- setMtry(lrn_T, mtry_UTIk)

  flt_Imp$calculate(tsk_UTIk)
    UTIk_Imp <- to_tibble(flt_Imp$scores)
    UTIk <- autoplot(flt_Imp, title="Prediction for UTIk") +
      labs(title="Prediction for UTIk")
#   UTIk

###################################################
#PU
  lrn_T <- setMtry(lrn_T, mtry_PU)
flt_Imp$calculate(tsk_PU)
    PU_Imp <- to_tibble(flt_Imp$scores)
    PU <- autoplot(flt_Imp, title="Prediction for Pressure Ulcer") +
      labs(title="Prediction for Pressure Ulcer")
#    PU
#    1000*flt_Imp$scores
#    summary(1000*flt_Imp$scores)
    keep.PU = names(which(flt_Imp$scores > 0.0001))

  lrn_T <- setMtry(lrn_T, mtry_PUk)
  flt_Imp$calculate(tsk_PUk)
    summary(1000*flt_Imp$scores)
    PUk_Imp <- to_tibble(flt_Imp$scores)
    PUk <- autoplot(flt_Imp, title="Prediction for Pressure Ulcerk") +
      labs(title="Prediction for Pressure Ulcerk")
#    PUk

###################################################
#DL
  lrn_T <- setMtry(lrn_T, mtry_DL)
flt_Imp$calculate(tsk_DL)
    DL_Imp <- to_tibble(flt_Imp$scores)
    DL <- autoplot(flt_Imp, title="Prediction for Delirium") +
      labs(title="Prediction for Delirium")
#    DL
#    1000*flt_Imp$scores
#    summary(1000*flt_Imp$scores)
    keep.DL = names(which(flt_Imp$scores > 0.0001))

  lrn_T <- setMtry(lrn_T, mtry_DLk)
  flt_Imp$calculate(tsk_DLk)
    DLk_Imp <- to_tibble(flt_Imp$scores)
    DLk <- autoplot(flt_Imp, title="Prediction for Deliriumk") +
      labs(title="Prediction for Deliriumk")
#    DLk

###################################################
#F2M
  lrn_T <- setMtry(lrn_T, mtry_F2M)
flt_Imp$calculate(tsk_F2M)
    F2M_Imp <- to_tibble(flt_Imp$scores)
    F2M <- autoplot(flt_Imp, title="Prediction for F2M") +
      labs(title="Prediction for F2M")
#    F2M
#    summary(1000*flt_Imp$scores)
    keep.F2M = names(which(flt_Imp$scores > 0.0001))

  lrn_T <- setMtry(lrn_T, mtry_F2Mk)
  flt_Imp$calculate(tsk_F2Mk)
      summary(1000*flt_Imp$scores)
  F2Mk_Imp <- to_tibble(flt_Imp$scores)
    F2Mk <- autoplot(flt_Imp, title="Prediction for F2Mk") +
      labs(title="Prediction for F2Mk")
#    F2Mk

############
# Graphs
PN  + PNk
UTI + UTIk
PU  + PUk
DL  + DLk
F2M + F2Mk

###########
# Tibble
PN_Imp <- PN_Imp |> mutate(Source = 'PN_IMP')
UTI_Imp <- UTI_Imp |> mutate(Source = 'UTI_IMP')
PU_Imp <- PU_Imp |> mutate(Source = 'PU_IMP')
DL_Imp <- DL_Imp |> mutate(Source = 'DL_IMP')
F2M_Imp <- F2M_Imp |> mutate(Source = 'F2M_IMP')

PNk_Imp <- PNk_Imp |> mutate(Source = 'PNk_IMP')
UTIk_Imp <- UTIk_Imp |> mutate(Source = 'UTIk_IMP')
PUk_Imp <- PUk_Imp |> mutate(Source = 'PUk_IMP')
DLk_Imp <- DLk_Imp |> mutate(Source = 'DLk_IMP')
F2Mk_Imp <- F2Mk_Imp |> mutate(Source = 'F2Mk_IMP')

ImpFinal <- bind_rows(PN_Imp, PNk_Imp, UTI_Imp, UTIk_Imp,
                 PU_Imp, PUk_Imp, DL_Imp, DLk_Imp,
                 F2M_Imp, F2Mk_Imp) %>%
  select(Source, names, flt_scores ) %>%
  rename(Variable = names) %>%
  rename(Importance = flt_scores) %>%
  mutate(Full = ifelse(str_detect(Source, 'k_'), 'Reduced' , 'Full'))

SumsFinal <- ImpFinal %>%
  group_by(Variable) %>%
  summarise(Mean_Imp = mean(Importance), Count = n()) %>%
  arrange(desc(Mean_Imp))

ImpFinal <- ImpFinal %>%
  left_join(Sums, by = join_by(Variable)) %>%
  arrange(Mean_Imp) %>%
  mutate(Variable = fct_inorder(as_factor(Variable)))

#######################
#Graph
ImpFinal <- ImpFinal |>
  mutate(Model = ifelse(str_detect(Source, 'k'), 'Restricted', 'Full')) |>
  mutate(Outcome = str_remove(Source, '_IMP$')) |>
  mutate(Outcome = str_remove(Outcome, 'k$'))
  
ggplot(ImpFinal, aes(x=Importance, y = Variable,
                colour=Outcome, shape = Model, group = Outcome)) +
  geom_point(size = 2) +
  geom_line() +
  scale_shape(guide = 'none') +
  facet_wrap(~Model) +
  theme_minimal() +
  labs(title = 'Importance scores by variable and outcome',
       subtitle = 'Full and restricted optimised models')

  rm(PN_Imp, PNk_Imp, UTI_Imp, UTIk_Imp, PU_Imp, PUk_Imp,
     DL_Imp, DLk_Imp, F2M_Imp, F2Mk_Imp)
```

# HIPE and Chart review predictions

Now we use the models trained on the training set, the thresholds from the test set, and we predict the results for the HIPE data, and for the chart review.

## PN

```{r PN_HIPE_predict}
PN_pred.H$set_threshold(threshold_PN)

autoplot(PN_pred) +  autoplot(PN_pred, type = 'roc') + autoplot(PN_pred.H) + plot_layout(ncol = 2) + plot_annotation(title =  'PN Outcome and Predictions', tag_levels = list(c('Review', '', 'HIPE')))
```

## PNk

```{r PNK_HIPE_predict}
  PNk_pred.H$set_threshold(threshold_PNk)

autoplot(PNk_pred) +  autoplot(PNk_pred, type='roc') + autoplot(PNk_pred.H) + plot_layout(ncol = 2)  +  plot_annotation(title = 'PNk Outcome and Predictions', tag_levels = list(c('Review', '', 'HIPE')))
```

## UTI

```{r UTI_HIPE_predict}
  UTI_pred.H$set_threshold(threshold_UTI)

autoplot(UTI_pred) +  autoplot(UTI_pred, type='roc') + autoplot(UTI_pred.H) + plot_layout(ncol = 2)  +  plot_annotation(title = 'UTI Outcome and Predictions', tag_levels = list(c('Review', '', 'HIPE')))
```

## UTIk

```{r UTIk_HIPE_predict}
  UTIk_pred.H$set_threshold(threshold_UTIk)

autoplot(UTIk_pred) +  autoplot(UTIk_pred, type='roc') + autoplot(UTIk_pred.H) + plot_layout(ncol = 2)  +  plot_annotation(title = 'UTIk Outcome and Predictions', tag_levels = list(c('Review', '', 'HIPE')))
```

## PU

```{r PU_HIPE_predict}
  PU_pred.H$set_threshold(threshold_PU)

autoplot(PU_pred) +  autoplot(PU_pred, type='roc') + autoplot(PU_pred.H) + plot_layout(ncol = 2) +    plot_annotation(title = 'PU Outcome and Predictions', tag_levels = list(c('Review', '', 'HIPE')))
```

## PUk

```{r PUk_HIPE_predict}
  PUk_pred.H$set_threshold(threshold_PUk)

autoplot(PUk_pred) +  autoplot(PUk_pred, type='roc') + autoplot(PUk_pred.H) + plot_layout(ncol = 2)  +  plot_annotation(title = 'PUk Outcome and Predictions', tag_levels = list(c('Review', '', 'HIPE')))
```

## DL

```{r DL_HIPE_predict}
  DL_pred.H$set_threshold(threshold_DL)

autoplot(DL_pred) +  autoplot(DL_pred, type='roc') + autoplot(DL_pred.H) + plot_layout(ncol = 2)  +    plot_annotation(title = 'DL Outcome and Predictions', tag_levels = list(c('Review', '', 'HIPE')))
```

## DLk

```{r DLk_HIPE_predict}
  DLk_pred.H$set_threshold(threshold_DLk)

autoplot(DLk_pred) +  autoplot(DLk_pred, type='roc') + autoplot(DLk_pred.H) + plot_layout(ncol = 2)  +  plot_annotation(title = 'DLk Outcome and Predictions', tag_levels = list(c('Review', '', 'HIPE')))
```

## F2M

```{r F2M_HIPE_predict}
  F2M_pred.H$set_threshold(threshold_F2M)

autoplot(F2M_pred) +  autoplot(F2M_pred, type='roc') + autoplot(F2M_pred.H) + plot_layout(ncol = 2)  +  plot_annotation(title = 'F2M Outcome and Predictions', tag_levels = list(c('Review', '', 'HIPE')))
```

## F2Mk

```{r F2Mk_HIPE_predict}
 F2Mk_pred.H$set_threshold(threshold_F2M)

autoplot(F2Mk_pred) +  autoplot(F2Mk_pred, type='roc') + autoplot(F2Mk_pred.H) + plot_layout(ncol = 2) +  plot_annotation(title = 'F2Mk Outcome and Predictions', tag_levels = list(c('Review', '', 'HIPE')))
```

# Assess the predictions

## Chart review

```{r Accumulate chart review predictions}
dt_PN  <- as_tibble(as.data.table(PN_pred))|>
  mutate(Outcome = 'PN')
dt_PNk <- as_tibble(as.data.table(PNk_pred))|>
  mutate(Outcome = 'PNk')

dt_UTI  <- as_tibble(as.data.table(UTI_pred))|>
  mutate(Outcome = 'UTI')
dt_UTIk <- as_tibble(as.data.table(UTIk_pred))|>
  mutate(Outcome = 'UTIk')

dt_PU  <- as_tibble(as.data.table(PU_pred))|>
  mutate(Outcome = 'PU')
dt_PUk <- as_tibble(as.data.table(PUk_pred))|>
  mutate(Outcome = 'PUk')

dt_DL  <- as_tibble(as.data.table(DL_pred))|>
  mutate(Outcome = 'DL')
dt_DLk <- as_tibble(as.data.table(DLk_pred))|>
  mutate(Outcome = 'DLk')

dt_F2M  <- as_tibble(as.data.table(F2M_pred))|>
  mutate(Outcome = 'F2M')
dt_F2Mk <- as_tibble(as.data.table(F2Mk_pred))|>
  mutate(Outcome = 'F2Mk')

```

## HIPE

```{r Accumulate HIPE predictions}
CODES <- HIPE |>
  select(HospCode, ModelCode) |>
  rowid_to_column()

dt_PN.H = as_tibble(
  bind_cols(
    as.data.table(PN_pred.H$response),
    as.data.table(PN_pred.H$prob))) |>
  rowid_to_column() |>
  left_join(CODES, by = join_by(rowid)) |>
  mutate(Outcome = 'PN')
dt_PNk.H = as_tibble(
  bind_cols(
    as.data.table(PNk_pred.H$response),
    as.data.table(PNk_pred.H$prob))) |>
  rowid_to_column() |>
  left_join(CODES, by = join_by(rowid)) |>
  mutate(Outcome = 'PNk')

dt_UTI.H = as_tibble(
  bind_cols(
    as.data.table(UTI_pred.H$response),
    as.data.table(UTI_pred.H$prob))) |>
  rowid_to_column() |>
  left_join(CODES, by = join_by(rowid)) |>
  mutate(Outcome = 'UTI')
dt_UTIk.H = as_tibble(
  bind_cols(
    as.data.table(UTIk_pred.H$response),
    as.data.table(UTIk_pred.H$prob))) |>
  rowid_to_column() |>
  left_join(CODES, by = join_by(rowid)) |>
  mutate(Outcome = 'UTIk')

dt_PU.H = as_tibble(
  bind_cols(
    as.data.table(PU_pred.H$response),
    as.data.table(PU_pred.H$prob))) |>
  rowid_to_column() |>
  left_join(CODES, by = join_by(rowid)) |>
  mutate(Outcome = 'PU')
dt_PUk.H = as_tibble(
  bind_cols(
    as.data.table(PUk_pred.H$response),
    as.data.table(PUk_pred.H$prob))) |>
  rowid_to_column() |>
  left_join(CODES, by = join_by(rowid)) |>
  mutate(Outcome = 'PUk')

dt_DL.H = as_tibble(
  bind_cols(
    as.data.table(DL_pred.H$response),
    as.data.table(DL_pred.H$prob))) |>
  rowid_to_column() |>
  left_join(CODES, by = join_by(rowid)) |>
  mutate(Outcome = 'DL')
dt_DLk.H = as_tibble(
  bind_cols(
    as.data.table(DLk_pred.H$response),
    as.data.table(DLk_pred.H$prob))) |>
  rowid_to_column() |>
  left_join(CODES, by = join_by(rowid)) |>
  mutate(Outcome = 'DLk')

dt_F2M.H = as_tibble(
  bind_cols(
    as.data.table(F2M_pred.H$response),
    as.data.table(F2M_pred.H$prob))) |>
  rowid_to_column() |>
  left_join(CODES, by = join_by(rowid)) |>
  mutate(Outcome = 'F2M')
dt_F2Mk.H = as_tibble(
  bind_cols(
    as.data.table(F2Mk_pred.H$response),
    as.data.table(F2Mk_pred.H$prob))) |>
  rowid_to_column() |>
  left_join(CODES, by = join_by(rowid)) |>
  mutate(Outcome = 'F2Mk')


```

# Predictions
We've three sets of predictions

* Outcomes, binary Yes, or No (mostly no)
* Probabilities of being a Yes (mostly low)
* Probabilities of being a No (Mostly high)

We want these

* For the review data
* Nationally
* By hospital

The only 'ground truth' we have is the review data.

```{r Summary of Chart Review and HIPE predictions}
########################################
# Chart review
Test_Observations  = nrow(dt_PN)
Test_predictions <- bind_rows(dt_PN, dt_PNk,
                              dt_UTI, dt_UTIk,
                              dt_PU, dt_PUk,
                              dt_DL, dt_DLk,
                              dt_F2M, dt_F2Mk,
                              ) |>
  rename(Prediction = response) |>
  rename(Truth = truth) |>
  mutate(Model = ifelse(str_detect(Outcome, 'k'), 'Restricted', 'Full')) |>
  mutate(Outcome_name = str_remove(Outcome,'k'))

TP.prob <- Test_predictions |>
  group_by(Model, Outcome_name) |>
  summarise(Prob = sum(prob.Yes)/Test_Observations)

Prediction <- Test_predictions |>
  group_by(Outcome) |>
  count(Prediction) %>% 
  mutate(prop = n/sum(n)) %>%
  mutate(pct = round(100*prop, 2)) %>%
  rename(Result = Prediction)

Truth <- Test_predictions |>
  group_by(Outcome) |>
  count(Truth) %>% 
  mutate(prop = n/sum(n)) %>%
  mutate(pct = round(100*prop, 2)) %>%
  rename(Result = Truth)

Chart_review <- Truth |> # Used in error bar graph
  full_join(Prediction,
            by = join_by(Outcome, Result),
            suffix = c('.True','.Predicted')) |>
    pivot_longer(cols = n.True:pct.Predicted,
            cols_vary = "slowest",
            names_to = c(".value", "Type"),
            names_sep = "\\.",
            names_repair = 'unique') |>
  arrange(Outcome, Result, Type, n, prop, pct) |>
  ungroup() |>
  mutate(prop.lower = qbeta(0.95/2,
                            n+0.5,
                            Test_Observations-n+0.5)) |>
  mutate(prop.upper = qbeta(1-0.95/2,
                            n+0.5,
                            Test_Observations-n+0.5))

CR <- Chart_review |> # Used in density graph for vertical lines
  filter(Type == 'True' & Result == 'Yes') |>
  mutate(Model =
     ifelse(str_detect(Outcome, 'k'),
            'Restricted', 'Full')) |>
   mutate(Outcome_name = str_remove(Outcome,'k'))

RESULT_CR_test <- TP.prob |>  # Used for table of results
  full_join(CR, by = join_by(Model, Outcome_name)) |>
  mutate(Ratio = round(Prob/prop,2)) |>
  mutate(Prob = 100 * round(Prob,4)) |>
  select(Model, Outcome_name, Prob, pct, Ratio, everything())


########################################
# HIPE
HIPE_Observations = nrow(dt_PN.H)
Test_predictions.H <- bind_rows(dt_PN.H, dt_PNk.H,
                              dt_UTI.H, dt_UTIk.H,
                              dt_PU.H, dt_PUk.H,
                              dt_DL.H, dt_DLk.H,
                              dt_F2M.H, dt_F2Mk.H,
                              ) |>
  rename(Prediction = V1)

Test_predictions.H <- Test_predictions.H |>
  mutate(Model = ifelse(str_detect(Outcome, 'k'),
                        'Restricted',
                        'Full')) |>
  mutate(Outcome_name = str_remove(Outcome,'k'))

TP.prob.H <- Test_predictions.H |>
  group_by(Model, Outcome_name) |>
  summarise(Prob = sum(Yes)/HIPE_Observations) |>
  mutate(HIPE_Predicted = 100*round(Prob,4))
```

## Prediction and observed results from the Test data

For each outcome separate test and training datasets were selected, stratified by outcome, so that test and training sets would have approximately the same proportion of their outcomes in each pair. For each training data set two models were developed, guided by the overall permutation importance of the potential predictor variables. there are referred to as the full model which included all 52 variables, and the restricted models, which excluded variable with a permutation importance under 0.0001. From each of these 10 trained model, predictions for the test data and the overall HIPE data were calculated.

The underlying random forest model is a classifier, but it generates two predictions, for each record. The first is the probability of that record being in the positive class, which here corresponds to one of the five adverse outcomes. The second is an estimate of whether that record is classified into the positive class or not.

This decision, in turn, depends on the choice of a threshold, a cutoff of probability above which the record is classified as positive and below which it is classified as negative. The default cutoff is 0.5, but this has been adjusted to the cutoff from the ROC curve, which minimises the difference between the sensitivity and the specificity (1. Jiménez-Valverde A, Lobo JM. Threshold criteria for conversion of probability of species presence to either–or presence–absence. Acta Oecologica. 2007 May 1;31(3):361–9). This is a better threshold for our purposes. These thresholds, derived as part of the analysis above, are used in the class prediction here.

```{r Chart of predictions and truths}
ggplot(Chart_review,
       aes(x = Outcome,
           y = prop,
           colour = Type,
           shape  = Result)) +
  geom_point(size = 3) +
  geom_errorbar(
    aes(ymin = prop.lower,
        ymax = prop.upper), colour = 'gray') +
  theme_minimal() +
#  scale_color_discrete() +
  scale_colour_brewer(palette = "Dark2") +
  scale_y_continuous(labels = scales::label_percent()) +
  labs(title = "True and predicted values for Chart review TEST set",
       y = 'Percentage')

```

This shows the observed values, and the predictions from our models for each outcome (DL to UTIk), for the two sets of random forest models, those with a limited number of variables (DLk to UTIk), and those with almost all variables (DL to UTI).

Most of the models do reasonably well on this test, but the models for UTI grossly underestimate the prevalence of UTI in the test data, and those for F2M significantly overestimate the prevalence. 

A different way of looking at the same model is to look at density of the estimated probabilities.

```{r Density chart of predictions}
ggplot(Test_predictions, aes(fill = Outcome_name)) +
  geom_density(aes(x = prob.Yes)) +
  geom_vline(data = CR, #   TRUTH from chart review test
          aes(xintercept = prop),
          colour = 'green',
          linewidth = 1) +
  geom_vline(data = TP.prob, #  PREDICTED from chart review test
          aes(xintercept = Prob,
          colour = Outcome_name),
          linewidth = 1) +
  facet_wrap(~ Outcome_name + Model, scales = 'free_y') +
  theme_minimal() +
  scale_fill_brewer(palette = "Dark2", guide = 'none') +
  scale_colour_brewer(palette = "Dark2", guide = 'none') +
  scale_x_continuous(labels = scales::label_percent(), limits = c(0,0.4)) +
  labs(title = "Predicted probabilities for Chart review TEST set",
       subtitle  = 'Vertical lines are observed results, and predicted results',
       x = 'Probability of Yes',
       y = 'Density')

```

These predictions are better, at population level, because they do not depend on setting an arbitrary threshold for decision. In this setting, where essentially, we are modelling individual level risk, the most reasonable approach is to add these together for the population under study. Here that population is the 330 cases in the test dataset.

```{r Table of Chart Review results}
RESULT_CR_test |>
  select(Model:Ratio) |>
  rename(Predicted = Prob) |>
  rename(Truth = pct) |>
  kable(caption = 'Percentage rates of adverse outcomes int he chart review data, comparing The model predictions and the observed values')
```

## Prediction from the HIPE data

```{r HIPE predictions}
# Two new variables
# Model Full or Restricted
# Outcome_name 5 values, 'PN', 'UTI', 'PU', 'DL', 'F2M', no k variants
# 

########## Check means!
#mean(dt_PN.H$Yes)
#mean(dt_PNk.H$Yes)
#mean(dt_UTI.H$Yes)
#mean(dt_UTIk.H$Yes)
#mean(dt_PU.H$Yes)
#mean(dt_PUk.H$Yes)
#mean(dt_PU.H$Yes)
#mean(dt_PUk.H$Yes)
#mean(dt_DL.H$Yes)
#mean(dt_DLk.H$Yes)
#mean(dt_F2M.H$Yes)
#mean(dt_F2Mk.H$Yes)
############# These are different as they should be


ggplot(Test_predictions.H, aes(fill = Outcome_name)) +
  geom_density(aes(x = Yes)) +
  geom_vline(data = CR, #   TRUTH from chart review test for one site
          aes(xintercept = prop),
          colour = 'green',
          linewidth = 1) +
  geom_vline(data = TP.prob.H, #  PREDICTED from HIPE data for all sites
          aes(xintercept = Prob,
          colour = Outcome_name),
          linewidth = 1) +
  facet_wrap(~ Outcome_name + Model, scales = 'free_y') +
  theme_minimal() +
  scale_fill_brewer(palette = "Dark2", guide = 'none') +
  scale_colour_brewer(palette = "Dark2", guide = 'none') +
  scale_x_continuous(labels = scales::label_percent(), limits = c(0,0.4)) +
  labs(title = "Predicted probabilities for HIPE data",
       subtitle  = 'Vertical lines are chart review results, and predicted results',
       x = 'Probability of Yes',
       y = 'Density')

```

```{r HIPE results}
RESULT_CR_test <- RESULT_CR_test |>
  select(Model:Ratio) |>
  rename(Predicted = Prob) |>
  rename(Truth = pct)

TP.prob.H2 <- TP.prob.H |>
  full_join(RESULT_CR_test, by = join_by(Model, Outcome_name)) |>
  rename(CR_Predicted = Predicted) |>
  rename(CR_Truth = Truth) |>
  select(-c(Ratio, Prob))
  

TP.prob.H2 |>
  kable(caption = 'Predicted percentage of adverse outcomes in the HIPE review data')
```

The prediction on the HIPE data are shown. These are not so far from the observed results on the chart review data, though this is not a measure of the quality or accuracy of the predictions, rather it is more of a sanity check, an indication than the predictions are not completely wrong.

The next step is to examine these at hospital level. There are 17 Model 3 hospitals and 9 Model 4 hospitals in these data. Model 3 hospitals primarily provide routine acute general hospital services. Model 4 hospitals both serve the function of Model 3 hospitals, and, usually several, regional or national speciality services. The number of discharges included from model 3 hospitals ranges from 1,779 to 4,965, and those in Model 4 centres from 5,906 to 8,868

There are 5 types of adverse event, with 2 models per event (full and restricted), for 2 Models of hospital, in a total of 17 Model 3 hospitals, and 9 Model 4 hospitals.

```{r Calculate mean risks and more}
TP.H <- Test_predictions.H |>
  group_by(Outcome_name, Model, ModelCode, HospCode) |>
  summarise(Prob = mean(Yes))

TP.Mean <- TP.H |> # Hospital level mean
  group_by(Outcome_name, Model) |>
  summarise(Hospital.mean = mean(Prob))

TP.H <- TP.H |>
  right_join(TP.Mean,
             by = join_by(Outcome_name, Model)) |>
  mutate(Diff1 = Prob - Hospital.mean)

TP.H <- TP.H |> # Person level mean
  right_join(TP.prob.H |> rename(Person.mean = Prob),
             by = join_by(Outcome_name, Model)) |>
  mutate(Diff2 = Prob - Person.mean)

```

The next graph is a plot of the mean of the percentage risk for each adverse event, for the two models, for each of the hospitals.

```{r Plot of probabilities by hospital}
ggplot(TP.H,
       aes(x = Prob, y = Outcome_name,
           colour = Model)) +
  geom_point() +
  facet_wrap(~ HospCode) +
  theme_minimal() +
  theme(legend.position = 'bottom') +
  scale_fill_brewer(palette = "Dark2", guide = 'none') +
  scale_colour_brewer(palette = "Dark2") + # guide = 'none') +
  scale_x_continuous(labels = scales::label_percent()) + # limits = c(0,0.4)) +
  labs(title = 'Risk estimates, by hospital',
       subtitle = 'Based on chart review trained model, and HIPE data',
       x = 'Risk',
       y = 'Outcome')

```

A more informative graph, when it comes to looking at differences between hospitals, is a graph of the differences from the overall mean. A graph against the unweighted hospital level means is very similar.

```{r Plot of differences by hospital}
ggplot(TP.H,
       aes(x = Diff2, y = Outcome_name,
           colour = Model)) +
  geom_point() +
  facet_wrap(~ HospCode) +
  geom_vline(xintercept = 0, colour = 'black') +
  theme_minimal() +
  theme(legend.position = 'bottom') +
  scale_fill_brewer(palette = "Dark2", guide = 'none') +
  scale_colour_brewer(palette = "Dark2") + # guide = 'none') +
  scale_x_continuous(labels = scales::label_percent()) + # limits = c(0,0.4)) +
  labs(title = 'Differences from overall mean risk estimates, by hospital',
       subtitle = 'Based on chart review trained model, and HIPE data',
       x = 'Risk',
       y = 'Outcome')

```
