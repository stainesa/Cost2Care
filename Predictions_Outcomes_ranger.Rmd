---
title: "Predictions_Outcomes_ranger.Rmd"
author: "Anthony Staines"
date: "`r Sys.Date()`"
output: html_document
---

```{r clean up environment}
rm(list = ls())
```


# Intent

Our aim is to develop models for prediction of the various adverse outcomes, pneumonia, UTI, delirium and pressure ulcer from HIPE data. There are two HIPE data sets, one for all the HIPE data (H.N) and one for HIPE data from the chart review site (H.S). The gold standard for complications these is the chart review data (CTC.H).

The most obvious limitation is that the chart review was done in one hospital, a model 4 centre. The national HIPE data used here has only Model 3 and Model 4 hospitals.

# Predictions

Prepares predictions for each of our outcomes using a tree-based (ranger) classification approach, for the study site file, using the study (chart review) outcomes, and the HIPE predictors. These are then applied to the national data to see how they perform.

```{r setup, include=FALSE}

library(tidyverse)
library(lubridate)
library(tibble)
library(readxl)
library(comorbidity)
library(tidymodels)
library(lme4)
#library(lmerTest)
library(knitr)
library(kableExtra)
library(summarytools)
library(patchwork)

library(ranger)
library(mlr3verse)
library(mlr3viz)
library(data.table)
library(future)

library(sjPlot)
library(sjtable2df)

library(broom)
library(broom.mixed)

library(stargazer)

tidymodels_prefer(quiet = TRUE)

knitr::opts_chunk$set(echo = FALSE, cache = TRUE, warning = NA, message = NA, fig.pos = 'H',
      cache.extra = knitr::rand_seed)

st_options(ctable.round.digits = 2)

#How many CPU's?
N = 16
  options(Ncpus = N - 1)
  options(mc.cores = N - 1)
  setDTthreads(threads = N - 1,
               restore_after_fork = TRUE,
               throttle = 1024)

options(dplyr.summarise.inform = FALSE, # shut up summarise
        ranger.num.threads = N) # Prepare for rf models
lgr::get_logger("mlr3")$set_threshold("error")
lgr::get_logger("bbotk")$set_threshold("error")

set.seed(979)
rm(N)
```

## Load the merged data file, and the national file
These are the chart review file merged with the matched HIPE file provided by HPO, and the national HIPE file. They have been extensively processed.

```{r Load data file}

#####################################################
# Load the Names
# 
NAMES <- read_excel(
      'data/Clean Cost2Care Merged Chart Review and HIPE Data_WORKING.xlsx',
      sheet = 'NAMES')

#####################################################
# Load the merged HIPE/Study data file
# 
CTC.H <- readRDS('data/transformed_data/CTC.H.Rds')

#####################################################
# Load the total national HIPE data file
# 
H.N <- readRDS('data/transformed_data/H.N.Rds')

```

## Tidy data

```{r Remove non-HIPE codes, except for outcomes, and duplicate codes}
    CTC.H <- CTC.H %>%
    select(-c("StudyID", "Gender_consensus",
              "Age_consensus", "AgeC",
              "Elective_Emergency", "discode","source",
              "Ward_moves", "Bed_moves",
              "hosp.anon", "HospNo", "ModelF"))
```

## Proportions of each adverse outcome

```{r Proportions of each adverse outcome}
Proportions <- CTC.H |>
  select(PN:F2M, Died) |>
  mutate(across(PN:Died,
                ~ fct_count(as_factor(.),
                            prop=TRUE)$p[[2]])) |> # Proportion of Yes for each adverse Outcome (the smaller proportion)
  unique()
```

```{r Table of percentage of adverse events in chart review}
Proportions |>
  mutate(across(everything(), ~(round(. * 100,2)))) |>
  kable(caption = 'Percentage of people from chart review  experiencing each adverse outcome')

```

# Functions for ranger analysis

## select columns given the colunn name

```{r Function to select columns by name}
##################################################################
# This function allows us to replace a given outcome (PN etc...)
#  with the generic variable Outcome. Hence we can model Outcome
#  without worrying about which Outcome we've got
#
Set_Outcome <- function(Outcome = PN, df = CTC.H ) {
    df |>
        mutate(Outcome = {{Outcome}})
}

# Check
Df <- Set_Outcome(PN,CTC.H)
    table(Df$PN,Df$Outcome)
Df <- Set_Outcome(UTI,CTC.H)
    table(Df$UTI,Df$Outcome)
Df <- Set_Outcome(PU,CTC.H)
    table(Df$PU,Df$Outcome)
Df <- Set_Outcome(DL,CTC.H)
    table(Df$DL,Df$Outcome)
Df <- Set_Outcome(F2M,CTC.H)
    table(Df$F2M,Df$Outcome)
```

## Now define an outcome task

```{r Define Outcome task}

# Set up task
##################################################
# Define a classification task for Outome (PN:F2M).
#

Set_task <- function(Outcome = Outcome,
                     Data_for_classification = Df){
  
  tsk_Outcome <- as_task_classif(Data_for_classification,
                          target = "Outcome",
                          id = "Outcome",
                          stratum = "Outcome",
                          positive = "Yes") # Makes 'Yes' the rarer outcome, the positive response

  tsk_Outcome
}
```

## Split task into test aand training sets

```{r Split task}

Split_task <- function(tsk_Outcome) {
  split_Outcome = mlr3::partition(tsk_Outcome)

    split_Outcome
}
```

## Plot task

```{r function to plot task}

plot_task <- function(tsk_Outcome) {
  autoplot(tsk_Outcome) + 
    labs (title = 'Outcome',
          x = 'Outcome',
          y = 'Coding in chart review')
}
```

## Calculate importance

```{r function to calculate importance}
importance <- function(task) {
  lrn_Filter = lrn("classif.ranger",
                 seed = 42)
  lrn_Filter$param_set$values = list(importance = "permutation")
  
  flt_Imp = flt("importance", learner = lrn_Filter)
  flt_Imp$calculate(task)
}
```

## Filter by importance

```{r function to filter by importance}
filter_importance <- function(flt_Imp) {
  
  
}
```

## Draw importance

```{r function to draw importance}
draw_importance <- function(IMPORTANCE,
                            Outcome_label = "Outcome",
                            cut_off = 0){
TITLE <-  paste0("Plot of importance by feature for ",
                   Outcome_label)
SUBTITLE <- paste0("Importance greater than ", cut_off)

IMPORTANCE.df <-
    setDF(as.data.table(IMPORTANCE)) |>
    filter(score > cut_off) |>
    arrange(desc(score), feature) |>
    mutate(featureF = factor(feature)) |>
    mutate(featureF = fct_reorder(featureF, score))

ggplot(IMPORTANCE.df, aes(x = score, y = featureF)) +
  geom_point(colour = 'red') +
  labs(title = TITLE, subtitle = SUBTITLE,
       x = "Permutation importance", y = "Feature") +
  theme_minimal()
  }
```

## Simple trainer

Produces predictions from test set

```{r function for simple trainer}
trainer <- function(tsk_Outcome, split_Outcome){
  lrn_Outcome = lrn('classif.ranger',
            predict_type = "prob",
            mtry = 12,
            num.trees = 2000,
            importance = "permutation"
            )
  measures = msrs(
    c('classif.auc','classif.bbrier',
      'classif.logloss','classif.acc'))
  
  lrn_Outcome$train(tsk_Outcome,
                    split_Outcome$train)
  
  lrn_Outcome$train(tsk_Outcome,
                    split_Outcome$train)
  
  prediction = lrn_Outcome$predict(tsk_Outcome,
                              split_Outcome$test)
}
```

```{r print trainer output}
# function for simple trainer outputs
## define measures to include
measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc'))

## print and plot
print_predictions <- function(prediction) {
  cat(prediction$score(measures),"\n")
  cat(prediction$confusion,"\n")
  autoplot(prediction, type = 'roc')
}
```

## Tune trainer for mtry and num.trees

```{r function to tune trainer}
tune_trainer <- function(tsk_Outcome = tsk_Outcome,
                         n_Evaluations = 500){
  
# Same as before
  measures_selected = msrs(c('classif.auc','classif.bbrier',
                  'classif.logloss','classif.acc'))
  cv10 = rsmp("cv", folds = 10) # 10 folds

  search_space = ps(
    mtry = p_int(lower = 3, upper = 13),
    num.trees = p_int(900, 1500)
  )

  lrn_Outcome = lrn('classif.ranger',
            predict_type = "prob",
            importance = "permutation"
            )

  glrn_Outcome = GraphLearner$new(lrn_Outcome)

  instance_Outcome = ti(
    task = tsk_Outcome,
    learner = glrn_Outcome,
    resampling = cv10,
    terminator = trm("evals",
                   n_evals = n_Evaluations), # For tuner "random_search"
    search_space = search_space,
    measures = measures_selected
  )

tuner = tnr("random_search")
  future::plan(multisession, workers = 10)

tuner$optimize(instance_Outcome)

Tuner_df <- as_tibble(
  as.data.table(instance_Outcome$archive))

Tuner_df
}
```

## Draw tuner results

```{r Draw tuners}
draw_tuners = function(Tuner_df,
                       TITLE = 'TITLE'){
  # mtry
    #Maximise
    AUC.m <- ggplot(data = Tuner_df,
          aes(x = mtry,
              y = classif.auc)) +
        geom_smooth(colour = 'red') +
      theme_minimal()
    #Minimise
    BBRIER.m <- ggplot(data = Tuner_df,
                 aes(x = mtry,
                     y = classif.bbrier)) +
        geom_smooth(colour = 'green') +
      scale_y_reverse() +
      theme_minimal()
    #Maximise
    ACC.m <- ggplot(data = Tuner_df,
              aes(x = mtry,
                  y = classif.acc)) +
        geom_smooth(colour = 'red') +
      theme_minimal()
    #Minimise
    LOGLOSS.m <- ggplot(data = Tuner_df,
                  aes(x = mtry,
                      y = classif.logloss)) +
        geom_smooth(colour = 'green') +
      scale_y_reverse() +
      theme_minimal()

  # num.trees
    #Maximise
    AUC.t <- ggplot(data = Tuner_df,
          aes(x = num.trees,
              y = classif.auc)) +
        geom_smooth(colour = 'red') +
      theme_minimal()
    #Minimise
    BBRIER.t <- ggplot(data = Tuner_df,
                 aes(x = num.trees,
                     y = classif.bbrier)) +
        geom_smooth(colour = 'green') +
      scale_y_reverse() +
      theme_minimal()
    #Maximise
    ACC.t <- ggplot(data = Tuner_df,
              aes(x = num.trees,
                  y = classif.acc)) +
        geom_smooth(colour = 'red') +
      theme_minimal()
    #Minimise
    LOGLOSS.t <- ggplot(data = Tuner_df,
                  aes(x = num.trees,
                      y = classif.logloss)) +
        geom_smooth(colour = 'green') +
      scale_y_reverse() +
      theme_minimal()
    IMAGE <- AUC.m + AUC.t +
      BBRIER.m + BBRIER.t +
      ACC.m+ ACC.t +
      LOGLOSS.m + LOGLOSS.t +
        plot_annotation(title = TITLE) +
        plot_layout(axis_titles = 'collect') +
      plot_layout(nrow = 4)
      
return(IMAGE)
}

```

## Set threshold

```{r Caclulate optimum threshold}
SELECT_THRESHOLD <- function(df) {
  df <- df %>%
    rowid_to_column(var = 'Threshold') %>%
    mutate(Threshold = Threshold/n()) %>%
    mutate(Specificity =  1 - x) %>%
    mutate(Sensitivity = y) %>%
    mutate(Diff = Sensitivity - Specificity) %>%
      slice(which.min(abs(Diff))) %>%
  select(Threshold)
    
return(df)
}
```

# Outcomes

## Set Outcome to "PN"

```{r Outcome = PN}
Df <- Set_Outcome(PN,CTC.H)
    table(Df$PN,Df$Outcome)
```

```{r Set up and stratify task and split}

# Remove all outcomes, except the desired one
Df <- Df |> select(-c(PN:F2M))

# Set up task
tsk_Outcome <- Set_task(Outcome, Df)

  split_Outcome <- Split_task(tsk_Outcome)
      plot_task(tsk_Outcome)
      tsk_Outcome

IMPORTANCE <- importance(tsk_Outcome)

draw_importance(IMPORTANCE = IMPORTANCE,
                Outcome_label = "PN",
                cut_off = 0)

prediction <- trainer(
  tsk_Outcome = tsk_Outcome,
  split_Outcome = split_Outcome)

  print_predictions(prediction)
  roc_outcome = autoplot(prediction)
  SELECT_THRESHOLD(roc_outcome$data)

tune_df <- tune_trainer(tsk_Outcome, 50)

draw_tuners(tune_df, TITLE = "PN - Pneumonia")

```

From inspection of the graphs, it looks as if the outcomes are relatively insensitive to tree depth (mtry) and the number of trees analysed (num.tree).
It looks if between 5 and 10 is the best value for mtry, and 1200 to 1400 is best for num.trees.
# UTI

```{r Outcome = UTI}
Df <- Set_Outcome(UTI,CTC.H)
    table(Df$UTI,Df$Outcome)
```

```{r Set up and stratify task and split}

# Remove all outcomes, except the desired one
Df <- Df |> select(-c(PN:F2M))

# Set up task
tsk_Outcome <- Set_task(Outcome, Df)
  tsk_Outcome$col_roles$stratum = c("MedSurg", "HospCode")
  
  split_Outcome <- Split_task(tsk_Outcome)
    #  plot_task(tsk_Outcome)
    #  tsk_Outcome

IMPORTANCE <- importance(tsk_Outcome)

draw_importance(IMPORTANCE = IMPORTANCE,
                Outcome_label = "UTI",
                cut_off = 0)
```

# PU

```{r Outcome = PU}
Df <- Set_Outcome(PU,CTC.H)
    table(Df$PU,Df$Outcome)
```

```{r Set up and stratify task and split}

# Remove all outcomes, except the desired one
Df <- Df |> select(-c(PN:F2M))

# Set up task
tsk_Outcome <- Set_task(Outcome, Df)
  tsk_Outcome$col_roles$stratum = c("MedSurg", "HospCode")
  
  split_Outcome <- Split_task(tsk_Outcome)
    #  plot_task(tsk_Outcome)
    #  tsk_Outcome

IMPORTANCE <- importance(tsk_Outcome)

draw_importance(IMPORTANCE = IMPORTANCE,
                Outcome_label = "PU",
                cut_off = 0)
```

# DL

```{r Outcome = DL}
Df <- Set_Outcome(DL,CTC.H)
    table(Df$DL,Df$Outcome)
```

```{r Set up and stratify task and split}

# Remove all outcomes, except the desired one
Df <- Df |> select(-c(PN:F2M))

# Set up task
tsk_Outcome <- Set_task(Outcome, Df)
  tsk_Outcome$col_roles$stratum = c("MedSurg", "HospCode")
  
  split_Outcome <- Split_task(tsk_Outcome)
    #  plot_task(tsk_Outcome)
    #  tsk_Outcome

IMPORTANCE <- importance(tsk_Outcome)

draw_importance(IMPORTANCE = IMPORTANCE,
                Outcome_label = "DL",
                cut_off = 0)
```

# F2M

```{r Outcome = F2M}
Df <- Set_Outcome(F2M,CTC.H)
    table(Df$F2M,Df$Outcome)
```

```{r Set up and stratify task and split}

# Remove all outcomes, except the desired one
Df <- Df |> select(-c(PN:F2M))

# Set up task
tsk_Outcome <- Set_task(Outcome, Df)
  tsk_Outcome$col_roles$stratum = c("MedSurg", "HospCode")
  
  split_Outcome <- Split_task(tsk_Outcome)
    #  plot_task(tsk_Outcome)
    #  tsk_Outcome

IMPORTANCE <- importance(tsk_Outcome)

draw_importance(IMPORTANCE = IMPORTANCE,
                Outcome_label = "F2M",
                cut_off = 0)
```
