---
title: "Predictions PN"
author: "Anthony Staines"
date: "`r Sys.Date()`"
output:
  html_document: default
  word_document: default
  pdf_document: 
    toc: true
    fig_caption: true
    number_sections: true
    latex_engine: xelatex
    includes:
      in_header: 
      - !expr system.file("includes/fig-valign.tex", package = "summarytools")
editor_options:
  chunk_output_type: console
bibliography: references.bib
csl: vancouver.csl
---

# Predictions

Prepares predictions for pneumonia using a classification approach, for the study site file, using the study (chart review) outcomes, and the HIPE predictors. These are then applied to the national data to see how they perform.

```{r setup, include=FALSE}
rm(list=ls())

library(tidyverse)
library(lubridate)
library(tibble)
library(readxl)
library(comorbidity)
library(tidymodels)
library(lme4)
#library(lmerTest)
library(knitr)
library(kableExtra)
library(summarytools)
library(patchwork)


library(ranger)
library(mlr3verse)
library(mlr3viz)
library(data.table)
library(future)

library(sjPlot)
library(sjtable2df)

library(broom)
library(broom.mixed)

library(stargazer)

tidymodels_prefer(quiet = TRUE)

knitr::opts_chunk$set(echo = FALSE, cache = TRUE, warning = NA, message = NA, fig.pos = 'H',
      cache.extra = knitr::rand_seed)

st_options(ctable.round.digits = 2)

#How many CPU's?
N = 16
  options(Ncpus = N - 1)
  options(mc.cores = N - 1)
  setDTthreads(threads = N - 1,
               restore_after_fork = TRUE,
               throttle = 1024)

options(dplyr.summarise.inform = FALSE, # shut up summarise
        ranger.num.threads = N) # Prepare for rf models
lgr::get_logger("mlr3")$set_threshold("error")
lgr::get_logger("bbotk")$set_threshold("error")


set.seed(979)
rm(N)
```

# Load the merged data file, and the national file
These are the chart review file merged with the matched HIPE file provided by HPO, and the national HIPE file.

```{r Load data file}

#####################################################
# Load the merged HIPE/Study data file
# 
Cost2Care.HIPE <- readRDS('data/Cost2Care.HIPE.Rds')
NAMES <- read_excel(
      'data/Clean Cost2Care Merged Chart Review and HIPE Data_WORKING.xlsx',
      sheet = 'NAMES')

#####################################################
# Load the total HIPE data file
# 
HIPE <- readRDS('data/HIPE.Rds')

```

## Make smaller analysis data file
We omit records where the outcome is missing - these are the two discussed previously. We leave out date records, and text descriptions, but Season of admission remains in.

```{r simplify data}
 
C2C.H <- Cost2Care.HIPE %>%
        filter(!is.na(PN)) %>%
    select(PN, PN_Associated, H.PN, H.PN_hadx, ModelF:proc.desc.20,) %>%
    select(-c(DateOfAdmission, DateOfDischarge,
              MonthOfAdmission, MonthOfDischarge,
              mon.adm, ICD_combined, age)
                  ) %>%
    select(!contains('desc')) #%>% # Basically HIPE text variables
#    select(!starts_with('proc.')) %>%
#    select(!starts_with('dx.')) %>%
#    select(!starts_with('hadx.')) #%>%
 
```

# Intent

Our aim is to develop models for prediction of the various adverse outcomes, pneumonia, UTI, delirium and pressure ulcer. The gold standard for these is the chart review.

These models can be sense checked by their ability to predict the length of stay, the destination on discharge, and specifically if this was different from the source of admission, and death in hospital.

The most obvious limitation is that the chart review was done in one hospital, a model 4 centre.

# Pneumonia

There are several pneumonia related codes. These are :-

* PN    Chart review evidence of pneumonia
* H.PN  HIPE code found reflecting pneumonia
* pneumonia HIPE code found reflecting pneumonia
* PN_Associated Chart review code reflecting the likelihood that the pneumonia was health care associated
* H.PN_hadx HIPE code indicating a health care associated infection

We start by recoding these to more useful values.

We add one variable

* PN_A Chart review code where Yes is that the pneumonia was likely or definitely health care associated, No, that it was not associated, or that there was no pneumonia

```{r PN recode}
# Recode PN to remove missing values
C2C.H <- C2C.H %>%
  mutate(PN = ifelse(is.na(PN),'No',PN)) %>%
  mutate(PN_Associated = ifelse(is.na(PN_Associated),'No',PN_Associated)) %>%
  mutate(PN_Associated = ifelse(PN_Associated == 'No' & PN == 'No','Not Applicable',PN_Associated)) %>%
  mutate(PN_Associated = case_match(PN_Associated,
                                    'Not Applicable' ~ 'No PN',
                                    'No' ~ 'Not',
                                    'LIKELY associated' ~ 'Likely',
                                    'DEFINITELY associated.' ~ 'Definite')) %>%
  mutate(PN_A = case_match(PN_Associated,
                            'No PN' ~ 'No',
                            'Not' ~ 'No',
                            'Likely' ~ 'Yes',
                            'Definite' ~ 'Yes'
                            ))
# chec
ctable(PN_A <- as_factor(C2C.H$PN_A),
        PN_Associated <- as_factor(C2C.H$PN_Associated),
  prop = "n")

```

Prepare a number of tables exploring the relationships between the various PN variables.

```{r PN}
ctable(PN <- as_factor(C2C.H$PN),
        PN_Associated <- as_factor(C2C.H$PN_Associated),
  prop = "n")
```

Of 79 total cases in the chart review, 61 are definitely or probably associated with health care.

```{r}
ctable(PN <- as_factor(C2C.H$PN),
        H.PN <- as_factor(C2C.H$H.PN),
  prop = "r")

ctable(PN <- as_factor(C2C.H$PN),
        H.PN_hadx <- as_factor(C2C.H$H.PN_hadx),
  prop = "r")

ctable(PN_Associated <- as_factor(C2C.H$PN_Associated),
        H.PN_hadx <- as_factor(C2C.H$H.PN_hadx),
  prop = "c")
```

A small majority of the chart review cases (PN) (just over 51%, 41/79) do not have a pneumonia code recorded in HIPE (H.PN). A majority of the chart review pneumonia cases do not have a HIPE PN hadx code (H.PN_hadx) either (59.5%, 47/79). However, of those with a HIPE PN hadx code (H.PN_hadx) most (60% 30/52) had pneumonia recorded in the chart review, but of these for 8 out of the 30, the chart review indicated that the pneumonia was not health care associated.  

```{r}

ctable(pneumonia <- as_factor(C2C.H$pneumonia),
       PN <- as_factor(C2C.H$PN),
  prop = "r")

ctable(H.PN <- as_factor(C2C.H$H.PN),
       PN <- as_factor(C2C.H$PN),
  prop = "r")

# The next two are identical, H.PN is probably more useful
ctable(pneumonia <- as_factor(C2C.H$pneumonia),
       H.PN <- as_factor(C2C.H$H.PN),
  prop = "r")
```

The H.PN code and the pneumonia code are identical, which is expected, but only about half the chart review pneumonias have a HIPE pneumonia code, as already mentioned.

```{r}
ctable(H.PN_hadx <- as_factor(C2C.H$H.PN_hadx),
       PN <- as_factor(C2C.H$PN),
  prop = "r")

ctable(H.PN_hadx <- as_factor(C2C.H$H.PN_hadx),
       H.PN <- as_factor(C2C.H$H.PN),
  prop = "c")
```


# MLR3 for PN and PN Definitely or likely health care associated.

There are two obvious candidates for the outcome variable - one is the chart review pneumonia code PN, and the other is the chart review likely or definitely associated, which we called PN_A

## Classification

First we set up a classification task. We split the data into a two-thirds training set, and a one third testing set. This is a simple random split.


```{r Define and draw two classification tasks and splits}
#############################################################
# We delete a couple of variables at this point.
# rawlos, because LosC is also in the data
# PN_Associated, which is  a function of one target, and closely related to the other
# pneumonia, as H.PN is is identical, and is in the data set
# The other target (PN_A or PN)

#################################################
# Define a classification task for PN
#
tsk_PN <- as_task_classif(CTC.H,
                          target = "PN",
                          id = 'PN')
  tsk_PN
  split_PN = mlr3::partition(tsk_PN)
  
  autoplot(tsk_PN) + 
    labs (title = 'Pneumonia',
          x = 'PN',
          y = 'Coding in chart review')
  
TRAIN <- bind_cols(
  SET = split_PN$train,
  Result = 'TRAIN',
  PN = C2C.H$PN[split_PN$train])
TEST <- bind_cols(
  SET = split_PN$test,
  Result = 'TEST',
  PN = C2C.H$PN[split_PN$test])
SPLIT <- bind_rows(TRAIN,TEST)

ggplot(data = SPLIT, aes(y = PN, group = Result, x = Result)) + geom_jitter(colour='darkgreen') +
    labs (title = 'PN for Training and Test units',
          x = 'Test data or training data',
          y = 'Pneumonia in chart review') +
  theme_minimal()

#################################################
# Define a classification task for PN_A
#
tsk_PN_A <- as_task_classif(C2C.H %>% select(-c(PN_Associated, PN, pneumonia, rawlos)), target = "PN_A", id = 'PN_A')
  tsk_PN_A
  split_PN_A = partition(tsk_PN_A)
  
  autoplot(tsk_PN_A) + 
    labs (title = 'Pneumonia',
          x = 'PN_A',
          y = 'Coding in chart review')
  
TRAIN <- bind_cols(
  SET = split_PN_A$train,
  Result = 'TRAIN',
  PN = C2C.H$PN_A[split_PN_A$train])
TEST <- bind_cols(
  SET = split_PN_A$test,
  Result = 'TEST',
  PN = C2C.H$PN_A[split_PN_A$test])
SPLIT <- bind_rows(TRAIN,TEST)

ggplot(data = SPLIT, aes(y = PN_A, group = Result, x = Result)) + geom_jitter(colour='darkgreen') +
    labs (title = 'PN_A for Training and Test units',
          x = 'Test data or training data',
          y = 'Pneumonia in chart review') +
  theme_minimal()

```

## Importance

Then we look briefly at the importance of the respective variables using a simple ranger model, with no tuning. We could use this to filter variables out. 

```{r Look at variable importance}
lrn_Filter = lrn("classif.ranger", seed = 42)
  lrn_Filter$param_set$values = list(importance = "permutation")

flt_Imp = flt("importance", learner = lrn_Filter)

#################################################
# PN
#
  flt_Imp$calculate(tsk_PN)

  Importance_Filtered.PN <-
    setDF(as.data.table(flt_Imp)) %>%
    arrange(score, feature)
  
      autoplot(flt_Imp) +
        labs( title  = 'Importance plot PN',
              y = 'Permutation importance')

     Importance_Filtered.PN %>% filter(score > 0.01) %>% select (-score)
     Importance_Filtered.PN %>% filter(score <= 0.01 & score > 0.008) %>% select (-score)
     Importance_Filtered.PN %>% filter(score <= 0.008 & score > 0.004) %>% select (-score)
     Importance_Filtered.PN %>% filter(score <= 0.004 & score > 0) %>% select (-score)
     Importance_Filtered.PN %>% filter(score == 0) %>% select (-score)

#################################################
# PN_A
#
  flt_Imp$calculate(tsk_PN_A)

  Importance_Filtered.PN_A <-
    setDF(as.data.table(flt_Imp)) %>%
    arrange(score, feature)
  
      autoplot(flt_Imp) +
        labs( title  = 'Importance plot PN_A',
              y = 'Permutation importance')

     Importance_Filtered.PN_A %>% filter(score > 0.01) %>% select (-score)
     Importance_Filtered.PN_A %>% filter(score <= 0.01 & score > 0.008) %>% select (-score)
     Importance_Filtered.PN_A %>% filter(score <= 0.008 & score > 0.004) %>% select (-score)
     Importance_Filtered.PN_A %>% filter(score <= 0.004 & score > 0) %>% select (-score)
     Importance_Filtered.PN_A %>% filter(score == 0) %>% select (-score)

     # Merge the two importance dataframes
     Imp <- Importance_Filtered.PN %>%
       full_join(Importance_Filtered.PN_A, by = 'feature') %>%
       arrange(desc(score.x), desc(score.y))
     ggplot(data = Imp, aes(x=score.x, y = score.y)) + geom_point() +geom_smooth()

```

The importances for the PN and PN_A targets are not greatly different.
Roughly, all the individual hadx scores are non-contributory. The main contributors are the number of health care associated codes, the number of procedures, the diagnoses, and the length of stay.

## Define learners

```{r Ranger clasifer learner for PN}
lrn_featureless = lrn('classif.featureless',
                      predict_type = 'prob')

lrn_PN = lrn('classif.ranger',
            predict_type = "prob",
            mtry = 12,
            num.trees = 2000,
            importance = "permutation"
            )
lrn_PN_A = lrn_PN

measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc'))

#Rubbish trainer - classification accuracy 0.915
lrn_featureless$train(tsk_PN, split_PN$train)
prediction = lrn_featureless$predict(tsk_PN, split_PN$test)
prediction
prediction$score(measures)
prediction$confusion
autoplot(prediction, type = 'roc')

#Real trainer  - classificiation accuracy 0.927
lrn_PN$train(tsk_PN, split_PN$train)
prediction = lrn_PN$predict(tsk_PN, split_PN$test)
prediction
prediction$score(measures)
prediction$confusion
autoplot(prediction, type = 'roc')


#Rubbish trainer - classification accuracy 0.915
lrn_featureless$train(tsk_PN_A, split_PN_A$train)
prediction = lrn_featureless$predict(tsk_PN_A, split_PN_A$test)
prediction
prediction$score(measures)
prediction$confusion
autoplot(prediction, type = 'roc')

#Real trainer  - classificiation accuracy 0.927
lrn_PN_A$train(tsk_PN_A, split_PN_A$train)

############ Training data
  prediction = lrn_PN_A$predict(tsk_PN_A, split_PN_A$train)
  prediction$score(measures)
  prediction$confusion
############ Training data

################ Test data  
prediction = lrn_PN_A$predict(tsk_PN_A, split_PN_A$test)
prediction
prediction$score(measures)
prediction$confusion
autoplot(prediction, type = 'roc')
autoplot(prediction, type = 'prc')

```

## Resampling

We have seen the effect of a single split of the data, separating training and test data completely. Performance is much better on the trained data than on the test data. Resampling is another approach to the same issue. This involves creating multiple training and test sets, and repeating the analysis for each, then aggregating the results of these.

```{r resampling}
cv10 = rsmp("cv", folds = 10) # 10 folds

rr_PN = resample(tsk_PN, lrn_PN, cv10, store_models = TRUE)
rr_PN_A = resample(tsk_PN_A, lrn_PN_A, cv10, store_models = TRUE)

autoplot(rr_PN, type = 'roc') + labs(title="PN")
autoplot(rr_PN_A, type = 'roc') + labs(title='PN_A')
autoplot(rr_PN, type = 'prc') + labs(title='PN')
autoplot(rr_PN_A, type = 'prc') + labs(title='PN_A')
```

## Threshold Tuning
So far the cut-off for being positive is set at 0.5. This is not necessarily right, or close to optimal. Looking at the predictions for PN_A in the non-resampled model, makes this point clearly.

```{r Impact of different thresholds on classification accuracy}
prediction$set_threshold(0.7)
prediction$score(measures)
prediction$confusion

prediction$set_threshold(0.5)
prediction$score(measures)
prediction$confusion

prediction$set_threshold(0.2)
prediction$score(measures)
prediction$confusion

```

There is a way to identify the optimum threshold for declaring an adverse event (PN or PN_A = Yes). This involves tuning the threshold.

```{r Tune thresholds}
# Same as before
measures = msrs(c('classif.auc','classif.bbrier','classif.logloss','classif.acc'))
cv10 = rsmp("cv", folds = 10) # 10 folds

lrn_PN = lrn('classif.ranger',
            predict_type = "prob",
            importance = "permutation"
            )  %>>% po("threshold")

glrn_PN = GraphLearner$new(lrn_PN)
lrn_PN_A = lrn_PN
glrn_PN_A = GraphLearner$new(lrn_PN_A)

search_space = ps(
  threshold.thresholds = p_dbl(lower = 0, upper = 1),
  mtry = p_int(lower = 10, upper = 30),
  num.trees = p_int(1000, 2000)
)

instance_PN = ti(
  task = tsk_PN,
  learner = glrn_PN,
  resampling = cv10,
  measures = msr("classif.auc"),
  terminator = trm("evals", n_evals=2000), # For tuner "random_search"
  search_space = search_space
  )

tuner = tnr("random_search")
  future::plan(multisession, workers = 10)
tuner$optimize(instance_PN)
Tuner_df <- as_tibble(
  as.data.table(instance_PN$archive,
                measures = msrs(
                  c('classif.bbrier',
                    'classif.logloss',
                    'classif.acc'))))
autoplot(instance_PN)

ggplot(data = Tuner_df %>% mutate(mtry = cut(mtry,4)), aes(x = threshold.thresholds, y = classif.auc, group = mtry, colour=mtry)) + geom_smooth()# + facet_wrap(~cut(num.trees/1000, 4))

```


