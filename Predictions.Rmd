---
title: "Predictions"
author: "Anthony Staines"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: true
    fig_caption: true
    number_sections: true
    latex_engine: xelatex
    includes:
      in_header: 
      - !expr system.file("includes/fig-valign.tex", package = "summarytools")
  word_document: default
  html_document: default
editor_options:
  chunk_output_type: console
bibliography: references.bib
csl: vancouver.csl
---

# Predictions

Prepares predictions for selected outcome variables for the study site file, using the study (chart review) outcomes, and the HIPE predictors.

```{r setup, include=FALSE}
rm(list=ls())

library(tidyverse)
library(lubridate)
library(tibble)
library(readxl)
library(comorbidity)
library(tidymodels)
library(lme4)
#library(lmerTest)
library(knitr)
library(kableExtra)
library(summarytools)

library(ranger)
library(mlr3verse)
library(mlr3viz)
library(data.table)
library(future)

library(sjPlot)
library(sjtable2df)

library(broom)
library(broom.mixed)

library(stargazer)

tidymodels_prefer(quiet = TRUE)

knitr::opts_chunk$set(echo = FALSE, cache = TRUE, warning = NA, message = NA, fig.pos = 'H',
      cache.extra = knitr::rand_seed)

st_options(ctable.round.digits = 2)
# How many CPU's?
N = 16
  options(Ncpus = N - 1)
  options(mc.cores = N - 1)
  setDTthreads(threads = N - 1,
               restore_after_fork = TRUE,
               throttle = 1024)

options(dplyr.summarise.inform = FALSE, # shut up summarise
        ranger.num.threads = N) # Prepare for rf models

set.seed(979)
rm(N)
```

# Load the merged data file
This is the chart review file merged with the HIPE file provided by HPO.

```{r Load data file}

#####################################################
# Load the merged HIPE/Study data file
# 
C2C <- readRDS('data/Cost2Care.HIPE.rds')
NAMES <- read_excel(
      'data/Clean Cost2Care Merged Chart Review and HIPE Data_WORKING.xlsx',
      sheet = 'NAMES')

#####################################################
# Load the total HIPE data file
# 
HIPE <- readRDS('data/Hipe.rds')

```

# Outcomes

## lmer based predictions of Length of Stay (centred and scaled)

```{r}
ModelLOS <- lmer(data=C2C,
             LosC ~
               AgeC + as_factor(sex) + ScoreEl +
               Source + SeasonOfAdmission +
               Outcome + ProcCount + itulos +
               fullelig +
               (1|mdc))

tab_model(ModelLOS)
PRED <- predict(ModelLOS, newdata = C2C)

C <- cbind(C2C, PRED)

ggplot(data = C,
       aes(x=LosC, y = PRED, colour=as_factor(sex))) +
    geom_jitter() +
  geom_smooth() +
    labs(
        title="Comparison of model predictions to observed data for length of stay",
        x = "Observed scaled length of stay (SD units)",
        y = "Predicted scaled length of stay (SD units)"
    ) +
    guides(colour = guide_legend("Sex")) 

rm(C)
```

This is not such a bad predictor! Adding extra variables has improved the fit quite a lot.

# RF models

```{r}
# simple ranger model
# 
# 
C <- C2C %>%
    filter(!is.na(LOS)) %>%
#    filter(Training == TRUE) %>%
    select(Training:proc.desc.20,) %>%
    select(-c(DateOfAdmission, DateOfDischarge,
              MonthOfAdmission, MonthOfDischarge,
              mon.adm, rawlos, ICD_combined, age)
                  ) %>%
    select(!contains('desc')) #%>% # Basically HIPE variables
#    select(!starts_with('proc.')) %>%
#    select(!starts_with('dx.')) %>%
#    select(!starts_with('hadx.')) #%>%
  

RANGER1 <- ranger(
    data = C %>% select(-Training),
    formula = LosC ~.,
    num.trees = 2500,
    mtry = 11,
    importance = "permutation",
    case.weights = C$Training,
    holdout = TRUE
)

RANGER1

############################################
# Importance
# 
IMP <- importance(RANGER1)
  IMP_Names =  names(IMP)
  DF <- bind_cols(Names = IMP_Names, Importance = IMP) %>%
      arrange(Importance) %>%
      mutate(Names = fct_inorder(Names))
############################################
# Plot importance
# 
ggplot(data  = DF %>% filter(Importance > 0.0),
       aes(x= Importance, y = Names)) +
  geom_col(colour='red') +
  labs( title = "Variable importance",
  subtitle = "Permutation importance, holdout") +
  theme_minimal()

############################################
# Predictions on test and training data
#  
RANGER1.train.predict <- predict(RANGER1, data = C %>% filter(Training))
  summary(RANGER1.train.predict$predictions)
    PRED.train <- bind_cols(LosC = C$LosC[C$Training],
                            Prediction = RANGER1.train.predict$predictions,
                            Train = TRUE)

RANGER1.test.predict <- predict(RANGER1, data = C %>% filter(!Training))
  summary(RANGER1.test.predict$predictions)
    PRED.test <- bind_cols(LosC = C$LosC[!C$Training],
                           Prediction = RANGER1.test.predict$predictions,
                           Train = FALSE)

PRED <- bind_rows(PRED.test, PRED.train) %>%
  mutate(sqdiff = (LosC - Prediction)^2)

############################################
# MSE
# 
MSE_train <- PRED %>%
  filter(Train) %>%
  summarise(sum = sum(sqdiff), N = n()) %>%
  mutate(MSE = sum/N)

MSE_test <- PRED %>%
  filter(!Train) %>%
  summarise(sum = sum(sqdiff), N = n()) %>%
  mutate(MSE = sum/N)

cat("Training MSE", MSE_train$MSE, "Test MSE", MSE_test$MSE, "\n")

############################################
# Plot predictions
# 
ggplot(data=PRED,
       aes(x= LosC, y = Prediction, colour=Train)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~Train) +
  theme_minimal()

```

The ranger predictions are quite good too, though still not great at high lengths of stay.

# MLR3 for LosC - regression

```{r}
#
tsk_C <- as_task_regr(C, target = "LosC", id = 'C')
  tsk_C
  autoplot(tsk_C)

lrn_Filter = lrn("regr.ranger", seed = 42)
  lrn_Filter$param_set$values = list(importance = "permutation")

flt_Imp = flt("importance", learner = lrn_Filter)
  flt_Imp$calculate(tsk_C)

  Importance_Filtered <-
    setDF(as.data.table(flt_Imp))
      autoplot(flt_Imp)

lrn_R = lrn('regr.ranger',
            mtry = to_tune(5,25),
            num.trees = to_tune(500,3000),
            importance = "permutation"
            )

split = partition(tsk_C)

instance = ti(
  task = tsk_C,
  learner = lrn_R,
  resampling = rsmp("holdout"),
  measures = msr("regr.mse"),
#  terminator = trm("evals", n_evals=2000), # Reduce sharply for tuner "mbo"
  terminator = trm("evals", n_evals=750), # InreaseReduce sharply for tuner "random_search"
)

instance

tuner_mbo <- tnr("mbo")
tuner_random_search <- tnr("random_search")

#######################################
# SLOW - about 3.5 hours
plan(multisession)
  #tuner_mbo$optimize(instance)
  tuner_random_search$optimize(instance)

instance$result$learner_param_vals
```

```{r}
ParameterSearchResults <- as.data.table(instance$archive)
PSR <- ParameterSearchResults %>%
  select(mtry, num.trees, regr.mse)
ggplot(data = ParameterSearchResults, aes(x = mtry, y = regr.mse)) +
  geom_jitter()
ggplot(data = ParameterSearchResults, aes(x = num.trees, y = regr.mse)) +
  geom_jitter()
ggplot(data = ParameterSearchResults, aes(y = num.trees, x = mtry)) +
  geom_jitter(aes(size = regr.mse, colour= regr.mse))
autoplot(instance, type = "surface") + scale_color_brewer(palette = "Set1")

instance$archive$benchmark_result
#
# Strongly suggests 11 or so for mtry, but that the number of tress matters much less
# 
#
```
```{r}
lrn_R_mbo = lrn('regr.ranger',
            mtry = 11,
            num.trees = 2500,
            importance = "permutation"
            )

lrn_R_mbo$train(tsk_C, split$train)
prediction = lrn_R_mbo$predict(tsk_C, split$test)
prediction$score(msrs(c("regr.mse", "regr.mae")))

prediction$truth
OUTCOME <- as_tibble(cbind(Truth = prediction$truth, Predicted = prediction$response))

ggplot(data = OUTCOME, aes(x = Truth, y = Predicted)) +
  geom_jitter() +
  geom_smooth()
```


# Predict LosC for national HIPE data

```{r}
HIPE.prediction <- lrn_R_mbo$predict_newdata(task = tsk_C, newdata = HIPE)
autoplot(HIPE.prediction)
HIPE.prediction$score(msrs(c("regr.mse", "regr.mae")))


OUTCOME <- as_tibble(cbind(Truth = HIPE.prediction$truth,
                           Predicted = HIPE.prediction$response))

ggplot(data = OUTCOME, aes(x = Truth, y = Predicted)) +
  geom_jitter() +
  geom_smooth()
```


Long stay rates
